[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "빅데이터분석 특강 (2023)",
    "section": "",
    "text": "질문하는 방법\n\n카카오톡: 질문하러 가기 // 학기종료이후 폐쇄함\n이메일: guebin@jbnu.ac.kr\n직접방문: 자연과학대학 본관 205호\nZoom: 카카오톡이나 이메일로 미리 시간을 정할 것\nLMS쪽지: https://ieilms.jbnu.ac.kr/\n\n공지사항\n\n강의영상의 경우 시간이 지나면 고화질로 시청할 수 있습니다. (인코딩중에는 유튜브에서 저화질로 송출됩니다)\n\n강의노트\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 12, 2023\n\n\n02wk-006: 타이타닉, Autogluon (Fsize)\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-007: 타이타닉, Autogluon (Fsize,Drop)\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-004: 타이타닉, Alexis Cook의 코드\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-005: 타이타닉, Autogluon\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-008: 타이타닉, Autogluon (best_quality)\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-003: 타이타닉, 첫 제출\n\n\n최규빈 \n\n\n\n\nSep 5, 2023\n\n\n01wk-001: 캐글소개\n\n\n최규빈 \n\n\n\n\nSep 5, 2023\n\n\n01wk-002: 타이타닉, 데이터의 이해\n\n\n최규빈 \n\n\n\n\nSep 12, 2000\n\n\n2000-09-12 – Quarto Blog 만들기\n\n\n최규빈 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/02wk-003-타이타닉, 첫 제출.html#a.-test-답을-모름-제출해야-알-수-있음",
    "href": "posts/02wk-003-타이타닉, 첫 제출.html#a.-test-답을-모름-제출해야-알-수-있음",
    "title": "02wk-003: 타이타닉, 첫 제출",
    "section": "A. test – 답을 모름, 제출해야 알 수 있음",
    "text": "A. test – 답을 모름, 제출해야 알 수 있음\n- 제출결과는 리더보드에서 확인할 수 있음."
  },
  {
    "objectID": "posts/02wk-003-타이타닉, 첫 제출.html#b.-train-스스로-풀어보고-채점할-수-있음",
    "href": "posts/02wk-003-타이타닉, 첫 제출.html#b.-train-스스로-풀어보고-채점할-수-있음",
    "title": "02wk-003: 타이타닉, 첫 제출",
    "section": "B. train – 스스로 풀어보고 채점할 수 있음",
    "text": "B. train – 스스로 풀어보고 채점할 수 있음\n- 캐글에서 code \\(\\to\\) New Notebook 클릭\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n\ntr=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntst=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\n# 예비학습 – accuracy의 계산\n\nimport pandas as pd \n\n\ndf = pd.DataFrame({'Surv':[1,0,1,1,0],'Sex':['f','m','f','m','m']})\n\n- Surv 열의 선택\n\ndf.Surv\n\n0    1\n1    0\n2    1\n3    1\n4    0\nName: Surv, dtype: int64\n\n\n- Sex 열의 선택\n\ndf.Sex\n\n0    f\n1    m\n2    f\n3    m\n4    m\nName: Sex, dtype: object\n\n\n- Sex == f이면 생존(1), 그렇지 않으면 사망(0)이라고 예측\n\n(df.Sex == 'f')*1\n\n0    1\n1    0\n2    1\n3    0\n4    0\nName: Sex, dtype: int64\n\n\n- 결과를 정리하면 아래와 같음\n\npd.DataFrame({'real': df.Surv, 'estimate': (df.Sex == 'f')*1})\n\n\n\n\n\n\n\n\nreal\nestimate\n\n\n\n\n0\n1\n1\n\n\n1\n0\n0\n\n\n2\n1\n1\n\n\n3\n1\n0\n\n\n4\n0\n0\n\n\n\n\n\n\n\n- accuracy를 손으로 계산하면 \\(\\frac{4}{5}=0.8\\).\n- 컴퓨터로 accuracy를 계산한다면\n\n(df.Surv == (df.Sex == 'f')*1).sum()/5 # 방법1\n\n0.8\n\n\n\n(df.Surv == (df.Sex == 'f')*1).mean() # 방법2\n\n0.8\n\n\n#\n- 실제자료의 accuracy를 구해보자.\n\ntr = pd.read_csv('./titanic/train.csv')\n\n\n(tr.Survived == (tr.Sex == 'female')).mean()\n\n0.7867564534231201"
  },
  {
    "objectID": "posts/A1.html",
    "href": "posts/A1.html",
    "title": "2000-09-12 – Quarto Blog 만들기",
    "section": "",
    "text": "- 윈도우 컴퓨터인 경우 새로운 계정을 만들어 준다.\n\n한글계정명일 경우는 필수\n영어계정이더라도 계정을 분리하는게 관리에 유리\n\n- 최종적으로는\n\ngithub 로그인 + repository 생성완료\nanaconda 설치완료\ngit 설치완료\nquarto 설치완료\n\n이어야 한다.\n\n\n\ngithub 로그인\nrepository 생성 (저는 asdf로 만들었어요)\n\n\n주의: repository 만들때 readme.md 파일을 생성 할 것. 그래야 이후의 작업이 편리.\n\n\n\n\n\n아나콘다 다운로드: https://www.anaconda.com/download\n아나콘다 설치\n그림1처럼 terminal을 열었을 경우 (base)로 표시되는지 확인\n그림2처럼 conda env list등의 명령어가 잘 동작하는지 확인\n\n\n그림1그림2\n\n\n\n\n\n그림1: 정상적으로 설치된 경우 (base)가 보임\n\n\n\n\n\n\n\n그림2: 정상적으로 설치된 경우 cond env list 등의 명령이 잘 동작함. 현재환경은 *로 표시됨\n\n\n\n\n\n\n\n\n\ngit 다운로드: https://git-scm.com/downloads\n설치\nterminal에서 git을 입력하여 정상적으로 설치되었는지 확인. 그림3 참고.\n\n\n본인이 편리한 경우 github desktop 혹은 sourcetree 등에서 작업해도 무방함. 하지만 수업에서는 다루지 않음.\n\n\n\n\n그림3: git이 정상적으로 설치된 경우 출력예시\n\n\n\n\n\n\nquarto 다운로드: https://quarto.org/docs/download/ 에서 pre-release 버전을 다운로드 할 것 (기능이 좀 더 많음)\n설치\n터미널에서 quarto help를 입력하여 정상적으로 설치되었는지 확인. 그림4 참고.\n\n\n\n\n그림4: quarto가 정상적으로 설치된 경우 출력예시"
  },
  {
    "objectID": "posts/A1.html#a.-github",
    "href": "posts/A1.html#a.-github",
    "title": "2000-09-12 – Quarto Blog 만들기",
    "section": "",
    "text": "github 로그인\nrepository 생성 (저는 asdf로 만들었어요)\n\n\n주의: repository 만들때 readme.md 파일을 생성 할 것. 그래야 이후의 작업이 편리."
  },
  {
    "objectID": "posts/A1.html#b.-anaconda",
    "href": "posts/A1.html#b.-anaconda",
    "title": "2000-09-12 – Quarto Blog 만들기",
    "section": "",
    "text": "아나콘다 다운로드: https://www.anaconda.com/download\n아나콘다 설치\n그림1처럼 terminal을 열었을 경우 (base)로 표시되는지 확인\n그림2처럼 conda env list등의 명령어가 잘 동작하는지 확인\n\n\n그림1그림2\n\n\n\n\n\n그림1: 정상적으로 설치된 경우 (base)가 보임\n\n\n\n\n\n\n\n그림2: 정상적으로 설치된 경우 cond env list 등의 명령이 잘 동작함. 현재환경은 *로 표시됨"
  },
  {
    "objectID": "posts/A1.html#c.-git",
    "href": "posts/A1.html#c.-git",
    "title": "2000-09-12 – Quarto Blog 만들기",
    "section": "",
    "text": "git 다운로드: https://git-scm.com/downloads\n설치\nterminal에서 git을 입력하여 정상적으로 설치되었는지 확인. 그림3 참고.\n\n\n본인이 편리한 경우 github desktop 혹은 sourcetree 등에서 작업해도 무방함. 하지만 수업에서는 다루지 않음.\n\n\n\n\n그림3: git이 정상적으로 설치된 경우 출력예시"
  },
  {
    "objectID": "posts/A1.html#d.-quarto",
    "href": "posts/A1.html#d.-quarto",
    "title": "2000-09-12 – Quarto Blog 만들기",
    "section": "",
    "text": "quarto 다운로드: https://quarto.org/docs/download/ 에서 pre-release 버전을 다운로드 할 것 (기능이 좀 더 많음)\n설치\n터미널에서 quarto help를 입력하여 정상적으로 설치되었는지 확인. 그림4 참고.\n\n\n\n\n그림4: quarto가 정상적으로 설치된 경우 출력예시"
  },
  {
    "objectID": "posts/02wk-004-타이타닉, Alexis Cook의 코드.html#part-1-get-started",
    "href": "posts/02wk-004-타이타닉, Alexis Cook의 코드.html#part-1-get-started",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 1: Get started",
    "text": "Part 1: Get started\nIn this section, you’ll learn more about the competition and make your first submission.\n\nJoin the competition!\nThe first thing to do is to join the competition! Open a new window with the competition page, and click on the “Join Competition” button, if you haven’t already. (If you see a “Submit Predictions” button instead of a “Join Competition” button, you have already joined the competition, and don’t need to do so again.)\n\nThis takes you to the rules acceptance page. You must accept the competition rules in order to participate. These rules govern how many submissions you can make per day, the maximum team size, and other competition-specific details. Then, click on “I Understand and Accept” to indicate that you will abide by the competition rules.\n\n\nThe challenge\nThe competition is simple: we want you to use the Titanic passenger data (name, age, price of ticket, etc) to try to predict who will survive and who will die.\n\n\nThe data\nTo take a look at the competition data, click on the Data tab at the top of the competition page. Then, scroll down to find the list of files.\nThere are three files in the data: (1) train.csv, (2) test.csv, and (3) gender_submission.csv.\n\n(1) train.csv\ntrain.csv contains the details of a subset of the passengers on board (891 passengers, to be exact – where each passenger gets a different row in the table). To investigate this data, click on the name of the file on the left of the screen. Once you’ve done this, you can view all of the data in the window.\n\nThe values in the second column (“Survived”) can be used to determine whether each passenger survived or not: - if it’s a “1”, the passenger survived. - if it’s a “0”, the passenger died.\nFor instance, the first passenger listed in train.csv is Mr. Owen Harris Braund. He was 22 years old when he died on the Titanic.\n\n\n(2) test.csv\nUsing the patterns you find in train.csv, you have to predict whether the other 418 passengers on board (in test.csv) survived.\nClick on test.csv (on the left of the screen) to examine its contents. Note that test.csv does not have a “Survived” column - this information is hidden from you, and how well you do at predicting these hidden values will determine how highly you score in the competition!\n\n\n(3) gender_submission.csv\nThe gender_submission.csv file is provided as an example that shows how you should structure your predictions. It predicts that all female passengers survived, and all male passengers died. Your hypotheses regarding survival will probably be different, which will lead to a different submission file. But, just like this file, your submission should have: - a “PassengerId” column containing the IDs of each passenger from test.csv. - a “Survived” column (that you will create!) with a “1” for the rows where you think the passenger survived, and a “0” where you predict that the passenger died."
  },
  {
    "objectID": "posts/02wk-004-타이타닉, Alexis Cook의 코드.html#part-2-your-coding-environment",
    "href": "posts/02wk-004-타이타닉, Alexis Cook의 코드.html#part-2-your-coding-environment",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 2: Your coding environment",
    "text": "Part 2: Your coding environment\nIn this section, you’ll train your own machine learning model to improve your predictions. If you’ve never written code before or don’t have any experience with machine learning, don’t worry! We don’t assume any prior experience in this tutorial.\n\nThe Notebook\nThe first thing to do is to create a Kaggle Notebook where you’ll store all of your code. You can use Kaggle Notebooks to getting up and running with writing code quickly, and without having to install anything on your computer. (If you are interested in deep learning, we also offer free GPU access!)\nBegin by clicking on the Code tab on the competition page. Then, click on “New Notebook”.\n\nYour notebook will take a few seconds to load. In the top left corner, you can see the name of your notebook – something like “kernel2daed3cd79”.\n\nYou can edit the name by clicking on it. Change it to something more descriptive, like “Getting Started with Titanic”.\n\n\n\nYour first lines of code\nWhen you start a new notebook, it has two gray boxes for storing code. We refer to these gray boxes as “code cells”.\n\nThe first code cell already has some code in it. To run this code, put your cursor in the code cell. (If your cursor is in the right place, you’ll notice a blue vertical line to the left of the gray box.) Then, either hit the play button (which appears to the left of the blue line), or hit [Shift] + [Enter] on your keyboard.\nIf the code runs successfully, three lines of output are returned. Below, you can see the same code that you just ran, along with the output that you should see in your notebook.\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nThis shows us where the competition data is stored, so that we can load the files into the notebook. We’ll do that next.\n\n\nLoad the data\nThe second code cell in your notebook now appears below the three lines of output with the file locations.\n\nType the two lines of code below into your second code cell. Then, once you’re done, either click on the blue play button, or hit [Shift] + [Enter].\n\ntrain_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\nYour code should return the output above, which corresponds to the first five rows of the table in train.csv. It’s very important that you see this output in your notebook before proceeding with the tutorial! &gt; If your code does not produce this output, double-check that your code is identical to the two lines above. And, make sure your cursor is in the code cell before hitting [Shift] + [Enter].\nThe code that you’ve just written is in the Python programming language. It uses a Python “module” called pandas (abbreviated as pd) to load the table from the train.csv file into the notebook. To do this, we needed to plug in the location of the file (which we saw was /kaggle/input/titanic/train.csv).\n&gt; If you’re not already familiar with Python (and pandas), the code shouldn’t make sense to you – but don’t worry! The point of this tutorial is to (quickly!) make your first submission to the competition. At the end of the tutorial, we suggest resources to continue your learning.\nAt this point, you should have at least three code cells in your notebook.\n\nCopy the code below into the third code cell of your notebook to load the contents of the test.csv file. Don’t forget to click on the play button (or hit [Shift] + [Enter])!\n\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n\n\n\n\n\n\n\nAs before, make sure that you see the output above in your notebook before continuing.\nOnce all of the code runs successfully, all of the data (in train.csv and test.csv) is loaded in the notebook. (The code above shows only the first 5 rows of each table, but all of the data is there – all 891 rows of train.csv and all 418 rows of test.csv!)"
  },
  {
    "objectID": "posts/02wk-004-타이타닉, Alexis Cook의 코드.html#part-3-your-first-submission",
    "href": "posts/02wk-004-타이타닉, Alexis Cook의 코드.html#part-3-your-first-submission",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 3: Your first submission",
    "text": "Part 3: Your first submission\nRemember our goal: we want to find patterns in train.csv that help us predict whether the passengers in test.csv survived.\nIt might initially feel overwhelming to look for patterns, when there’s so much data to sort through. So, we’ll start simple.\n\nExplore a pattern\nRemember that the sample submission file in gender_submission.csv assumes that all female passengers survived (and all male passengers died).\nIs this a reasonable first guess? We’ll check if this pattern holds true in the data (in train.csv).\nCopy the code below into a new code cell. Then, run the cell.\n\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)/len(women)\n\nprint(\"% of women who survived:\", rate_women)\n\n% of women who survived: 0.7420382165605095\n\n\n\n여성의 생존률을 구하는 코드입니다, 이전에 accuracy를 구하던 테크닉을 활용하면 아래의 코드도 가능합니다\n\ntrain_data[train_data.Sex == 'female'].Survived.mean()\n\n0.7420382165605095\n\n\n\nBefore moving on, make sure that your code returns the output above. The code above calculates the percentage of female passengers (in train.csv) who survived.\nThen, run the code below in another code cell:\n\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)/len(men)\n\nprint(\"% of men who survived:\", rate_men)\n\n% of men who survived: 0.18890814558058924\n\n\nThe code above calculates the percentage of male passengers (in train.csv) who survived.\nFrom this you can see that almost 75% of the women on board survived, whereas only 19% of the men lived to tell about it. Since gender seems to be such a strong indicator of survival, the submission file in gender_submission.csv is not a bad first guess!\nBut at the end of the day, this gender-based submission bases its predictions on only a single column. As you can imagine, by considering multiple columns, we can discover more complex patterns that can potentially yield better-informed predictions. Since it is quite difficult to consider several columns at once (or, it would take a long time to consider all possible patterns in many different columns simultaneously), we’ll use machine learning to automate this for us."
  },
  {
    "objectID": "posts/02wk-004-타이타닉, Alexis Cook의 코드.html#your-first-machine-learning-model",
    "href": "posts/02wk-004-타이타닉, Alexis Cook의 코드.html#your-first-machine-learning-model",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Your first machine learning model",
    "text": "Your first machine learning model\nWe’ll build what’s known as a random forest model. This model is constructed of several “trees” (there are three trees in the picture below, but we’ll construct 100!) that will individually consider each passenger’s data and vote on whether the individual survived. Then, the random forest model makes a democratic decision: the outcome with the most votes wins!\n\nThe code cell below looks for patterns in four different columns (“Pclass”, “Sex”, “SibSp”, and “Parch”) of the data. It constructs the trees in the random forest model based on patterns in the train.csv file, before generating predictions for the passengers in test.csv. The code also saves these new predictions in a CSV file submission.csv.\nCopy this code into your notebook, and run it in a new code cell.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission_AlexisCook.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!\n\n\nMake sure that your notebook outputs the same message above (Your submission was successfully saved!) before moving on. &gt; Again, don’t worry if this code doesn’t make sense to you! For now, we’ll focus on how to generate and submit predictions.\nOnce you’re ready, click on the “Save Version” button in the top right corner of your notebook. This will generate a pop-up window.\n- Ensure that the “Save and Run All” option is selected, and then click on the “Save” button. - This generates a window in the bottom left corner of the notebook. After it has finished running, click on the number to the right of the “Save Version” button. This pulls up a list of versions on the right of the screen. Click on the ellipsis (…) to the right of the most recent version, and select Open in Viewer.\n- Click on the Data tab on the top of the screen. Then, click on the “Submit” button to submit your results.\n\nCongratulations for making your first submission to a Kaggle competition! Within ten minutes, you should receive a message providing your spot on the leaderboard. Great work!"
  },
  {
    "objectID": "posts/02wk-004-타이타닉, Alexis Cook의 코드.html#part-4-learn-more",
    "href": "posts/02wk-004-타이타닉, Alexis Cook의 코드.html#part-4-learn-more",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 4: Learn more!",
    "text": "Part 4: Learn more!\nIf you’re interested in learning more, we strongly suggest our (3-hour) Intro to Machine Learning course, which will help you fully understand all of the code that we’ve presented here. You’ll also know enough to generate even better predictions!"
  },
  {
    "objectID": "posts/02wk-004-타이타닉, Alexis Cook의 코드.html#a.-alexis-cook의-분석은-train에서-얼마나-잘-맞출까",
    "href": "posts/02wk-004-타이타닉, Alexis Cook의 코드.html#a.-alexis-cook의-분석은-train에서-얼마나-잘-맞출까",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "A. Alexis Cook의 분석은 train에서 얼마나 잘 맞출까?",
    "text": "A. Alexis Cook의 분석은 train에서 얼마나 잘 맞출까?\n- 원래코드\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission_AlexisCook.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!\n\n\n\nlen(predictions), len(X_test)\n\n(418, 418)\n\n\n- 이렇게 수정하면 될 듯\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)\n\n\nlen(predictions),len(y)\n\n(891, 891)\n\n\n\n(predictions == y).mean()\n\n0.8159371492704826"
  },
  {
    "objectID": "posts/02wk-004-타이타닉, Alexis Cook의 코드.html#b.-alexis-cook의-코드를-수정해보자",
    "href": "posts/02wk-004-타이타닉, Alexis Cook의 코드.html#b.-alexis-cook의-코드를-수정해보자",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "B. Alexis Cook의 코드를 수정해보자!",
    "text": "B. Alexis Cook의 코드를 수정해보자!\n- 코드를 수정해보자.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=5000, max_depth=1000, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)\n\n\n(predictions == y).mean()\n\n0.8170594837261503\n\n\n\n내가 만든게 더 좋은데??\n\n- 이것도 제출결과로 만들어보자.\n\npredictions = model.predict(X_test)\n\n- 아래와 같이 제출하면 에러가 발생\n\npd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\")\\\n.assign(Survived=predictions)\\\n.to_csv(\"AlexisCook수정_submission.csv\")\n\n- 아래와 같이 제출파일을 저장해야 한다.\n\npd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\")\\\n.assign(Survived=predictions)\\\n.to_csv(\"AlexisCook수정2_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/02wk-006-타이타닉, Autogluon (Fsize).html#a.-데이터",
    "href": "posts/02wk-006-타이타닉, Autogluon (Fsize).html#a.-데이터",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"/kaggle/input/titanic/train.csv\")\ntst = TabularDataset(\"/kaggle/input/titanic/test.csv\")\n\n- 피처엔지니어링\n\ntr.eval('Fsize = SibSp + Parch')\ntst.eval('Fsize = SibSp + Parch')\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nFsize\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n0\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n1\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n0\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n0\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n413\n1305\n3\nSpector, Mr. Woolf\nmale\nNaN\n0\n0\nA.5. 3236\n8.0500\nNaN\nS\n0\n\n\n414\n1306\n1\nOliva y Ocana, Dona. Fermina\nfemale\n39.0\n0\n0\nPC 17758\n108.9000\nC105\nC\n0\n\n\n415\n1307\n3\nSaether, Mr. Simon Sivertsen\nmale\n38.5\n0\n0\nSOTON/O.Q. 3101262\n7.2500\nNaN\nS\n0\n\n\n416\n1308\n3\nWare, Mr. Frederick\nmale\nNaN\n0\n0\n359309\n8.0500\nNaN\nS\n0\n\n\n417\n1309\n3\nPeter, Master. Michael J\nmale\nNaN\n1\n1\n2668\n22.3583\nNaN\nC\n2\n\n\n\n\n418 rows × 12 columns"
  },
  {
    "objectID": "posts/02wk-006-타이타닉, Autogluon (Fsize).html#b.-predictor-생성",
    "href": "posts/02wk-006-타이타닉, Autogluon (Fsize).html#b.-predictor-생성",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230912_115814/\""
  },
  {
    "objectID": "posts/02wk-006-타이타닉, Autogluon (Fsize).html#c.-적합fit",
    "href": "posts/02wk-006-타이타닉, Autogluon (Fsize).html#c.-적합fit",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(tr.eval('Fsize = SibSp + Parch'))\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230912_115814/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   294.00 GB / 490.57 GB (59.9%)\nTrain Data Rows:    891\nTrain Data Columns: 12\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    125936.3 MB\n    Train Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    12 features in original data used to generate 29 features in processed data.\n    Train Data (Processed) Memory Usage: 0.08 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.21s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.648    = Validation score   (accuracy)\n    0.09s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: KNeighborsDist ...\n    0.648    = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8156   = Validation score   (accuracy)\n    0.2s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8212   = Validation score   (accuracy)\n    0.18s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.31s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8212   = Validation score   (accuracy)\n    0.48s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.53s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8045   = Validation score   (accuracy)\n    0.88s    = Training   runtime\n    0.12s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.51s    = Training   runtime\n    0.08s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n    0.8324   = Validation score   (accuracy)\n    1.32s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.15s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.838    = Validation score   (accuracy)\n    1.38s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8324   = Validation score   (accuracy)\n    0.38s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8603   = Validation score   (accuracy)\n    0.24s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 7.68s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230912_115814/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f46f0228310&gt;\n\n\n- 리더보드확인 (모의고사채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.860335       0.016608  2.934600                0.000400           0.236214            2       True         14\n1        NeuralNetTorch   0.837989       0.008671  1.382263                0.008671           1.382263            1       True         12\n2         LightGBMLarge   0.832402       0.003077  0.377827                0.003077           0.377827            1       True         13\n3       NeuralNetFastAI   0.832402       0.007537  1.316123                0.007537           1.316123            1       True         10\n4              CatBoost   0.826816       0.003649  0.528946                0.003649           0.528946            1       True          7\n5               XGBoost   0.826816       0.004545  0.150251                0.004545           0.150251            1       True         11\n6              LightGBM   0.821229       0.003294  0.180805                0.003294           0.180805            1       True          4\n7      RandomForestEntr   0.821229       0.194986  0.478185                0.194986           0.478185            1       True          6\n8            LightGBMXT   0.815642       0.003372  0.198229                0.003372           0.198229            1       True          3\n9      RandomForestGini   0.815642       0.097445  0.306731                0.097445           0.306731            1       True          5\n10       ExtraTreesEntr   0.810056       0.076015  0.512563                0.076015           0.512563            1       True          9\n11       ExtraTreesGini   0.804469       0.118582  0.881634                0.118582           0.881634            1       True          8\n12       KNeighborsDist   0.648045       0.001869  0.007722                0.001869           0.007722            1       True          2\n13       KNeighborsUnif   0.648045       0.004271  0.092099                0.004271           0.092099            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.860335\n0.016608\n2.934600\n0.000400\n0.236214\n2\nTrue\n14\n\n\n1\nNeuralNetTorch\n0.837989\n0.008671\n1.382263\n0.008671\n1.382263\n1\nTrue\n12\n\n\n2\nLightGBMLarge\n0.832402\n0.003077\n0.377827\n0.003077\n0.377827\n1\nTrue\n13\n\n\n3\nNeuralNetFastAI\n0.832402\n0.007537\n1.316123\n0.007537\n1.316123\n1\nTrue\n10\n\n\n4\nCatBoost\n0.826816\n0.003649\n0.528946\n0.003649\n0.528946\n1\nTrue\n7\n\n\n5\nXGBoost\n0.826816\n0.004545\n0.150251\n0.004545\n0.150251\n1\nTrue\n11\n\n\n6\nLightGBM\n0.821229\n0.003294\n0.180805\n0.003294\n0.180805\n1\nTrue\n4\n\n\n7\nRandomForestEntr\n0.821229\n0.194986\n0.478185\n0.194986\n0.478185\n1\nTrue\n6\n\n\n8\nLightGBMXT\n0.815642\n0.003372\n0.198229\n0.003372\n0.198229\n1\nTrue\n3\n\n\n9\nRandomForestGini\n0.815642\n0.097445\n0.306731\n0.097445\n0.306731\n1\nTrue\n5\n\n\n10\nExtraTreesEntr\n0.810056\n0.076015\n0.512563\n0.076015\n0.512563\n1\nTrue\n9\n\n\n11\nExtraTreesGini\n0.804469\n0.118582\n0.881634\n0.118582\n0.881634\n1\nTrue\n8\n\n\n12\nKNeighborsDist\n0.648045\n0.001869\n0.007722\n0.001869\n0.007722\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.648045\n0.004271\n0.092099\n0.004271\n0.092099\n1\nTrue\n1"
  },
  {
    "objectID": "posts/02wk-006-타이타닉, Autogluon (Fsize).html#d.-예측-predict",
    "href": "posts/02wk-006-타이타닉, Autogluon (Fsize).html#d.-예측-predict",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(tr.eval('Fsize = SibSp + Parch'))).mean()\n\n0.9169472502805837\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(tst.eval('Fsize = SibSp + Parch'))).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"autogluon(Fsize)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/01wk-001-캐글소개.html",
    "href": "posts/01wk-001-캐글소개.html",
    "title": "01wk-001: 캐글소개",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. 타이타닉 경진대회 찾기\n- 방법1: 검색을 통하여 접속하기\n\n\n\n그림1: 타이타닉 경진대회 검색\n\n\n- 방법2: 직접주소를 입력 https://www.kaggle.com/competitions/titanic\n\n\n3. 데이터구경\n- 데이터의 설명을 빠르게 파악하는 방법\n\n방법1: 변수위주로 Kaggle 홈페이지에서 파악\n방법2: 구글번역기 활용\n방법3: \bChatGPT이용\n\n- 제 생각\n\nChatGPT가 옳지 않은 소리를 할 떄도 있음. 하지만 처음에 데이터에 대한 개념을 빠르게 정리하고자 할 때 도움이 된다.\n변수이름이 약어로 된 경우가 많은데 이럴경우 GPT가 유용하다.\n\n\n\n4. 메뉴소개\n- 캐글 웹사이트의 메뉴탭 소개\n\nOverview (개요):\n\n\n경진대회 주최자가 경진대회의 배경, 목표, 데이터셋 설명 등을 설명\n\n\nData (데이터):\n\n\n경진대회에 사용되는 데이터셋에 관한 정보를 찾을 수 있음.\n데이터의 구성, 변수 설명, 예시 데이터 등이 제공되며, 데이터를 이해하고 분석할 수 있는데 필요한 정보들이 여기에 포함됨.\n\n\nCode (코드):\n\n\n경진대회 참가자들이 코드를 공유하고 토론하는 공간.\n주로 주어진 문제에 대한 데이터 분석 및 모델링 코드, 데이터 전처리 방법, 모델 학습 등에 관련된 내용이 이곳에 포함됨.\n\n\nDiscussion (토론):\n\n\n참가자들이 서로 의견을 교환하고 질문을 주고받을 수 있는 공간.\n데이터 분석 방법, 모델 구축 전략, 문제 해결 과정 등에 대한 토론이 이루어짐.\n\n\nLeaderboard (리더보드):\n\n\n경진대회 참가자들의 성능 평가 지표와 순위가 나열.\n참가자들의 모델 성능을 비교하고 경쟁 상황을 실시간으로 확인할 수 있음.\n\n\nRules (규칙):\n\n\n참가자들이 따라야 할 규칙, 데이터 사용 방법, 평가 지표 등이 명시되어 있음.\n\n- 체크하면 좋은 것들\n\nOveview: 평가지표(metric), 상(prize), 시간(timeline)\nRules: 평가지표(metric), 외부데이터 사용여부, 하루 최대 제출 수, 최종선택가능한 솔루션 수 (limit)\n\n- 대회의 유형 (ref: https://github.com/PacktPublishing/The-Kaggle-Book)\n\nGetting Started: 상을 제공하지 않음. 튜토리얼용. 매우 협력적인 환경.\nFeatured: 가장 일반적인 유형. 스폰서 회사의 비지니스 관련문제. 상금이 후함. 솔루션을 소개하는 자세한 리포트를 준비해야하고 발표할 것을 요구받을 수 있음.\nAnalytics: 질적 평가. 참가자의 PPT를 제출로 받음."
  },
  {
    "objectID": "posts/02wk-007-타이타닉, Autogluon (Fsize,Drop).html#a.-데이터",
    "href": "posts/02wk-007-타이타닉, Autogluon (Fsize,Drop).html#a.-데이터",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"/kaggle/input/titanic/train.csv\")\ntst = TabularDataset(\"/kaggle/input/titanic/test.csv\")\n\nLoaded data from: ./titanic/train.csv | Columns = 12 / 12 | Rows = 891 -&gt; 891\nLoaded data from: ./titanic/test.csv | Columns = 11 / 11 | Rows = 418 -&gt; 418\n\n\n- 피처엔지니어링\n\n_tr = tr.eval('Fsize = SibSp + Parch').drop(['SibSp','Parch'],axis=1)\n_tst = tst.eval('Fsize = SibSp + Parch').drop(['SibSp','Parch'],axis=1)"
  },
  {
    "objectID": "posts/02wk-007-타이타닉, Autogluon (Fsize,Drop).html#b.-predictor-생성",
    "href": "posts/02wk-007-타이타닉, Autogluon (Fsize,Drop).html#b.-predictor-생성",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230912_120900/\""
  },
  {
    "objectID": "posts/02wk-007-타이타닉, Autogluon (Fsize,Drop).html#c.-적합fit",
    "href": "posts/02wk-007-타이타닉, Autogluon (Fsize,Drop).html#c.-적합fit",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(_tr) # 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230912_120900/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   293.85 GB / 490.57 GB (59.9%)\nTrain Data Rows:    891\nTrain Data Columns: 10\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    124991.67 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    10 features in original data used to generate 27 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.19s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8101   = Validation score   (accuracy)\n    0.2s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8268   = Validation score   (accuracy)\n    0.28s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.33s    = Training   runtime\n    0.09s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8212   = Validation score   (accuracy)\n    0.38s    = Training   runtime\n    0.21s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.66s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8045   = Validation score   (accuracy)\n    0.52s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.7989   = Validation score   (accuracy)\n    0.35s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n    0.8268   = Validation score   (accuracy)\n    0.46s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8212   = Validation score   (accuracy)\n    0.15s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.838    = Validation score   (accuracy)\n    1.55s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8268   = Validation score   (accuracy)\n    0.44s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8603   = Validation score   (accuracy)\n    0.25s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 6.75s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230912_120900/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f463532b730&gt;\n\n\n- 리더보드확인 (모의고사 채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.860335       0.210158  3.510778                0.000527           0.246271            2       True         14\n1        NeuralNetTorch   0.837989       0.008443  1.547239                0.008443           1.547239            1       True         12\n2         LightGBMLarge   0.826816       0.003162  0.443558                0.003162           0.443558            1       True         13\n3              CatBoost   0.826816       0.003559  0.661513                0.003559           0.661513            1       True          7\n4              LightGBM   0.826816       0.003659  0.283715                0.003659           0.283715            1       True          4\n5       NeuralNetFastAI   0.826816       0.007515  0.462052                0.007515           0.462052            1       True         10\n6               XGBoost   0.821229       0.004702  0.148207                0.004702           0.148207            1       True         11\n7      RandomForestEntr   0.821229       0.214481  0.381199                0.214481           0.381199            1       True          6\n8      RandomForestGini   0.815642       0.091963  0.328555                0.091963           0.328555            1       True          5\n9            LightGBMXT   0.810056       0.003035  0.204797                0.003035           0.204797            1       True          3\n10       ExtraTreesGini   0.804469       0.184527  0.518099                0.184527           0.518099            1       True          8\n11       ExtraTreesEntr   0.798883       0.179535  0.353106                0.179535           0.353106            1       True          9\n12       KNeighborsUnif   0.653631       0.002189  0.009097                0.002189           0.009097            1       True          1\n13       KNeighborsDist   0.653631       0.002325  0.009845                0.002325           0.009845            1       True          2\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.860335\n0.210158\n3.510778\n0.000527\n0.246271\n2\nTrue\n14\n\n\n1\nNeuralNetTorch\n0.837989\n0.008443\n1.547239\n0.008443\n1.547239\n1\nTrue\n12\n\n\n2\nLightGBMLarge\n0.826816\n0.003162\n0.443558\n0.003162\n0.443558\n1\nTrue\n13\n\n\n3\nCatBoost\n0.826816\n0.003559\n0.661513\n0.003559\n0.661513\n1\nTrue\n7\n\n\n4\nLightGBM\n0.826816\n0.003659\n0.283715\n0.003659\n0.283715\n1\nTrue\n4\n\n\n5\nNeuralNetFastAI\n0.826816\n0.007515\n0.462052\n0.007515\n0.462052\n1\nTrue\n10\n\n\n6\nXGBoost\n0.821229\n0.004702\n0.148207\n0.004702\n0.148207\n1\nTrue\n11\n\n\n7\nRandomForestEntr\n0.821229\n0.214481\n0.381199\n0.214481\n0.381199\n1\nTrue\n6\n\n\n8\nRandomForestGini\n0.815642\n0.091963\n0.328555\n0.091963\n0.328555\n1\nTrue\n5\n\n\n9\nLightGBMXT\n0.810056\n0.003035\n0.204797\n0.003035\n0.204797\n1\nTrue\n3\n\n\n10\nExtraTreesGini\n0.804469\n0.184527\n0.518099\n0.184527\n0.518099\n1\nTrue\n8\n\n\n11\nExtraTreesEntr\n0.798883\n0.179535\n0.353106\n0.179535\n0.353106\n1\nTrue\n9\n\n\n12\nKNeighborsUnif\n0.653631\n0.002189\n0.009097\n0.002189\n0.009097\n1\nTrue\n1\n\n\n13\nKNeighborsDist\n0.653631\n0.002325\n0.009845\n0.002325\n0.009845\n1\nTrue\n2\n\n\n\n\n\n\n\n- validation set의 의미:"
  },
  {
    "objectID": "posts/02wk-007-타이타닉, Autogluon (Fsize,Drop).html#d.-예측-predict",
    "href": "posts/02wk-007-타이타닉, Autogluon (Fsize,Drop).html#d.-예측-predict",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(_tr)).mean()\n\n0.9539842873176206\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(_tst)).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"autogluon(Fsize,Drop)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/02wk-005-타이타닉, Autogluon.html#a.-데이터",
    "href": "posts/02wk-005-타이타닉, Autogluon.html#a.-데이터",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"/kaggle/input/titanic/train.csv\")\ntst = TabularDataset(\"/kaggle/input/titanic/test.csv\")"
  },
  {
    "objectID": "posts/02wk-005-타이타닉, Autogluon.html#b.-predictor-생성",
    "href": "posts/02wk-005-타이타닉, Autogluon.html#b.-predictor-생성",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230910_162629/\""
  },
  {
    "objectID": "posts/02wk-005-타이타닉, Autogluon.html#c.-적합fit",
    "href": "posts/02wk-005-타이타닉, Autogluon.html#c.-적합fit",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(tr) \n# 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230910_162629/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   306.27 GB / 490.57 GB (62.4%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    126262.11 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.2s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    0.09s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8156   = Validation score   (accuracy)\n    0.2s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8212   = Validation score   (accuracy)\n    0.16s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.34s    = Training   runtime\n    0.16s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8156   = Validation score   (accuracy)\n    0.53s    = Training   runtime\n    0.09s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.53s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8156   = Validation score   (accuracy)\n    0.85s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.4s     = Training   runtime\n    0.06s    = Validation runtime\nFitting model: NeuralNetFastAI ...\nNo improvement since epoch 9: early stopping\n    0.8268   = Validation score   (accuracy)\n    1.33s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8101   = Validation score   (accuracy)\n    0.14s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8324   = Validation score   (accuracy)\n    1.49s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8324   = Validation score   (accuracy)\n    0.42s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8603   = Validation score   (accuracy)\n    0.22s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 7.58s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230910_162629/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7fa18f4444f0&gt;\n\n\n- 리더보드확인 (모의고사 채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.860335       0.193071  4.142821                0.000378           0.218161            2       True         14\n1         LightGBMLarge   0.832402       0.003180  0.422871                0.003180           0.422871            1       True         13\n2        NeuralNetTorch   0.832402       0.009243  1.488933                0.009243           1.488933            1       True         12\n3              CatBoost   0.826816       0.003432  0.526762                0.003432           0.526762            1       True          7\n4       NeuralNetFastAI   0.826816       0.007648  1.326021                0.007648           1.326021            1       True         10\n5              LightGBM   0.821229       0.003042  0.157547                0.003042           0.157547            1       True          4\n6            LightGBMXT   0.815642       0.003350  0.197612                0.003350           0.197612            1       True          3\n7        ExtraTreesGini   0.815642       0.050255  0.848478                0.050255           0.848478            1       True          8\n8      RandomForestEntr   0.815642       0.091255  0.525193                0.091255           0.525193            1       True          6\n9      RandomForestGini   0.815642       0.162890  0.340251                0.162890           0.340251            1       True          5\n10              XGBoost   0.810056       0.004424  0.141125                0.004424           0.141125            1       True         11\n11       ExtraTreesEntr   0.810056       0.060458  0.398653                0.060458           0.398653            1       True          9\n12       KNeighborsDist   0.653631       0.001958  0.007848                0.001958           0.007848            1       True          2\n13       KNeighborsUnif   0.653631       0.004457  0.087323                0.004457           0.087323            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.860335\n0.193071\n4.142821\n0.000378\n0.218161\n2\nTrue\n14\n\n\n1\nLightGBMLarge\n0.832402\n0.003180\n0.422871\n0.003180\n0.422871\n1\nTrue\n13\n\n\n2\nNeuralNetTorch\n0.832402\n0.009243\n1.488933\n0.009243\n1.488933\n1\nTrue\n12\n\n\n3\nCatBoost\n0.826816\n0.003432\n0.526762\n0.003432\n0.526762\n1\nTrue\n7\n\n\n4\nNeuralNetFastAI\n0.826816\n0.007648\n1.326021\n0.007648\n1.326021\n1\nTrue\n10\n\n\n5\nLightGBM\n0.821229\n0.003042\n0.157547\n0.003042\n0.157547\n1\nTrue\n4\n\n\n6\nLightGBMXT\n0.815642\n0.003350\n0.197612\n0.003350\n0.197612\n1\nTrue\n3\n\n\n7\nExtraTreesGini\n0.815642\n0.050255\n0.848478\n0.050255\n0.848478\n1\nTrue\n8\n\n\n8\nRandomForestEntr\n0.815642\n0.091255\n0.525193\n0.091255\n0.525193\n1\nTrue\n6\n\n\n9\nRandomForestGini\n0.815642\n0.162890\n0.340251\n0.162890\n0.340251\n1\nTrue\n5\n\n\n10\nXGBoost\n0.810056\n0.004424\n0.141125\n0.004424\n0.141125\n1\nTrue\n11\n\n\n11\nExtraTreesEntr\n0.810056\n0.060458\n0.398653\n0.060458\n0.398653\n1\nTrue\n9\n\n\n12\nKNeighborsDist\n0.653631\n0.001958\n0.007848\n0.001958\n0.007848\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.653631\n0.004457\n0.087323\n0.004457\n0.087323\n1\nTrue\n1"
  },
  {
    "objectID": "posts/02wk-005-타이타닉, Autogluon.html#d.-예측-predict",
    "href": "posts/02wk-005-타이타닉, Autogluon.html#d.-예측-predict",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.9349046015712682\n\n\n\n(tr.Survived == (tr.Sex == \"female\")).mean() # 예전점수와 비교\n\n0.7867564534231201\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(tst)).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"autogluon_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/02wk-008-타이타닉, Autogluon (best_quality).html#a.-데이터",
    "href": "posts/02wk-008-타이타닉, Autogluon (best_quality).html#a.-데이터",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"/kaggle/input/titanic/train.csv\")\ntst = TabularDataset(\"/kaggle/input/titanic/test.csv\")\n\nLoaded data from: ./titanic/train.csv | Columns = 12 / 12 | Rows = 891 -&gt; 891\nLoaded data from: ./titanic/test.csv | Columns = 11 / 11 | Rows = 418 -&gt; 418"
  },
  {
    "objectID": "posts/02wk-008-타이타닉, Autogluon (best_quality).html#b.-predictor-생성",
    "href": "posts/02wk-008-타이타닉, Autogluon (best_quality).html#b.-predictor-생성",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230912_120554/\""
  },
  {
    "objectID": "posts/02wk-008-타이타닉, Autogluon (best_quality).html#c.-적합fit",
    "href": "posts/02wk-008-타이타닉, Autogluon (best_quality).html#c.-적합fit",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(tr,presets='best_quality') # 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nPresets specified: ['best_quality']\nStack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230912_120554/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   293.94 GB / 490.57 GB (59.9%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    125495.28 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.1s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.16s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif_BAG_L1 ...\n    0.6319   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ...\n    0.6364   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.835    = Validation score   (accuracy)\n    0.46s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    0.65s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ...\n    0.8373   = Validation score   (accuracy)\n    0.34s    = Training   runtime\n    0.09s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ...\n    0.8361   = Validation score   (accuracy)\n    0.34s    = Training   runtime\n    0.08s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8608   = Validation score   (accuracy)\n    1.85s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ...\n    0.8294   = Validation score   (accuracy)\n    0.58s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ...\n    0.8328   = Validation score   (accuracy)\n    0.3s     = Training   runtime\n    0.07s    = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8507   = Validation score   (accuracy)\n    2.85s    = Training   runtime\n    0.09s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    1.09s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    4.25s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    1.36s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8608   = Validation score   (accuracy)\n    0.4s     = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 23.18s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230912_120554/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f46e583f8e0&gt;\n\n\n- 리더보드확인 (모의고사채점)\n\npredictr.leaderboard()\n\n                      model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           CatBoost_BAG_L1   0.860831       0.026401  1.851676                0.026401           1.851676            1       True          7\n1       WeightedEnsemble_L2   0.860831       0.027539  2.251745                0.001138           0.400069            2       True         14\n2    NeuralNetFastAI_BAG_L1   0.850730       0.093112  2.852662                0.093112           2.852662            1       True         10\n3      LightGBMLarge_BAG_L1   0.840629       0.029128  1.360948                0.029128           1.360948            1       True         13\n4           LightGBM_BAG_L1   0.840629       0.031956  0.654864                0.031956           0.654864            1       True          4\n5            XGBoost_BAG_L1   0.840629       0.048379  1.094147                0.048379           1.094147            1       True         11\n6     NeuralNetTorch_BAG_L1   0.840629       0.102560  4.247738                0.102560           4.247738            1       True         12\n7   RandomForestGini_BAG_L1   0.837262       0.088506  0.341183                0.088506           0.341183            1       True          5\n8   RandomForestEntr_BAG_L1   0.836139       0.081470  0.341543                0.081470           0.341543            1       True          6\n9         LightGBMXT_BAG_L1   0.835017       0.027167  0.462362                0.027167           0.462362            1       True          3\n10    ExtraTreesEntr_BAG_L1   0.832772       0.073440  0.297061                0.073440           0.297061            1       True          9\n11    ExtraTreesGini_BAG_L1   0.829405       0.097789  0.580898                0.097789           0.580898            1       True          8\n12    KNeighborsDist_BAG_L1   0.636364       0.001740  0.003219                0.001740           0.003219            1       True          2\n13    KNeighborsUnif_BAG_L1   0.631874       0.001945  0.003195                0.001945           0.003195            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nCatBoost_BAG_L1\n0.860831\n0.026401\n1.851676\n0.026401\n1.851676\n1\nTrue\n7\n\n\n1\nWeightedEnsemble_L2\n0.860831\n0.027539\n2.251745\n0.001138\n0.400069\n2\nTrue\n14\n\n\n2\nNeuralNetFastAI_BAG_L1\n0.850730\n0.093112\n2.852662\n0.093112\n2.852662\n1\nTrue\n10\n\n\n3\nLightGBMLarge_BAG_L1\n0.840629\n0.029128\n1.360948\n0.029128\n1.360948\n1\nTrue\n13\n\n\n4\nLightGBM_BAG_L1\n0.840629\n0.031956\n0.654864\n0.031956\n0.654864\n1\nTrue\n4\n\n\n5\nXGBoost_BAG_L1\n0.840629\n0.048379\n1.094147\n0.048379\n1.094147\n1\nTrue\n11\n\n\n6\nNeuralNetTorch_BAG_L1\n0.840629\n0.102560\n4.247738\n0.102560\n4.247738\n1\nTrue\n12\n\n\n7\nRandomForestGini_BAG_L1\n0.837262\n0.088506\n0.341183\n0.088506\n0.341183\n1\nTrue\n5\n\n\n8\nRandomForestEntr_BAG_L1\n0.836139\n0.081470\n0.341543\n0.081470\n0.341543\n1\nTrue\n6\n\n\n9\nLightGBMXT_BAG_L1\n0.835017\n0.027167\n0.462362\n0.027167\n0.462362\n1\nTrue\n3\n\n\n10\nExtraTreesEntr_BAG_L1\n0.832772\n0.073440\n0.297061\n0.073440\n0.297061\n1\nTrue\n9\n\n\n11\nExtraTreesGini_BAG_L1\n0.829405\n0.097789\n0.580898\n0.097789\n0.580898\n1\nTrue\n8\n\n\n12\nKNeighborsDist_BAG_L1\n0.636364\n0.001740\n0.003219\n0.001740\n0.003219\n1\nTrue\n2\n\n\n13\nKNeighborsUnif_BAG_L1\n0.631874\n0.001945\n0.003195\n0.001945\n0.003195\n1\nTrue\n1"
  },
  {
    "objectID": "posts/02wk-008-타이타닉, Autogluon (best_quality).html#d.-예측-predict",
    "href": "posts/02wk-008-타이타닉, Autogluon (best_quality).html#d.-예측-predict",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.898989898989899\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(tst)).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"autogluon(best_quality)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/01wk-002-타이타닉, 데이터의 이해.html#a.-gender_submission.csv",
    "href": "posts/01wk-002-타이타닉, 데이터의 이해.html#a.-gender_submission.csv",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "A. gender_submission.csv",
    "text": "A. gender_submission.csv\n- 왜 여자만 살까?\n\n\n\n\n\n\nGTP 답변\n\n\n\ngender_submission.csv파일은 단순히 모든 여성 승객이 생존한다고 가정한 예측을 담은 파일로, 실제 데이터에 기반한 예측이 아닙니다. 이 파일은 제출 파일의 형식과 구성을 보여주기 위한 예시입니다. 실제로는 성별이나 다른 특성에 따라 생존 여부가 결정되는 것이 아닌, 여러 다양한 변수들이 생존에 영향을 미치게 됩니다.\n실제로 타이타닉호에서는 여성들이 생존율이 높았습니다. 타이타닉호 침몰 사건 당시 여성과 아이들을 먼저 구조하고 구명보트에 태우는 등의 우선순위가 있었기 때문입니다. 따라서 “gender_submission.csv” 파일은 예시로서 제공되었고, 실제 예측 모델을 개발할 때에는 여러 다른 변수들도 고려하여 예측해야 합니다."
  },
  {
    "objectID": "posts/01wk-002-타이타닉, 데이터의 이해.html#b.-embarked",
    "href": "posts/01wk-002-타이타닉, 데이터의 이해.html#b.-embarked",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "B. embarked",
    "text": "B. embarked\n- 항구는 승객의 배경과 사회적 상황을 반영할 수 있다.\n\n\n\n\n\n\nGPT 답변\n\n\n\n네, 항구별로 승객들의 배경과 사회적 상황이 다를 수 있습니다. 다양한 항구에서 탑승한 승객들은 지역적, 경제적, 문화적인 차이를 반영할 수 있습니다. 몇 가지 예시를 들어보겠습니다:\n\n경제적 차이: 서로 다른 항구에서 탑승한 승객들은 다른 경제적 배경을 가지고 있을 수 있습니다. 상류층 승객이나 부유한 승객들이 탑승한 항구일수록 1등급 티켓을 선택한 경우가 많을 것입니다.\n문화적 차이: 다른 항구에서 탑승한 승객들은 다른 문화적 배경을 가지고 있을 수 있습니다. 이로 인해 승객들의 습관, 언어, 관습 등이 다를 수 있습니다.\n지역적 차이: 항구의 위치와 지역적 특성에 따라 승객들의 배경이 달라질 수 있습니다. 해당 지역의 주요 산업이나 경제 활동에 따라 승객들의 직업이나 사회적 지위가 다를 수 있습니다.\n가족 관계: 특정 항구에서 가족들이 함께 탑승한 경우, 해당 지역의 가족 구조와 관련된 차이가 반영될 수 있습니다.\n\n이러한 다양한 차이를 분석하여 항구별로 생존율이나 다른 변수들과의 상관 관계를 파악할 수 있습니다. 이는 모델링 과정에서 추가적인 특성 엔지니어링이나 분석에 활용될 수 있습니다.\n\n\n- 항구는 국적에 대한 정보를 내포할 수도 있다.\n\n\n\n\n\n\nGPT 답변\n\n\n\n타이타닉호의 데이터에서는 승객들의 국적에 대한 직접적인 정보는 제공되지 않습니다. 따라서 승객들의 국적에 대한 정보를 확인하는 것은 어렵습니다. 그러나 항구별로 승객들의 출발지나 탑승 도시에 따라서 일부 국적적 특성이 반영될 수는 있습니다.\n예를 들어, Cherbourg 항구가 프랑스에 위치하며, Queenstown 항구가 아일랜드에 위치해 있습니다. 따라서 해당 항구에서 탑승한 승객들 중 일부는 해당 국가의 국적을 가질 가능성이 높을 것입니다. 그러나 국적은 항상 탑승 항구와 일치하지 않을 수 있으며, 탑승한 승객들의 다양한 출신지와 배경을 고려해야 합니다.\n요약하면, 타이타닉호 데이터에서는 직접적인 국적 정보는 제공되지 않지만, 항구별로 승객들의 출신지나 탑승 도시에 따라서 일부 국적적 특성이 반영될 수 있습니다."
  },
  {
    "objectID": "posts/01wk-002-타이타닉, 데이터의 이해.html#c.-sibsp와-parch에-대한-피처엔지니어링-아이디어",
    "href": "posts/01wk-002-타이타닉, 데이터의 이해.html#c.-sibsp와-parch에-대한-피처엔지니어링-아이디어",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "C. sibsp와 parch에 대한 피처엔지니어링 아이디어",
    "text": "C. sibsp와 parch에 대한 피처엔지니어링 아이디어\n- 두 변수 sibsp와 parch를 합쳐 family_size라는 하나의 변수를 만들고, 이를 활용하여 y를 예측할 수 있다.\n\n데이터를 봐야 하겠지만 가족단위로 보트를 줬을 확률도 큼\n가족단위로 보트를 실제로 주었을 경우 family_size라는 변수는 매우 의미있는 해석이 됨\n\n\n\n\n\n\n\nGPT 답변\n\n\n\n가족 규모 특성 생성: ‘sibsp’와 ’parch’ 변수를 활용하여 승객의 가족 규모를 나타내는 새로운 변수를 만들 수 있습니다. 예를 들어, ‘sibsp’와 ’parch’ 값을 더한 후 1을 더해 ‘family_size’ 변수로 만들 수 있습니다. 이를 통해 가족 규모가 큰 승객이 생존 여부에 영향을 미칠 수 있는지 확인할 수 있습니다."
  },
  {
    "objectID": "posts/01wk-002-타이타닉, 데이터의 이해.html#d.-embarked와-class에-대한-피처엔지니어링의-아이디어",
    "href": "posts/01wk-002-타이타닉, 데이터의 이해.html#d.-embarked와-class에-대한-피처엔지니어링의-아이디어",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "D. embarked와 class에 대한 피처엔지니어링의 아이디어",
    "text": "D. embarked와 class에 대한 피처엔지니어링의 아이디어\n- 경우에 따라서 두가지 범주를 하나도 합쳐서 만든 새로운 변수가 매우 의미있을 수 있음\n\n경우에 따라서 특정 국적 (프랑스)의 1등급 손님이 구명보트를 많이 얻었을 수도 있음.\n이럴경우 embarked와 class를 독립적으로 생각하는것보다 둘을 합친 변수가 훨씬 유의미한 변수임\n\n\n\n\n\n\n\nGPT 답변\n\n\n\n물론, 자세한 설명을 드리겠습니다. “항구와 클래스의 조합 특성 생성”이라는 개념은 ‘embarked’ (탑승 항구)와 ‘pclass’ (티켓 클래스) 두 변수를 활용하여 새로운 특성을 만들어 분석하는 것을 의미합니다.\n예를 들어, “embarked” 변수에는 ‘C’, ‘Q’, ‘S’와 같은 항구 정보가 있으며, “pclass” 변수에는 ’1’, ‘2’, ’3’과 같은 티켓 클래스 정보가 있습니다. 이 두 변수를 조합하여 새로운 변수를 만들어보겠습니다.\n생성할 새로운 변수의 예시:\n“embarked_pclass”: ‘C_1’, ‘C_2’, ‘C_3’, ‘Q_1’, ‘Q_2’, ‘Q_3’, ‘S_1’, ‘S_2’, ‘S_3’ 위와 같이 “embarked”와 “pclass”를 조합하여 새로운 변수 값을 만듭니다. 예를 들어 ’embarked’가 ’C’이고 ’pclass’가 ’1’인 경우, “embarked_pclass” 변수 값은 ’C_1’이 됩니다. 이렇게 조합된 변수는 해당 승객이 어떤 항구에서 탑승했고 어떤 티켓 클래스를 사용했는지를 동시에 나타내는 특성입니다.\n이렇게 생성된 “embarked_pclass” 변수를 사용하여 특정 항구와 클래스의 조합이 생존 여부에 어떤 영향을 미치는지를 분석할 수 있습니다. 예를 들어, ‘C_1’ 조합이 생존율과 관련이 있는지, ‘Q_3’ 조합이 생존 여부에 영향을 미치는지 등을 확인하여 해당 특성이 모델링에 유용한 정보를 제공하는지 알아볼 수 있습니다.\n\n\n\n\n\n\n\n\nGPT 답변 2 – 그냥 “embarked” 변수와 “pclass” 변수를 각각 넣고 분석하는 것과 무슨차이냐는 질문에 대한 답변\n\n\n\n네, “embarked” 변수와 “pclass” 변수를 각각 넣고 분석하는 것도 가능합니다. 그러나 새로운 변수를 만드는 것은 데이터를 조금 더 유용하게 활용하는 방법 중 하나입니다. 여기에 몇 가지 이유가 있습니다:\n상호작용 효과 강조: 새로운 변수를 만들면 “embarked”와 “pclass”의 조합이 생존 여부에 미치는 영향을 강조할 수 있습니다. 단순히 각각의 변수를 개별적으로 사용하는 것보다 조합된 변수를 통해 서로 간의 상호작용 효과를 쉽게 확인할 수 있습니다.\n더 풍부한 패턴 탐색: 새로운 변수를 만들면 서로 다른 항구와 클래스 조합들 간의 생존률 차이를 더 세부적으로 분석할 수 있습니다. 이를 통해 특정 항구와 클래스의 승객들이 생존 여부에 미치는 영향을 더 자세히 이해할 수 있습니다.\n다양한 조합 고려: 조합된 변수를 만들면 “embarked”와 “pclass”의 가능한 모든 조합을 커버할 수 있습니다. 이는 분석의 전체적인 시각을 확장하고, 어떤 조합이 중요한지를 더 효과적으로 파악할 수 있도록 도와줍니다.\n모델 학습에 더 적합한 데이터 생성: 몇몇 머신 러닝 알고리즘은 상호작용 변수를 활용하여 모델의 성능을 향상시킬 수 있습니다. 이러한 알고리즘들은 새로운 변수를 활용하여 더 복잡한 패턴을 학습할 수 있습니다.\n새로운 변수를 만들어 분석하는 것은 분석의 창의성과 도메인 지식을 활용하여 데이터로부터 더 많은 의미를 도출하려는 노력의 일부입니다. 분석의 목적에 따라 각 변수를 개별적으로 사용하거나 조합하여 새로운 변수를 만들 수 있습니다."
  }
]