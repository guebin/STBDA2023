[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "빅데이터분석 특강 (2023)",
    "section": "",
    "text": "질문하는 방법\n\n카카오톡: 질문하러 가기 // 학기종료이후 폐쇄함\n이메일: guebin@jbnu.ac.kr\n직접방문: 자연과학대학 본관 205호\nZoom: 카카오톡이나 이메일로 미리 시간을 정할 것\nLMS쪽지: https://ieilms.jbnu.ac.kr/\n\n공지사항\n\n강의영상의 경우 시간이 지나면 고화질로 시청할 수 있습니다. (인코딩중에는 유튜브에서 저화질로 송출됩니다)\n\n강의노트\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nOct 10, 2023\n\n\nA2: 코드스페이스, 리눅스명령어, kaggle.json\n\n\n최규빈 \n\n\n\n\nSep 12, 2000\n\n\nA1: Quarto Blog 만들기\n\n\n최규빈 \n\n\n\n\nNov 1, 2023\n\n\n09wk-mid\n\n\n최규빈 \n\n\n\n\nOct 24, 2023\n\n\n08wk-037: 중간점검\n\n\n최규빈 \n\n\n\n\nOct 16, 2023\n\n\n07wk-035: 아이스크림(이상치) / 의사결정나무\n\n\n최규빈 \n\n\n\n\nOct 16, 2023\n\n\n07wk-034: 취업(오버피팅) / 의사결정나무\n\n\n최규빈 \n\n\n\n\nOct 17, 2023\n\n\n07wk-033: 취업(다중공선성) / 의사결정나무\n\n\n최규빈 \n\n\n\n\nOct 17, 2023\n\n\n07wk-032: 아이스크림(교호작용) / 의사결정나무\n\n\n최규빈 \n\n\n\n\nOct 17, 2023\n\n\n07wk-031: 체중감량(교호작용) / 의사결정나무\n\n\n최규빈 \n\n\n\n\nOct 17, 2023\n\n\n07wk-030: 아이스크림(교호작용) / 회귀분석\n\n\n최규빈 \n\n\n\n\nOct 17, 2023\n\n\n07wk-029: 체중감량(교호작용) / 회귀분석 – 추가해설\n\n\n최규빈 \n\n\n\n\nOct 17, 2023\n\n\n07wk-028: 선형모형의 적\n\n\n최규빈 \n\n\n\n\nOct 17, 2023\n\n\n07wk-027: 아이스크림(이상치) / 회귀분석 – 추가해설\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n06wk-026: 취업(다중공선성) / LassoCV\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n06wk-025: 취업(다중공선성) / Lasso\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n06wk-024: 취업(다중공선성) / RidgeCV\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n06wk-023: 취업(다중공선성) / Ridge – 추가해설\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n06wk-022: 취업(다중공선성) / 다중공선성의 개념 – 추가해설\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n05wk-021: 취업(오버피팅) / 오버피팅의 개념 – 추가해설\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n05wk-020: StandardScaler를 이용한 전처리\n\n\n최규빈 \n\n\n\n\nOct 5, 2023\n\n\n05wk-019: MinMaxScaler를 이용한 전처리\n\n\n최규빈 \n\n\n\n\nSep 26, 2023\n\n\n04wk-018: Predictor의 깊은 이해, 기호정리\n\n\n최규빈 \n\n\n\n\nSep 26, 2023\n\n\n04wk-017: 취업 / 로지스틱의 깊은 이해\n\n\n최규빈 \n\n\n\n\nSep 26, 2023\n\n\n04wk-016: 타이타닉 / 결측치처리+로지스틱 – 추가해설\n\n\n최규빈 \n\n\n\n\nSep 26, 2023\n\n\n04wk-015: sklearn.impute를 이용한 결측치 처리\n\n\n최규빈 \n\n\n\n\nSep 26, 2023\n\n\n04wk-014: msno를 이용한 결측치 시각화\n\n\n최규빈 \n\n\n\n\nSep 21, 2023\n\n\n03wk-013: 타이타닉 / 로지스틱 – 추가해설\n\n\n최규빈 \n\n\n\n\nSep 21, 2023\n\n\n03wk-012: 취업 / 로지스틱 – 추가해설\n\n\n최규빈 \n\n\n\n\nSep 21, 2023\n\n\n03wk-011: Medical Cost / 회귀분석\n\n\n최규빈 \n\n\n\n\nSep 21, 2023\n\n\n03wk-010: 아이스크림(범주) / 회귀분석 – 추가해설\n\n\n최규빈 \n\n\n\n\nSep 21, 2023\n\n\n03wk-009: 아이스크림 / 회귀분석 – 추가해설\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-008: 타이타닉 / Autogluon (best_quality)\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-007: 타이타닉 / Autogluon (Fsize,Drop)\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-006: 타이타닉 / Autogluon (Fsize)\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-005: 타이타닉 / Autogluon\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-004: 타이타닉 / Alexis Cook의 코드\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-003: 타이타닉 / 첫 제출\n\n\n최규빈 \n\n\n\n\nSep 5, 2023\n\n\n01wk-002: 타이타닉 데이터의 이해 – 추가해설\n\n\n최규빈 \n\n\n\n\nSep 5, 2023\n\n\n01wk-001: 캐글소개\n\n\n최규빈 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/04wk-015.html#a.-숫자형자료의-impute",
    "href": "posts/04wk-015.html#a.-숫자형자료의-impute",
    "title": "04wk-015: sklearn.impute를 이용한 결측치 처리",
    "section": "A. 숫자형자료의 impute",
    "text": "A. 숫자형자료의 impute\n- 주어진자료\n\ndf = pd.DataFrame({'A':[2.1,1.9,2.2,np.nan,1.9], 'B':[0,0,np.nan,0,0]})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.1\n0.0\n\n\n1\n1.9\n0.0\n\n\n2\n2.2\nNaN\n\n\n3\nNaN\n0.0\n\n\n4\n1.9\n0.0\n\n\n\n\n\n\n\n- 빈칸은 대충 아래와 같이 추정하면 되지 않을까?\n\ndf.loc[3,'A'] = df.A.mean()\ndf.loc[2,'B'] = df.B.mean()\n\n\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.100\n0.0\n\n\n1\n1.900\n0.0\n\n\n2\n2.200\n0.0\n\n\n3\n2.025\n0.0\n\n\n4\n1.900\n0.0\n\n\n\n\n\n\n\n- 자동으로 하려면?\n\ndf = pd.DataFrame({'A':[2.1,1.9,2.2,np.nan,1.9], 'B':[0,0,np.nan,0,0]})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.1\n0.0\n\n\n1\n1.9\n0.0\n\n\n2\n2.2\nNaN\n\n\n3\nNaN\n0.0\n\n\n4\n1.9\n0.0\n\n\n\n\n\n\n\n(방법1)\n\nimputer = sklearn.impute.SimpleImputer()\nimputer.fit(df)\nimputer.transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n(방법2)\n\nimputer = sklearn.impute.SimpleImputer()\nimputer.fit_transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n- 다른방식으로 결측값 대체\n(방법1) – 평균으로 대체\n\nimputer = sklearn.impute.SimpleImputer(strategy='mean')\nimputer.fit_transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n(방법2) – 중앙값으로 대체\n\nimputer = sklearn.impute.SimpleImputer(strategy='median')\nimputer.fit_transform(df)\n\narray([[2.1, 0. ],\n       [1.9, 0. ],\n       [2.2, 0. ],\n       [2. , 0. ],\n       [1.9, 0. ]])\n\n\n(방법3) – 최빈값으로 대체\n\nimputer = sklearn.impute.SimpleImputer(strategy='most_frequent')\nimputer.fit_transform(df)\n\narray([[2.1, 0. ],\n       [1.9, 0. ],\n       [2.2, 0. ],\n       [1.9, 0. ],\n       [1.9, 0. ]])\n\n\n(방법4) – 상수대체\n\nimputer = sklearn.impute.SimpleImputer(strategy='constant',fill_value=-999)\nimputer.fit_transform(df)\n\narray([[   2.1,    0. ],\n       [   1.9,    0. ],\n       [   2.2, -999. ],\n       [-999. ,    0. ],\n       [   1.9,    0. ]])"
  },
  {
    "objectID": "posts/04wk-015.html#b.-범주형자료의-impute",
    "href": "posts/04wk-015.html#b.-범주형자료의-impute",
    "title": "04wk-015: sklearn.impute를 이용한 결측치 처리",
    "section": "B. 범주형자료의 impute",
    "text": "B. 범주형자료의 impute\n- 자료\n\ndf = pd.DataFrame({'A':['Y','N','Y','Y',np.nan], 'B':['stat','math',np.nan,'stat','bio']})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nY\nstat\n\n\n1\nN\nmath\n\n\n2\nY\nNaN\n\n\n3\nY\nstat\n\n\n4\nNaN\nbio\n\n\n\n\n\n\n\n- 최빈값 혹은 상수대체만 가능\n(방법1) – 최빈값을 이용\n\nimptr = sklearn.impute.SimpleImputer(strategy='most_frequent')\nimptr.fit_transform(df)\n\narray([['Y', 'stat'],\n       ['N', 'math'],\n       ['Y', 'stat'],\n       ['Y', 'stat'],\n       ['Y', 'bio']], dtype=object)\n\n\n(방법2) – 상수로 대체함\n\nimptr1 = sklearn.impute.SimpleImputer(strategy='constant',fill_value='Y')\nimptr1.fit_transform(df[['A']])\nimptr2 = sklearn.impute.SimpleImputer(strategy='constant',fill_value='math')\nimptr2.fit_transform(df[['B']])\n\narray([['stat'],\n       ['math'],\n       ['math'],\n       ['stat'],\n       ['bio']], dtype=object)\n\n\n\nnp.concatenate([imptr1.fit_transform(df[['A']]),imptr2.fit_transform(df[['B']])],axis=1)\n\narray([['Y', 'stat'],\n       ['N', 'math'],\n       ['Y', 'math'],\n       ['Y', 'stat'],\n       ['Y', 'bio']], dtype=object)"
  },
  {
    "objectID": "posts/04wk-015.html#c.-혼합형자료의-impute-1-모두-최빈값으로-impute",
    "href": "posts/04wk-015.html#c.-혼합형자료의-impute-1-모두-최빈값으로-impute",
    "title": "04wk-015: sklearn.impute를 이용한 결측치 처리",
    "section": "C. 혼합형자료의 impute – (1) 모두 최빈값으로 impute",
    "text": "C. 혼합형자료의 impute – (1) 모두 최빈값으로 impute\n# 예제: 아래의 df에서 결측치를 모두 최빈값으로 impute하라.\n\ndf = pd.DataFrame(\n    {'A':[2.1,1.9,2.2,np.nan,1.9],\n     'B':[0,0,np.nan,0,0],\n     'C':['Y','N','Y','Y',np.nan], \n     'D':['stat','math',np.nan,'stat','bio']}\n)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n2.1\n0.0\nY\nstat\n\n\n1\n1.9\n0.0\nN\nmath\n\n\n2\n2.2\nNaN\nY\nNaN\n\n\n3\nNaN\n0.0\nY\nstat\n\n\n4\n1.9\n0.0\nNaN\nbio\n\n\n\n\n\n\n\n(풀이)\n\nimptr = sklearn.impute.SimpleImputer(strategy='most_frequent')\nimptr.fit_transform(df)\n\narray([[2.1, 0.0, 'Y', 'stat'],\n       [1.9, 0.0, 'N', 'math'],\n       [2.2, 0.0, 'Y', 'stat'],\n       [1.9, 0.0, 'Y', 'stat'],\n       [1.9, 0.0, 'Y', 'bio']], dtype=object)\n\n\n#"
  },
  {
    "objectID": "posts/04wk-015.html#d.-혼합형자료의-impute-2-숫자형은-평균값으로-범주는-최빈값으로-impute",
    "href": "posts/04wk-015.html#d.-혼합형자료의-impute-2-숫자형은-평균값으로-범주는-최빈값으로-impute",
    "title": "04wk-015: sklearn.impute를 이용한 결측치 처리",
    "section": "D. 혼합형자료의 impute – (2) 숫자형은 평균값으로, 범주는 최빈값으로 impute",
    "text": "D. 혼합형자료의 impute – (2) 숫자형은 평균값으로, 범주는 최빈값으로 impute\n# 예제: 아래의 df를 숫자형일 경우는 평균대치, 문자형일 경우는 최빈값으로 대치하라.\n\ndf = pd.DataFrame(\n    {'A':[2.1,1.9,2.2,np.nan,1.9],\n     'B':[0,0,np.nan,0,0],\n     'C':['Y','N','Y','Y',np.nan], \n     'D':['stat','math',np.nan,'stat','bio']}\n)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n2.1\n0.0\nY\nstat\n\n\n1\n1.9\n0.0\nN\nmath\n\n\n2\n2.2\nNaN\nY\nNaN\n\n\n3\nNaN\n0.0\nY\nstat\n\n\n4\n1.9\n0.0\nNaN\nbio\n\n\n\n\n\n\n\n(풀이)\n- step1: 복사본 생성\n\ndf_imputed = df.copy()\ndf_imputed\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n2.1\n0.0\nY\nstat\n\n\n1\n1.9\n0.0\nN\nmath\n\n\n2\n2.2\nNaN\nY\nNaN\n\n\n3\nNaN\n0.0\nY\nstat\n\n\n4\n1.9\n0.0\nNaN\nbio\n\n\n\n\n\n\n\n- step2: 데이터프레임 분리\n\ndf_num = df.select_dtypes(include=\"number\")\ndf_num\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.1\n0.0\n\n\n1\n1.9\n0.0\n\n\n2\n2.2\nNaN\n\n\n3\nNaN\n0.0\n\n\n4\n1.9\n0.0\n\n\n\n\n\n\n\n\ndf_cat = df.select_dtypes(exclude=\"number\")\ndf_cat \n\n\n\n\n\n\n\n\nC\nD\n\n\n\n\n0\nY\nstat\n\n\n1\nN\nmath\n\n\n2\nY\nNaN\n\n\n3\nY\nstat\n\n\n4\nNaN\nbio\n\n\n\n\n\n\n\n- step3: impute\n\ndf_imputed[df_num.columns] = sklearn.impute.SimpleImputer(strategy='mean').fit_transform(df_num)\ndf_imputed[df_cat.columns] = sklearn.impute.SimpleImputer(strategy='most_frequent').fit_transform(df_cat)\n\n\ndf_imputed\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n2.100\n0.0\nY\nstat\n\n\n1\n1.900\n0.0\nN\nmath\n\n\n2\n2.200\n0.0\nY\nstat\n\n\n3\n2.025\n0.0\nY\nstat\n\n\n4\n1.900\n0.0\nY\nbio"
  },
  {
    "objectID": "posts/05wk-019.html#a.-모티브",
    "href": "posts/05wk-019.html#a.-모티브",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "A. 모티브",
    "text": "A. 모티브\n- 예제자료 로드\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv').loc[:7,['toeic','gpa']]\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n5\n65\n1.846885\n\n\n6\n290\n0.309928\n\n\n7\n730\n0.336081\n\n\n\n\n\n\n\n- 모형을 돌려보고 해석한 결과\nu = X.toeic*0.00571598 + X.gpa*2.46520018 -8.45433334\nv = 1/(1+np.exp(-u))\nv # 확률같은것임\n\n토익이 중요해? 아니면 학점이 중요해?\n얼만큼 중요해?\n\n- 모티브: 토익점수를 0-1사이로 맞추고 gpa도 0-1사이로 맞추면 해석이 쉽지 않을까?"
  },
  {
    "objectID": "posts/05wk-019.html#b.-사용방법",
    "href": "posts/05wk-019.html#b.-사용방법",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "B. 사용방법",
    "text": "B. 사용방법\n- 스케일러 생성\n\nsclr = sklearn.preprocessing.MinMaxScaler()\n\n- fit, transform\n\nsclr.fit(df)\n\nMinMaxScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MinMaxScalerMinMaxScaler()\n\n\n\nsclr.transform(df)\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])\n\n\n- fit_transform\n\nsclr.fit_transform(df)\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])"
  },
  {
    "objectID": "posts/05wk-019.html#c.-잘못된-사용",
    "href": "posts/05wk-019.html#c.-잘못된-사용",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "C. 잘못된 사용",
    "text": "C. 잘못된 사용\n- sclr.fit()와 sclr.fit_transform()은 입력으로 2차원 자료구조를 기대한다. (그중에서도 은근히 numpy array를 기대함)\n\nsclr.fit_transform(df['toeic']) # df['toeic']는 1차원 자료구조\n\nValueError: Expected 2D array, got 1D array instead:\narray=[135. 935. 485.  65. 445.  65. 290. 730.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n\n\n\nsclr.fit_transform(df[['toeic']]) # df[['toeic']]는 2차원 자료구조\n\narray([[0.08045977],\n       [1.        ],\n       [0.48275862],\n       [0.        ],\n       [0.43678161],\n       [0.        ],\n       [0.25862069],\n       [0.76436782]])"
  },
  {
    "objectID": "posts/05wk-019.html#a.-사용방법",
    "href": "posts/05wk-019.html#a.-사용방법",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "A. 사용방법",
    "text": "A. 사용방법\n- scaler를 오브젝트로 따로 만들지 않고 함수형으로 구현\n\nsklearn.preprocessing.minmax_scale(df)\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])\n\n\n- 이것은 심지어 1차원 자료구조에도 적용가능하다.\n\nsklearn.preprocessing.minmax_scale(df['toeic'])\n\narray([0.08045977, 1.        , 0.48275862, 0.        , 0.43678161,\n       0.        , 0.25862069, 0.76436782])\n\n\n- 열별로 스케일링을 하는게 아니라 행별로 스케일링을 하는 것도 가능하다. (여기서는 필요없지만..)\n\nsklearn.preprocessing.minmax_scale(df,axis=1)\n\narray([[1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.]])"
  },
  {
    "objectID": "posts/05wk-019.html#b.-discussions",
    "href": "posts/05wk-019.html#b.-discussions",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "B. discussions",
    "text": "B. discussions\n- 언뜻 보기에는 MinMaxScaler 보다 minmax_scale이 좋아보이는데, 생각보다 일반적으로 minmax_scale을 사용하지는 않음. 이유는 아래와 같음.\n\n파이썬을 쓰는 사람들이 함수형 접근방식보다 객체지향 접근방식을 선호한다. (이건 제생각)\n학습데이터와 테스트데이터의 스케일링시 동일한 변환을 유지하는 상황에서는 MinMaxScaler 가 유리함.\ninverse_transform 메서드를 같은 부가기능을 제공함."
  },
  {
    "objectID": "posts/05wk-019.html#a.-잘못된-스케일링-방법-비효율의-문제",
    "href": "posts/05wk-019.html#a.-잘못된-스케일링-방법-비효율의-문제",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "A. 잘못된 스케일링 방법 – 비효율의 문제",
    "text": "A. 잘못된 스케일링 방법 – 비효율의 문제\n\nsklearn.preprocessing.minmax_scale(X)\n\narray([[0.  ],\n       [0.25],\n       [0.5 ],\n       [0.75],\n       [1.  ]])\n\n\n\nsklearn.preprocessing.minmax_scale(XX)\n\narray([[0. ],\n       [0.5],\n       [1. ]])\n\n\n- 이 방법은 전략적으로 비효율적인 문제이지, 치팅과 관련된 치명적인 잘못은 아니다.\n\n만약에 어떠한 경우에 이러한 전처리 방식이 오히려 전략적이라고 판단될 경우 사용할수도 있음."
  },
  {
    "objectID": "posts/05wk-019.html#b.-올바른-스케일링-방법",
    "href": "posts/05wk-019.html#b.-올바른-스케일링-방법",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "B. 올바른 스케일링 방법",
    "text": "B. 올바른 스케일링 방법\n- 방법1\n\nsclr = sklearn.preprocessing.MinMaxScaler()\nsclr.fit(X) # \n\nMinMaxScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MinMaxScalerMinMaxScaler()\n\n\n\nsclr.transform(X)\n\narray([[0.  ],\n       [0.25],\n       [0.5 ],\n       [0.75],\n       [1.  ]])\n\n\n\nsclr.transform(XX)\n\narray([[0.125],\n       [0.375],\n       [0.625]])\n\n\n- 방법2\n\nsclr = sklearn.preprocessing.MinMaxScaler()\nsclr.fit_transform(X)\n\narray([[0.  ],\n       [0.25],\n       [0.5 ],\n       [0.75],\n       [1.  ]])\n\n\n\nsclr.transform(XX)\n\narray([[0.125],\n       [0.375],\n       [0.625]])"
  },
  {
    "objectID": "posts/05wk-019.html#c.-scaled_value-in-01",
    "href": "posts/05wk-019.html#c.-scaled_value-in-01",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "C. scaled_value \\(\\in\\) [0,1]?",
    "text": "C. scaled_value \\(\\in\\) [0,1]?\n- 주어진 자료가 아래와 같다고 하자.\n\nX = np.array([1.0, 2.0, 3.0, 4.0, 3.5]).reshape(-1,1)\nXX = np.array([1.5, 2.5, 5.0]).reshape(-1,1)\n\n\nsclr = sklearn.preprocessing.MinMaxScaler()\nsclr.fit_transform(X)\nsclr.transform(XX)\n\narray([[0.16666667],\n       [0.5       ],\n       [1.33333333]])\n\n\n\n스케일링된 값이 1보다 클 수도 있다."
  },
  {
    "objectID": "posts/05wk-019.html#d.-아주-잘못된-스케일링-방법-정보누수",
    "href": "posts/05wk-019.html#d.-아주-잘못된-스케일링-방법-정보누수",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "D. 아주 잘못된 스케일링 방법 – 정보누수",
    "text": "D. 아주 잘못된 스케일링 방법 – 정보누수\n- 주어진 자료가 아래와 같다고 하자.\n\nX = np.array([1.0, 2.0, 3.0, 4.0, 3.5]).reshape(-1,1)\nXX = np.array([1.5, 2.5, 5.0]).reshape(-1,1)\n\n- 데이터를 합친다.. (미쳤어??)\n\nnp.concatenate([X,XX])\n\narray([[1. ],\n       [2. ],\n       [3. ],\n       [4. ],\n       [3.5],\n       [1.5],\n       [2.5],\n       [5. ]])\n\n\n- 합친데이터에서 스케일링\n\nsklearn.preprocessing.minmax_scale(np.concatenate([X,XX]))[:5]\n\narray([[0.   ],\n       [0.25 ],\n       [0.5  ],\n       [0.75 ],\n       [0.625]])\n\n\n- 이러한 전처리 방식을 정보누수라고 한다. (대회 규정에 따라서 탈락사유에 해당함)"
  },
  {
    "objectID": "posts/06wk-026.html",
    "href": "posts/06wk-026.html",
    "title": "06wk-026: 취업(다중공선성) / LassoCV",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.linear_model\nimport matplotlib.pyplot as plt\n\n\n\n3. Data\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nnp.random.seed(43052)\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\n\n\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\n\n4. LassoCV\n- LassoCV 클래스에서 모형을 선택해보자.\n\n## step1\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.LassoCV(alphas= np.linspace(0,2,100))\n## step3\npredictr.fit(X,y)\n## step4 -- pass\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.256e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.652e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.606e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.694e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.950e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.776e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.379e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.839e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.182e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.368e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.157e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.121e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.062e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.033e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.667e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.202e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.410e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.363e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.107e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.023e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.036e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.050e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.043e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.022e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.211e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.432e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.682e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.873e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.890e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.955e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.133e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.278e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.196e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.822e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.619e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.615e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.663e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.716e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.874e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.076e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.567e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.048e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.629e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.070e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.461e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.806e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.742e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.590e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.535e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.552e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.512e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.746e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.771e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.253e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.761e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.898e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.657e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.876e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.096e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.101e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.186e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.907e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.861e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.073e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.116e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.124e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.071e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.424e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.659e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.496e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.021e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.055e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.085e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.212e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.339e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.503e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.684e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.803e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.897e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.991e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.252e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.602e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.907e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.098e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.392e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.630e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.790e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.793e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.675e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.340e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.623e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.670e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.597e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.611e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.417e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.999e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.284e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.148e+00, tolerance: 2.707e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.771e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.263e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.173e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.671e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.317e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.372e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.132e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.367e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.155e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.074e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.322e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.089e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.867e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.038e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.063e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.028e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.531e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.686e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.031e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.075e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.099e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.129e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.146e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.089e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.458e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.637e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.744e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.979e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.371e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.905e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.939e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.322e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.215e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.838e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.373e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.925e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.559e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.174e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.236e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.877e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.641e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.404e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.174e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.833e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.358e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.812e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.197e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.644e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.129e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.555e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.854e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.064e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.141e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.198e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.247e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.240e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.183e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.156e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.116e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.078e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.186e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.308e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.426e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.755e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.950e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.141e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.279e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.344e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.413e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.430e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.501e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.526e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.518e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.807e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.050e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.138e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.158e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.185e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.238e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.341e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.402e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.280e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.150e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.173e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.104e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.067e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.970e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.733e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.357e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.054e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.661e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.076e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.290e+00, tolerance: 2.707e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.021e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.739e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.749e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.104e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.254e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.181e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.324e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.057e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.330e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.041e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.544e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.969e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.370e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.501e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.239e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.585e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.942e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.005e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.750e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.008e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.097e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.320e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.642e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.913e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.018e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.223e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.570e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.774e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.292e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.959e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.804e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.822e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.961e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.097e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.040e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.837e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.006e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.174e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.087e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.304e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.659e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.993e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.153e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.229e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.118e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.808e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.491e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.042e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.231e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.454e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.437e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.412e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.513e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.981e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.463e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.113e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.770e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.355e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.757e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.891e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.036e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.085e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.244e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.422e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.639e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.822e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.914e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.006e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.216e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.495e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.764e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.933e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.952e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.845e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.643e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.445e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.606e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.783e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.891e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.877e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.721e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.539e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.457e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.476e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.625e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.708e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.620e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.362e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.950e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.254e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.393e+00, tolerance: 2.670e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.197e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.270e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.127e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.962e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.102e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.142e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.889e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.129e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.839e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.037e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.187e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.159e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.009e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.035e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.507e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.887e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.061e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.291e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.625e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.850e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.891e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.744e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.504e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.189e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.201e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.821e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.064e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.220e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.425e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.580e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.699e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.673e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.553e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.360e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.118e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.063e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.166e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.150e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.074e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.933e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.788e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.624e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.675e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.796e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.642e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.234e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.569e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.900e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.798e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.457e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.823e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.293e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.609e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.510e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.789e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.410e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.038e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.209e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.365e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.482e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.639e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.782e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.862e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.911e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.929e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.003e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.056e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.040e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.035e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.009e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.973e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.105e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.305e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.590e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.849e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.026e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.989e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.921e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.077e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.181e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.226e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.289e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.330e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.570e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.744e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.730e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.588e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.491e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.287e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.947e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.513e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.394e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.414e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.192e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.801e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.565e+00, tolerance: 2.721e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.107e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.577e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.421e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.473e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.119e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.834e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.138e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.546e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.948e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.917e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.409e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.762e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.701e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.126e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.237e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.337e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.311e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.201e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.063e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.786e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.199e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.040e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.150e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.121e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.338e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.364e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.288e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.247e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.355e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.611e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.947e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.317e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.819e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.165e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.426e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.748e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.115e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.297e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.679e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.415e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.820e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.534e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.187e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.834e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.370e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.166e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.067e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.924e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.816e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.875e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.412e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.271e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.280e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.259e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.209e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.826e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.719e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.006e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.120e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.270e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.457e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.620e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.863e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.935e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.963e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.933e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.607e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.400e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.298e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.439e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.569e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.714e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.869e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.773e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.654e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.988e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.395e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.979e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.277e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.566e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.866e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.129e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.319e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.439e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.399e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.270e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.128e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.926e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.656e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.321e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.010e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.703e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.331e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.078e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.059e+00, tolerance: 2.540e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.315e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\nLassoCV(alphas=array([0.        , 0.02020202, 0.04040404, 0.06060606, 0.08080808,\n       0.1010101 , 0.12121212, 0.14141414, 0.16161616, 0.18181818,\n       0.2020202 , 0.22222222, 0.24242424, 0.26262626, 0.28282828,\n       0.3030303 , 0.32323232, 0.34343434, 0.36363636, 0.38383838,\n       0.4040404 , 0.42424242, 0.44444444, 0.46464646, 0.48484848,\n       0.50505051, 0.52525253, 0.54545455, 0.56565657, 0.58585859,\n       0.60606061...\n       1.31313131, 1.33333333, 1.35353535, 1.37373737, 1.39393939,\n       1.41414141, 1.43434343, 1.45454545, 1.47474747, 1.49494949,\n       1.51515152, 1.53535354, 1.55555556, 1.57575758, 1.5959596 ,\n       1.61616162, 1.63636364, 1.65656566, 1.67676768, 1.6969697 ,\n       1.71717172, 1.73737374, 1.75757576, 1.77777778, 1.7979798 ,\n       1.81818182, 1.83838384, 1.85858586, 1.87878788, 1.8989899 ,\n       1.91919192, 1.93939394, 1.95959596, 1.97979798, 2.        ]))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoCVLassoCV(alphas=array([0.        , 0.02020202, 0.04040404, 0.06060606, 0.08080808,\n       0.1010101 , 0.12121212, 0.14141414, 0.16161616, 0.18181818,\n       0.2020202 , 0.22222222, 0.24242424, 0.26262626, 0.28282828,\n       0.3030303 , 0.32323232, 0.34343434, 0.36363636, 0.38383838,\n       0.4040404 , 0.42424242, 0.44444444, 0.46464646, 0.48484848,\n       0.50505051, 0.52525253, 0.54545455, 0.56565657, 0.58585859,\n       0.60606061...\n       1.31313131, 1.33333333, 1.35353535, 1.37373737, 1.39393939,\n       1.41414141, 1.43434343, 1.45454545, 1.47474747, 1.49494949,\n       1.51515152, 1.53535354, 1.55555556, 1.57575758, 1.5959596 ,\n       1.61616162, 1.63636364, 1.65656566, 1.67676768, 1.6969697 ,\n       1.71717172, 1.73737374, 1.75757576, 1.77777778, 1.7979798 ,\n       1.81818182, 1.83838384, 1.85858586, 1.87878788, 1.8989899 ,\n       1.91919192, 1.93939394, 1.95959596, 1.97979798, 2.        ]))\n\n\n\npredictr.score(X,y)\n\n0.9483494983570958\n\n\n\npredictr.score(XX,yy)\n\n0.8749691641081606\n\n\n\n\n5. HW\n- 현재 predictr 에 저장된1 alpha값을 조사하고, 그것을 바탕으로 아래의 코드를 수정하여 위와 동일한 train_score, test_score가 나오도록 하라.\n1 LassoCV클래스에서 생성된 predictor수정할코드\n## step1\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.Lasso(alpha=????)\n## step3\npredictr.fit(X,y)\n## step4 -- pass\n나와야할 결과\n\npredictr.score(X,y)\n\n0.9483494983570958\n\n\n\npredictr.score(XX,yy)\n\n0.8749691641081606"
  },
  {
    "objectID": "posts/08wk-036.html",
    "href": "posts/08wk-036.html",
    "title": "08wk-037: 중간점검",
    "section": "",
    "text": "1. 강의영상"
  },
  {
    "objectID": "posts/03wk-012.html#a.-데이터-정리",
    "href": "posts/03wk-012.html#a.-데이터-정리",
    "title": "03wk-012: 취업 / 로지스틱 – 추가해설",
    "section": "A. 데이터 정리",
    "text": "A. 데이터 정리\n\nX = pd.get_dummies(df_train[['toeic','gpa']])\ny = df_train[['employment']]\n\n\nX\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n...\n...\n...\n\n\n495\n280\n4.288465\n\n\n496\n310\n2.601212\n\n\n497\n225\n0.042323\n\n\n498\n320\n1.041416\n\n\n499\n375\n3.626883\n\n\n\n\n500 rows × 2 columns"
  },
  {
    "objectID": "posts/03wk-012.html#b.-predictor-starstarstar",
    "href": "posts/03wk-012.html#b.-predictor-starstarstar",
    "title": "03wk-012: 취업 / 로지스틱 – 추가해설",
    "section": "B. Predictor (\\(\\star\\star\\star\\))",
    "text": "B. Predictor (\\(\\star\\star\\star\\))\n- 여기가 중요함. \\(y\\)가 연속형이 아니라 범주형으로 이루어진 경우는 sklearn.linear_model.LogisticRegression() 이용하여 predictor를 만들 것\n\npredictr = sklearn.linear_model.LogisticRegression()"
  },
  {
    "objectID": "posts/03wk-012.html#c.-학습",
    "href": "posts/03wk-012.html#c.-학습",
    "title": "03wk-012: 취업 / 로지스틱 – 추가해설",
    "section": "C. 학습",
    "text": "C. 학습\n\npredictr.fit(X,y)\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/03wk-012.html#d.-예측",
    "href": "posts/03wk-012.html#d.-예측",
    "title": "03wk-012: 취업 / 로지스틱 – 추가해설",
    "section": "D. 예측",
    "text": "D. 예측\n\npredictr.predict(X) \n\narray([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n       0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n       0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n       0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1])"
  },
  {
    "objectID": "posts/03wk-012.html#e.-평가",
    "href": "posts/03wk-012.html#e.-평가",
    "title": "03wk-012: 취업 / 로지스틱 – 추가해설",
    "section": "E. 평가",
    "text": "E. 평가\n\npredictr.score(X,y)\n\n0.882\n\n\n\n(predictr.predict(X) == y.employment).mean()\n\n0.882\n\n\n\nplt.plot(df_train.toeic,df_train.gpa,'o')\ndf_filtered = df_train[predictr.predict(X)==1]\nplt.plot(df_filtered.toeic,df_filtered.gpa,'o') \n\n\n\n\n\n이 정도면 합리적임\n\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\n오버피팅: 물론 정확률이 좀 아쉽긴하지만 여기서 더 맞추려고 한다면 오버핏이다. 모형을 개선하는게 의미가 없을것 같다는 판단을 해야함."
  },
  {
    "objectID": "posts/06wk-022.html#a.-분석절차",
    "href": "posts/06wk-022.html#a.-분석절차",
    "title": "06wk-022: 취업(다중공선성) / 다중공선성의 개념 – 추가해설",
    "section": "A. 분석절차",
    "text": "A. 분석절차\n- step1: 데이터정리\n\ndf_train,df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\n\n\nX = df_train.loc[:,'gpa':'toeic499']\nXX = df_test.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nyy = df_test.loc[:,'employment_score']\n\n- step2: predictor 생성\n\npredictr = sklearn.linear_model.LinearRegression()\n\n- step3: 학습\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- step4: 예측: 생략"
  },
  {
    "objectID": "posts/06wk-022.html#b.-계수해석-및-평가",
    "href": "posts/06wk-022.html#b.-계수해석-및-평가",
    "title": "06wk-022: 취업(다중공선성) / 다중공선성의 개념 – 추가해설",
    "section": "B. 계수해석 및 평가",
    "text": "B. 계수해석 및 평가\n- 계수해석\n\ns = pd.Series(predictr.coef_)\ns.index = X.columns \ns\n\ngpa         0.035315\ntoeic       0.002680\ntoeic0      0.009333\ntoeic1     -0.017511\ntoeic2      0.005205\n              ...   \ntoeic495   -0.012811\ntoeic496   -0.007390\ntoeic497   -0.007487\ntoeic498    0.003379\ntoeic499   -0.002187\nLength: 502, dtype: float64\n\n\n\n실제계수값은 토익*1/100, GPA*1.0, 나머지 toeic0~toeic499 는 모두 계수값이 0임\n그러나 학습된 계수값은 그렇지 않음.\n\n- 평가: train/test score 계산\n\npredictr.score(X,y)\n\n1.0\n\n\n\ntrain 에서는 잘맞음 (퍼펙트) – 모의고사는 기가막히게 잘품\n\n\npredictr.score(XX,yy)\n\n0.1170507821249005\n\n\n\ntest 에서는 잘 맞지 않음 – 수능을 보면 망한다."
  },
  {
    "objectID": "posts/06wk-022.html#a.-toeic과-gpa가-유의미한-변수라는걸-눈치챘다면-오라클..",
    "href": "posts/06wk-022.html#a.-toeic과-gpa가-유의미한-변수라는걸-눈치챘다면-오라클..",
    "title": "06wk-022: 취업(다중공선성) / 다중공선성의 개념 – 추가해설",
    "section": "A. toeic과 gpa가 유의미한 변수라는걸 눈치챘다면? (오라클..)",
    "text": "A. toeic과 gpa가 유의미한 변수라는걸 눈치챘다면? (오라클..)\n- 분석절차수행\n\n## step1: 데이터의 정리  \ndf_train,df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic']\nXX = df_test.loc[:,'gpa':'toeic']\ny = df_train.loc[:,'employment_score']\nyy = df_test.loc[:,'employment_score']\n## step2: predictor 생성 \npredictr = sklearn.linear_model.LinearRegression()\n## step3: predictor.fit을 이용하여 predictor 학습\npredictr.fit(X,y)\n## step4: predictor.predict을 이용하여 예측 -- pass \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 계수해석\n\ns = pd.Series(predictr.coef_)\ns.index = X.columns\ns\n\ngpa      0.972163\ntoeic    0.010063\ndtype: float64\n\n\n\n실제계수값인 GPA*1.0, 토익*1/100이 잘 추정됨\n\n- score도 괜찮음\n\nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 0.9133\ntest_score: 0.9127"
  },
  {
    "objectID": "posts/06wk-022.html#b.-하다못해-toeic0과-gpa로-적합했다면",
    "href": "posts/06wk-022.html#b.-하다못해-toeic0과-gpa로-적합했다면",
    "title": "06wk-022: 취업(다중공선성) / 다중공선성의 개념 – 추가해설",
    "section": "B. 하다못해 toeic0과 gpa로 적합했다면?",
    "text": "B. 하다못해 toeic0과 gpa로 적합했다면?\n- 분석절차\n\n## step1: 데이터의 정리  \ndf_train,df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,['gpa','toeic0']]\nXX = df_test.loc[:,['gpa','toeic0']]\ny = df_train.loc[:,'employment_score']\nyy = df_test.loc[:,'employment_score']\n## step2: predictor 생성 \npredictr = sklearn.linear_model.LinearRegression()\n## step3: predictor.fit을 이용하여 predictor 학습\npredictr.fit(X,y)\n## step4: predictor.predict을 이용하여 예측 -- pass \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 계수해석\n\npredictr.coef_\n\narray([0.98130228, 0.0101011 ])\n\n\n\n합리적으로 추정된것 같음\n\n- 평가\n\nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 0.9121\ntest_score: 0.9115\n\n\n\n오라클 만큼은 아니지만 이정도만 되어도 합리적임"
  },
  {
    "objectID": "posts/02wk-006.html#a.-데이터",
    "href": "posts/02wk-006.html#a.-데이터",
    "title": "02wk-006: 타이타닉 / Autogluon (Fsize)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\n!kaggle competitions download -c titanic\n!unzip titanic.zip -d ./titanic\ndf_train = TabularDataset('titanic/train.csv')\ndf_test = TabularDataset('titanic/test.csv')\n!rm titanic.zip\n!rm -rf titanic/\n\nDownloading titanic.zip to /home/cgb2/Dropbox/07_lectures/2023-09-MP2023/posts\n  0%|                                               | 0.00/34.1k [00:00&lt;?, ?B/s]\n100%|███████████████████████████████████████| 34.1k/34.1k [00:00&lt;00:00, 395kB/s]\nArchive:  titanic.zip\n  inflating: ./titanic/gender_submission.csv  \n  inflating: ./titanic/test.csv      \n  inflating: ./titanic/train.csv     \n\n\n- 피처엔지니어링\n\ndf_train.eval('Fsize = SibSp + Parch')\ndf_test.eval('Fsize = SibSp + Parch')\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nFsize\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n0\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n1\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n0\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n0\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n413\n1305\n3\nSpector, Mr. Woolf\nmale\nNaN\n0\n0\nA.5. 3236\n8.0500\nNaN\nS\n0\n\n\n414\n1306\n1\nOliva y Ocana, Dona. Fermina\nfemale\n39.0\n0\n0\nPC 17758\n108.9000\nC105\nC\n0\n\n\n415\n1307\n3\nSaether, Mr. Simon Sivertsen\nmale\n38.5\n0\n0\nSOTON/O.Q. 3101262\n7.2500\nNaN\nS\n0\n\n\n416\n1308\n3\nWare, Mr. Frederick\nmale\nNaN\n0\n0\n359309\n8.0500\nNaN\nS\n0\n\n\n417\n1309\n3\nPeter, Master. Michael J\nmale\nNaN\n1\n1\n2668\n22.3583\nNaN\nC\n2\n\n\n\n\n418 rows × 12 columns"
  },
  {
    "objectID": "posts/02wk-006.html#b.-predictor-생성",
    "href": "posts/02wk-006.html#b.-predictor-생성",
    "title": "02wk-006: 타이타닉 / Autogluon (Fsize)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231024_084218\""
  },
  {
    "objectID": "posts/02wk-006.html#c.-적합fit",
    "href": "posts/02wk-006.html#c.-적합fit",
    "title": "02wk-006: 타이타닉 / Autogluon (Fsize)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(df_train.eval('Fsize = SibSp + Parch'))\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231024_084218\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   265.27 GB / 490.57 GB (54.1%)\nTrain Data Rows:    891\nTrain Data Columns: 12\nLabel Column: Survived\nPreprocessing data ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/learner/default_learner.py:215: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    124116.32 MB\n    Train Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/features/generators/fillna.py:58: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  X.fillna(self._fillna_feature_map, inplace=True, downcast=False)\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    12 features in original data used to generate 29 features in processed data.\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2417.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    Train Data (Processed) Memory Usage: 0.08 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.22s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.648    = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.648    = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8156   = Validation score   (accuracy)\n    0.4s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8212   = Validation score   (accuracy)\n    0.19s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.42s    = Training   runtime\n    0.14s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8212   = Validation score   (accuracy)\n    0.54s    = Training   runtime\n    0.11s    = Validation runtime\nFitting model: CatBoost ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8268   = Validation score   (accuracy)\n    0.54s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8045   = Validation score   (accuracy)\n    0.84s    = Training   runtime\n    0.24s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.77s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n    0.8324   = Validation score   (accuracy)\n    1.82s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n    0.8268   = Validation score   (accuracy)\n    0.2s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8324   = Validation score   (accuracy)\n    3.7s     = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBMLarge ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8324   = Validation score   (accuracy)\n    0.4s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8547   = Validation score   (accuracy)\n    0.48s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 11.61s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231024_084218\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f29d8593b80&gt;\n\n\n- 리더보드확인 (모의고사채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.860335       0.016608  2.934600                0.000400           0.236214            2       True         14\n1        NeuralNetTorch   0.837989       0.008671  1.382263                0.008671           1.382263            1       True         12\n2         LightGBMLarge   0.832402       0.003077  0.377827                0.003077           0.377827            1       True         13\n3       NeuralNetFastAI   0.832402       0.007537  1.316123                0.007537           1.316123            1       True         10\n4              CatBoost   0.826816       0.003649  0.528946                0.003649           0.528946            1       True          7\n5               XGBoost   0.826816       0.004545  0.150251                0.004545           0.150251            1       True         11\n6              LightGBM   0.821229       0.003294  0.180805                0.003294           0.180805            1       True          4\n7      RandomForestEntr   0.821229       0.194986  0.478185                0.194986           0.478185            1       True          6\n8            LightGBMXT   0.815642       0.003372  0.198229                0.003372           0.198229            1       True          3\n9      RandomForestGini   0.815642       0.097445  0.306731                0.097445           0.306731            1       True          5\n10       ExtraTreesEntr   0.810056       0.076015  0.512563                0.076015           0.512563            1       True          9\n11       ExtraTreesGini   0.804469       0.118582  0.881634                0.118582           0.881634            1       True          8\n12       KNeighborsDist   0.648045       0.001869  0.007722                0.001869           0.007722            1       True          2\n13       KNeighborsUnif   0.648045       0.004271  0.092099                0.004271           0.092099            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.860335\n0.016608\n2.934600\n0.000400\n0.236214\n2\nTrue\n14\n\n\n1\nNeuralNetTorch\n0.837989\n0.008671\n1.382263\n0.008671\n1.382263\n1\nTrue\n12\n\n\n2\nLightGBMLarge\n0.832402\n0.003077\n0.377827\n0.003077\n0.377827\n1\nTrue\n13\n\n\n3\nNeuralNetFastAI\n0.832402\n0.007537\n1.316123\n0.007537\n1.316123\n1\nTrue\n10\n\n\n4\nCatBoost\n0.826816\n0.003649\n0.528946\n0.003649\n0.528946\n1\nTrue\n7\n\n\n5\nXGBoost\n0.826816\n0.004545\n0.150251\n0.004545\n0.150251\n1\nTrue\n11\n\n\n6\nLightGBM\n0.821229\n0.003294\n0.180805\n0.003294\n0.180805\n1\nTrue\n4\n\n\n7\nRandomForestEntr\n0.821229\n0.194986\n0.478185\n0.194986\n0.478185\n1\nTrue\n6\n\n\n8\nLightGBMXT\n0.815642\n0.003372\n0.198229\n0.003372\n0.198229\n1\nTrue\n3\n\n\n9\nRandomForestGini\n0.815642\n0.097445\n0.306731\n0.097445\n0.306731\n1\nTrue\n5\n\n\n10\nExtraTreesEntr\n0.810056\n0.076015\n0.512563\n0.076015\n0.512563\n1\nTrue\n9\n\n\n11\nExtraTreesGini\n0.804469\n0.118582\n0.881634\n0.118582\n0.881634\n1\nTrue\n8\n\n\n12\nKNeighborsDist\n0.648045\n0.001869\n0.007722\n0.001869\n0.007722\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.648045\n0.004271\n0.092099\n0.004271\n0.092099\n1\nTrue\n1"
  },
  {
    "objectID": "posts/02wk-006.html#d.-예측-predict",
    "href": "posts/02wk-006.html#d.-예측-predict",
    "title": "02wk-006: 타이타닉 / Autogluon (Fsize)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(df_train.Survived == predictr.predict(df_train.eval('Fsize = SibSp + Parch'))).mean()\n\n0.9169472502805837\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\n# df_test.assign(Survived = predictr.predict(df_test.eval('Fsize = SibSp + Parch'))).loc[:,['PassengerId','Survived']]\\\n# .to_csv(\"autogluon(Fsize)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/01wk-001.html",
    "href": "posts/01wk-001.html",
    "title": "01wk-001: 캐글소개",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. 타이타닉 경진대회 찾기\n- 방법1: 검색을 통하여 접속하기\n\n\n\n그림1: 타이타닉 경진대회 검색\n\n\n- 방법2: 직접주소를 입력 https://www.kaggle.com/competitions/titanic\n\n\n3. 데이터구경\n- 데이터의 설명을 빠르게 파악하는 방법\n\n방법1: 변수위주로 Kaggle 홈페이지에서 파악\n방법2: 구글번역기 활용\n방법3: \bChatGPT이용\n\n- 제 생각\n\nChatGPT가 옳지 않은 소리를 할 떄도 있음. 하지만 처음에 데이터에 대한 개념을 빠르게 정리하고자 할 때 도움이 된다.\n변수이름이 약어로 된 경우가 많은데 이럴경우 GPT가 유용하다.\n\n\n\n4. 메뉴소개\n- 캐글 웹사이트의 메뉴탭 소개\n\nOverview (개요):\n\n\n경진대회 주최자가 경진대회의 배경, 목표, 데이터셋 설명 등을 설명\n\n\nData (데이터):\n\n\n경진대회에 사용되는 데이터셋에 관한 정보를 찾을 수 있음.\n데이터의 구성, 변수 설명, 예시 데이터 등이 제공되며, 데이터를 이해하고 분석할 수 있는데 필요한 정보들이 여기에 포함됨.\n\n\nCode (코드):\n\n\n경진대회 참가자들이 코드를 공유하고 토론하는 공간.\n주로 주어진 문제에 대한 데이터 분석 및 모델링 코드, 데이터 전처리 방법, 모델 학습 등에 관련된 내용이 이곳에 포함됨.\n\n\nDiscussion (토론):\n\n\n참가자들이 서로 의견을 교환하고 질문을 주고받을 수 있는 공간.\n데이터 분석 방법, 모델 구축 전략, 문제 해결 과정 등에 대한 토론이 이루어짐.\n\n\nLeaderboard (리더보드):\n\n\n경진대회 참가자들의 성능 평가 지표와 순위가 나열.\n참가자들의 모델 성능을 비교하고 경쟁 상황을 실시간으로 확인할 수 있음.\n\n\nRules (규칙):\n\n\n참가자들이 따라야 할 규칙, 데이터 사용 방법, 평가 지표 등이 명시되어 있음.\n\n- 체크하면 좋은 것들\n\nOveview: 평가지표(metric), 상(prize), 시간(timeline)\nRules: 평가지표(metric), 외부데이터 사용여부, 하루 최대 제출 수, 최종선택가능한 솔루션 수 (limit)\n\n- 대회의 유형 (ref: https://github.com/PacktPublishing/The-Kaggle-Book)\n\nGetting Started: 상을 제공하지 않음. 튜토리얼용. 매우 협력적인 환경.\nFeatured: 가장 일반적인 유형. 스폰서 회사의 비지니스 관련문제. 상금이 후함. 솔루션을 소개하는 자세한 리포트를 준비해야하고 발표할 것을 요구받을 수 있음.\nAnalytics: 질적 평가. 참가자의 PPT를 제출로 받음."
  },
  {
    "objectID": "posts/06wk-025.html#a.-정확한-설명",
    "href": "posts/06wk-025.html#a.-정확한-설명",
    "title": "06wk-025: 취업(다중공선성) / Lasso",
    "section": "A. 정확한 설명",
    "text": "A. 정확한 설명\n- 어려워요.."
  },
  {
    "objectID": "posts/06wk-025.html#b.-직관적-설명-엄밀하지-않은-설명",
    "href": "posts/06wk-025.html#b.-직관적-설명-엄밀하지-않은-설명",
    "title": "06wk-025: 취업(다중공선성) / Lasso",
    "section": "B. 직관적 설명 (엄밀하지 않은 설명)",
    "text": "B. 직관적 설명 (엄밀하지 않은 설명)\n- 느낌: 몇 개의 toeic coef들로 쉽게 0.01을 만들게 해서는 안된다.\n\n아이디어1: 0.01을 동일한 값으로 균등하게 배분한다. – Ridge, L2-penalty\n아이디어2: 아주 적은숫자의 coef만을 살려두고 나머지 coef값은 0으로 강제한다. – Lasso, L1-penalty\n\n- 계수값이 0이라는 의미: 그 변수를 제거한것과 같은 효과\n- 아이디어2의 기원: y ~ toeic + gpa 가 트루이지만, y ~ toeic0 + gpa 으로 적합해도 괜찮잖아?\n- 진짜 학습된 계수값이 대부분 0인지 확인해보자.\n\nplt.plot(predictr.coef_[1:])"
  },
  {
    "objectID": "posts/06wk-025.html#c.-alpha-에-따른-변화-관찰",
    "href": "posts/06wk-025.html#c.-alpha-에-따른-변화-관찰",
    "title": "06wk-025: 취업(다중공선성) / Lasso",
    "section": "C. \\(\\alpha\\) 에 따른 변화 관찰",
    "text": "C. \\(\\alpha\\) 에 따른 변화 관찰\n- 여러개의 predictor 학습\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train[['employment_score']]\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test[['employment_score']]\n## step2 \nalphas = np.linspace(0,2,100)\npredictrs = [sklearn.linear_model.Lasso(alpha=alpha) for alpha in alphas]\n## step3\nfor predictr in predictrs: \n    predictr.fit(X,y)\n## step4 : pass \n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/base.py:1152: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n  return fit_method(estimator, *args, **kwargs)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.510e+01, tolerance: 3.337e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.730e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.734e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.620e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.391e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.141e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.634e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.121e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.523e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.895e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.027e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.049e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.048e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.048e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.055e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.045e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.005e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.655e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.315e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.628e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.093e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.556e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.539e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.488e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.407e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.367e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.315e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.261e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.332e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.488e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.628e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.387e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.166e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.010e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.831e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.670e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.446e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.207e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.999e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.776e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.467e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.330e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.288e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.197e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.040e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.853e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.622e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.327e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.026e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.745e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.505e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.301e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.121e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.950e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.692e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.502e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.333e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.099e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.806e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.543e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.289e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.050e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.832e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.662e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.520e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.475e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.420e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.361e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.737e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.617e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.472e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.327e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.186e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.048e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.167e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.743e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.946e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.539e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.406e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.608e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.662e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.710e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.827e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.061e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.072e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.953e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.853e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.231e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.107e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.934e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.787e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.613e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.629e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.524e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.151e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.098e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.087e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.080e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.073e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\n\nplt.plot(predictrs[0].coef_[1:],label=r'$\\alpha={}$'.format(predictrs[0].alpha))\nplt.plot(predictrs[50].coef_[1:],label=r'$\\alpha={}$'.format(predictrs[50].alpha))\nplt.plot(predictrs[-1].coef_[1:],label=r'$\\alpha={}$'.format(predictrs[-1].alpha))\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f773637acb0&gt;\n\n\n\n\n\n- predictor 들의 toeic 계수합은 여전히 0.01 근처\n\nprint(f'alpha={predictrs[0].alpha:.4f}\\tsum(toeic_coef)={predictrs[0].coef_[1:].sum()}')\nprint(f'alpha={predictrs[50].alpha:.4f}\\tsum(toeic_coef)={predictrs[50].coef_[1:].sum()}')\nprint(f'alpha={predictrs[-1].alpha:.4f}\\tsum(toeic_coef)={predictrs[-1].coef_[1:].sum()}')\n\nalpha=0.0000    sum(toeic_coef)=0.010291301406468518\nalpha=1.0101    sum(toeic_coef)=0.009986115762478664\nalpha=2.0000    sum(toeic_coef)=0.009864586871194559\n\n\n- number of non-zero coefs 를 시각화\n\nnon_zero_coefs = [(abs(predictr.coef_[1:])&gt;0).sum() for predictr in predictrs]\n\n\nplt.plot(alphas,non_zero_coefs)"
  },
  {
    "objectID": "posts/06wk-025.html#d.-coef를-0으로-만드는-수학적-장치",
    "href": "posts/06wk-025.html#d.-coef를-0으로-만드는-수학적-장치",
    "title": "06wk-025: 취업(다중공선성) / Lasso",
    "section": "D. coef를 0으로 만드는 수학적 장치",
    "text": "D. coef를 0으로 만드는 수학적 장치\n- Ridge(복습): coef의 값들을 엔빵하는 수학적 장치\n\n패널티: 유사토익들의 계수값을 제곱한뒤 합치고(=L2-norm을 구하고), 그 값이 0에서 떨어져 있을 수록 패널티를 줄꺼야!\n\n- Lasso: coef의 값들을 대부분 0으로 만드는 수학적 장치\n\n패널티: 유사토익들의 계수값의 절대값을 구한뒤에 합치고(=L1-norm을 구하고), 그 값이 0에서 떨어져 있을 수록 패널티를 줄꺼야!\n\n- 사실 L1, L2 패널티에 따라서 이러한 결과가 나오는 것은 이해하기 어렵다. (그래서 취업/대학원 진학시 단골질문중 하나)"
  },
  {
    "objectID": "posts/02wk-003.html#a.-test-답을-모름-제출해야-알-수-있음",
    "href": "posts/02wk-003.html#a.-test-답을-모름-제출해야-알-수-있음",
    "title": "02wk-003: 타이타닉 / 첫 제출",
    "section": "A. test – 답을 모름, 제출해야 알 수 있음",
    "text": "A. test – 답을 모름, 제출해야 알 수 있음\n- 제출결과는 리더보드에서 확인할 수 있음."
  },
  {
    "objectID": "posts/02wk-003.html#b.-train-스스로-풀어보고-채점할-수-있음",
    "href": "posts/02wk-003.html#b.-train-스스로-풀어보고-채점할-수-있음",
    "title": "02wk-003: 타이타닉 / 첫 제출",
    "section": "B. train – 스스로 풀어보고 채점할 수 있음",
    "text": "B. train – 스스로 풀어보고 채점할 수 있음\n- 캐글에서 code \\(\\to\\) New Notebook 클릭\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n\ndf_train=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndf_test=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\n\n# !kaggle competitions download -c titanic\n# !unzip titanic.zip -d ./titanic\n# df_train = pd.read_csv('titanic/train.csv')\n# df_test = pd.read_csv('titanic/test.csv')\n# !rm titanic.zip\n# !rm -rf titanic/\n\n# 예비학습 – accuracy의 계산\n\ndf = pd.DataFrame({'Surv':[1,0,1,1,0],'Sex':['f','m','f','m','m']})\n\n- Surv 열의 선택\n\ndf.Surv\n\n0    1\n1    0\n2    1\n3    1\n4    0\nName: Surv, dtype: int64\n\n\n- Sex 열의 선택\n\ndf.Sex\n\n0    f\n1    m\n2    f\n3    m\n4    m\nName: Sex, dtype: object\n\n\n- Sex == f이면 생존(1), 그렇지 않으면 사망(0)이라고 예측\n\n(df.Sex == 'f')*1\n\n0    1\n1    0\n2    1\n3    0\n4    0\nName: Sex, dtype: int64\n\n\n- 결과를 정리하면 아래와 같음\n\npd.DataFrame({'real': df.Surv, 'estimate': (df.Sex == 'f')*1})\n\n\n\n\n\n\n\n\nreal\nestimate\n\n\n\n\n0\n1\n1\n\n\n1\n0\n0\n\n\n2\n1\n1\n\n\n3\n1\n0\n\n\n4\n0\n0\n\n\n\n\n\n\n\n- accuracy를 손으로 계산하면 \\(\\frac{4}{5}=0.8\\).\n- 컴퓨터로 accuracy를 계산한다면\n\n(df.Surv == (df.Sex == 'f')*1).sum()/5 # 방법1\n\n0.8\n\n\n\n(df.Surv == (df.Sex == 'f')*1).mean() # 방법2\n\n0.8\n\n\n#\n- 실제자료의 accuracy를 구해보자.\n\n(df_train.Survived == (df_train.Sex == 'female')).mean()\n\n0.7867564534231201"
  },
  {
    "objectID": "posts/07wk-032.html",
    "href": "posts/07wk-032.html",
    "title": "07wk-032: 아이스크림(교호작용) / 의사결정나무",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport sklearn.linear_model \nimport sklearn.tree\n\n\n\n3. Data\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()\nchoco = 40 + temp * 2.0 + np.random.randn(100)*3\nvanilla = 60 + temp * 5.0 + np.random.randn(100)*3\ndf1 = pd.DataFrame({'temp':temp,'sales':choco}).assign(type='choco')\ndf2 = pd.DataFrame({'temp':temp,'sales':vanilla}).assign(type='vanilla')\ndf_train = pd.concat([df1,df2])\ndf_train\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n\n\n1\n-3.7\n35.852524\nchoco\n\n\n2\n-3.0\n37.428335\nchoco\n\n\n3\n-1.3\n38.323681\nchoco\n\n\n4\n-0.5\n39.713362\nchoco\n\n\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n\n\n96\n13.4\n129.300464\nvanilla\n\n\n97\n14.7\n136.596568\nvanilla\n\n\n98\n15.0\n136.213140\nvanilla\n\n\n99\n15.2\n135.595252\nvanilla\n\n\n\n\n200 rows × 3 columns\n\n\n\n\nplt.plot(df_train.temp[df_train.type=='choco'],df_train.sales[df_train.type=='choco'],'o',label='choco')\nplt.plot(df_train.temp[df_train.type=='vanilla'],df_train.sales[df_train.type=='vanilla'],'o',label='vanilla')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f81d29533d0&gt;\n\n\n\n\n\n\n\n5. 분석\n- 분석1: 선형회귀\n\n# step1\nX = pd.get_dummies(df_train[['temp','type']],drop_first=True)\ny = df_train['sales']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train['sales_hat'] = predictr.predict(X)\n#---#\nf'train score = {predictr.score(X,y):.4f}'\n\n'train score = 0.9250'\n\n\n\nplt.plot(df_train.temp[df_train.type=='choco'],df_train.sales[df_train.type=='choco'],'o',alpha=0.2,label='choco')\nplt.plot(df_train.temp[df_train.type=='choco'],df_train.sales_hat[df_train.type=='choco'],'--',color='C0')\nplt.plot(df_train.temp[df_train.type=='vanilla'],df_train.sales[df_train.type=='vanilla'],'o',alpha=0.2,label='vanilla')\nplt.plot(df_train.temp[df_train.type=='vanilla'],df_train.sales_hat[df_train.type=='vanilla'],'--',color='C1')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f81d2cee8c0&gt;\n\n\n\n\n\n- 분석2\n\n# step1\nX = pd.get_dummies(df_train[['temp','type']],drop_first=True)\ny = df_train['sales']\n# step2 \npredictr = sklearn.tree.DecisionTreeRegressor()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train['sales_hat'] = predictr.predict(X)\n#---#\nf'train score = {predictr.score(X,y):.4f}'\n\n'train score = 0.9964'\n\n\n\nplt.plot(df_train.temp[df_train.type=='choco'],df_train.sales[df_train.type=='choco'],'o',alpha=0.2,label='choco')\nplt.plot(df_train.temp[df_train.type=='choco'],df_train.sales_hat[df_train.type=='choco'],'--',color='C0')\nplt.plot(df_train.temp[df_train.type=='vanilla'],df_train.sales[df_train.type=='vanilla'],'o',alpha=0.2,label='vanilla')\nplt.plot(df_train.temp[df_train.type=='vanilla'],df_train.sales_hat[df_train.type=='vanilla'],'--',color='C1')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f81d2560c40&gt;\n\n\n\n\n\n* 오버피팅에 대한 제 개념: 통계에서 “관측치 = 언더라잉 + 랜덤” 으로 볼 수 있다. 모형이 설명해야할 영역은 “언더라잉” 이다. 만약에 모형이 언더라잉을 잘 설명하지 못한다면 언더피팅이고, 주어진 모형이 언더라잉을 넘어 오차항까지 설명하고 있다면 오버피팅이다.\n\n마음속의 underlying 을 간직한다 – 애매하죠?\n그 underlying 보다 잘 맞추면 오버피팅이다.\n내 마음속의 underlying 제대로 학습못하고 있다고 판단되면 모형미스 혹은 언더피팅이다.\n\n이러한 논리로 인하면 위의 의사결정나무로 적합된 결과는 오버피팅이다. (그렇지만 언더피팅보단 나을지도?)"
  },
  {
    "objectID": "posts/07wk-034.html",
    "href": "posts/07wk-034.html",
    "title": "07wk-034: 취업(오버피팅) / 의사결정나무",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.model_selection\nimport sklearn.linear_model\nimport sklearn.tree\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\n3. Data\n\ndef generating_df(n_balance):\n    df = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\n    df_balance = pd.DataFrame((np.random.randn(500,n_balance)).reshape(500,n_balance)*1,columns = ['balance'+str(i) for i in range(n_balance)])\n    return pd.concat([df,df_balance],axis=1)\n\n\ndf = generating_df(10)\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\nbalance3\nbalance4\nbalance5\nbalance6\nbalance7\nbalance8\nbalance9\n\n\n\n\n0\n135\n0.051535\n0\n0.184517\n0.415330\n-0.686366\n-1.819431\n-1.173594\n-1.215262\n-1.610389\n-0.820352\n0.449367\n0.725023\n\n\n1\n935\n0.355496\n0\n-0.632594\n-0.664892\n0.211959\n-0.958648\n1.242596\n-2.303714\n-1.450162\n-0.295893\n-0.212096\n0.526111\n\n\n2\n485\n2.228435\n0\n1.165994\n-0.756352\n-0.777127\n-0.643733\n1.051517\n1.899984\n2.451968\n0.294510\n-1.462545\n-0.397320\n\n\n3\n65\n1.179701\n0\n-0.373464\n-0.175916\n-0.096167\n1.362542\n1.285671\n2.568078\n-0.706288\n0.033109\n-0.022258\n-0.127342\n\n\n4\n445\n3.962356\n1\n0.118567\n1.506343\n0.265080\n-0.488586\n-0.354807\n-0.715808\n0.868123\n1.744717\n0.101916\n0.025840\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n0.310288\n1.199902\n0.278003\n0.079322\n1.518781\n0.108520\n1.877523\n0.571970\n0.765288\n0.472346\n\n\n496\n310\n2.601212\n1\n-0.869109\n-1.798781\n-0.801852\n-0.304045\n0.413868\n1.303797\n2.261232\n-1.036498\n-0.540375\n1.263321\n\n\n497\n225\n0.042323\n0\n-0.065514\n-0.133791\n2.126346\n-1.352515\n0.296947\n-0.893480\n0.569711\n0.139151\n-0.643992\n0.126148\n\n\n498\n320\n1.041416\n0\n-0.927475\n1.515043\n1.626494\n0.322667\n1.016824\n1.055700\n-1.795255\n0.497891\n-1.101028\n-1.164185\n\n\n499\n375\n3.626883\n1\n-0.464310\n-2.043014\n1.043372\n-0.080865\n0.633239\n2.330138\n1.390587\n0.872401\n-0.502196\n-0.171452\n\n\n\n\n500 rows × 13 columns\n\n\n\n\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size=0.7, random_state=42)\n\n\nX,y = df_train.drop(['employment'],axis=1), df_train['employment']\nXX,yy = df_test.drop(['employment'],axis=1), df_test['employment']\n\n\n\n4. 분석\n- 분석1: 의사결정나무\n\n## step1 -- pass\n## step2 \npredictr = sklearn.tree.DecisionTreeClassifier(random_state=42)\n## step3 \npredictr.fit(X,y)\n## step4\ndf_train['employment'] = predictr.predict(X)\ndf_test['employment'] = predictr.predict(XX)\n#--#\nprint(f'train_score = {predictr.score(X,y):.4f}')\nprint(f'test_score = {predictr.score(XX,yy):.4f}')\n\ntrain_score = 1.0000\ntest_score = 0.7571\n\n\n- 분석2: 로지스틱 + Ridge\n\n## step1 -- pass\n## step2 \npredictr = sklearn.linear_model.LogisticRegressionCV(penalty='l2')\n## step3 \npredictr.fit(X,y)\n## step4\ndf_train['employment'] = predictr.predict(X)\ndf_test['employment'] = predictr.predict(XX)\n#--#\nprint(f'train_score = {predictr.score(X,y):.4f}')\nprint(f'test_score = {predictr.score(XX,yy):.4f}')\n\ntrain_score = 0.8800\ntest_score = 0.8800\n\n\n- 분석3: 로지스틱 + Lasso\n\n## step1 -- pass\n## step2 \npredictr = sklearn.linear_model.LogisticRegressionCV(penalty='l1', solver='liblinear')\n## step3 \npredictr.fit(X,y)\n## step4\ndf_train['employment'] = predictr.predict(X)\ndf_test['employment'] = predictr.predict(XX)\n#--#\nprint(f'train_score = {predictr.score(X,y):.4f}')\nprint(f'test_score = {predictr.score(XX,yy):.4f}')\n\ntrain_score = 0.8667\ntest_score = 0.8857\n\n\n\n\n5. 연구\n- Balance 변수들의 수가 커짐에 따라서 각 방법들(의사결정나무, 로지스틱+Ridge, 로지스틱+Lasso)의 train/test score는 어떻게 변화할까?\n- df, predictor -&gt; train_score, test_score 와 같은 함수를 만들자.\n\ndef anal(df,predictr):\n    df_train, df_test = sklearn.model_selection.train_test_split(df, test_size=0.7, random_state=42)\n    X,y = df_train.drop(['employment'],axis=1), df_train['employment']\n    XX,yy = df_test.drop(['employment'],axis=1), df_test['employment']\n    ## step1 -- pass\n    ## step2 -- pass \n    ## step3 \n    predictr.fit(X,y)\n    ## step4 -- pass \n    #--#\n    return predictr.score(X,y),predictr.score(XX,yy)\n\n\npredictr = sklearn.tree.DecisionTreeClassifier()\n\n\nanal(df,predictr)\n\n(1.0, 0.7657142857142857)\n\n\n- 실험해보자.\n\nn_balance_lst = range(0,5000,50)\n\n\npredictrs = [sklearn.tree.DecisionTreeClassifier(random_state=42),\n             sklearn.linear_model.LogisticRegressionCV(penalty='l2'),\n             sklearn.linear_model.LogisticRegressionCV(penalty='l1', solver='liblinear')]\n\n\nlst = [[anal(generating_df(n_balance),predictr) for predictr in predictrs] for n_balance in n_balance_lst]\n\n- 실험결과 정리\n\narr = np.array(lst)\ntr = arr[:,:,0]\ntst = arr[:,:,1]\n\n\ndf1= pd.DataFrame(tr,columns=['tree','ridge','lasso']).eval('dataset = \"train\"').eval('n_balance = @n_balance_lst')\ndf2= pd.DataFrame(tst,columns=['tree','ridge','lasso']).eval('dataset = \"test\"').eval('n_balance = @n_balance_lst')\nresult_df = pd.concat([df1,df2]).set_index(['dataset','n_balance']).stack().reset_index().set_axis(['dataset','n_balance','method','score'],axis=1)\n\n\nsns.lineplot(result_df.query('dataset==\"test\"'),x='n_balance',y='score',hue='method')\n\n&lt;Axes: xlabel='n_balance', ylabel='score'&gt;"
  },
  {
    "objectID": "posts/04wk-017.html",
    "href": "posts/04wk-017.html",
    "title": "04wk-017: 취업 / 로지스틱의 깊은 이해",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model\n\n\n\n3. 데이터 불러오기 \\(\\to\\) 학습\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf_train\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\n\n\n\n\n0\n135\n0.051535\n0\n\n\n1\n935\n0.355496\n0\n\n\n2\n485\n2.228435\n0\n\n\n3\n65\n1.179701\n0\n\n\n4\n445\n3.962356\n1\n\n\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n\n\n496\n310\n2.601212\n1\n\n\n497\n225\n0.042323\n0\n\n\n498\n320\n1.041416\n0\n\n\n499\n375\n3.626883\n1\n\n\n\n\n500 rows × 3 columns\n\n\n\n\nX = df_train[['toeic','gpa']]\ny = df_train[['employment']]\npredictr = sklearn.linear_model.LogisticRegression()\npredictr.fit(X,y)\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\n\n4. yhat이 나오는 방식?\n- 확인: 무슨 수식에 의하여 나오긴함\n\npredictr.coef_, predictr.intercept_\n\n(array([[0.00571598, 2.46520018]]), array([-8.45433334]))\n\n\n\nu = X.toeic*0.00571598 + X.gpa*2.46520018 -8.45433334\nv = 1/(1+np.exp(-u))\nv # 확률같은것임\n\n0      0.000523\n1      0.096780\n2      0.453003\n3      0.005627\n4      0.979312\n         ...   \n495    0.976295\n496    0.432939\n497    0.000855\n498    0.016991\n499    0.932777\nLength: 500, dtype: float64\n\n\n\n((v &gt; 0.5) == predictr.predict(X)).mean()\n\n1.0\n\n\n- 하여튼 아래와 같은 구조임\n(구조1)\n\n\n\n\nflowchart LR\n  A[\"predictr (not learned)\"]\n  B[\"predictr (learned)\"]\n  C(\"v (=prob)\")\n  D(\"yhat\")\n  A --&gt; |\"`.fit(X,y)`\"| B --&gt; |\"`????`\" | C --&gt; |\"`v&gt;0.5`\"| D\n\n\n\n\n\n(구조2) – 단순화\n\n\n\n\nflowchart LR\n  A[\"predictr (not learned)\"]\n  B[\"predictr (learned)\"]\n  C(\"yhat\")\n  A --&gt; |\"`.fit(X,y)`\"| B --&gt; |\"`.predict(X)`\" | C\n\n\n\n\n\n- v 값을 알고 싶다면 어쩌지?\n\nv[:5].round(3)\n\n0    0.001\n1    0.097\n2    0.453\n3    0.006\n4    0.979\ndtype: float64\n\n\n\npredictr.predict_proba(X)[:5].round(3) \n\narray([[0.999, 0.001],\n       [0.903, 0.097],\n       [0.547, 0.453],\n       [0.994, 0.006],\n       [0.021, 0.979]])"
  },
  {
    "objectID": "posts/07wk-035.html",
    "href": "posts/07wk-035.html",
    "title": "07wk-035: 아이스크림(이상치) / 의사결정나무",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.model_selection\nimport sklearn.linear_model\nimport sklearn.tree\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n\n\n3. Data\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:100,3].to_numpy()\ntemp.sort()\nice_sales = 10 + temp * 0.5 + np.random.randn(100)\nice_sales[0] = 200\ndf_train = pd.DataFrame({'temp':temp,'ice_sales':ice_sales})\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n200.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n...\n...\n...\n\n\n95\n12.4\n17.508688\n\n\n96\n13.4\n17.105376\n\n\n97\n14.7\n17.164930\n\n\n98\n15.0\n18.555388\n\n\n99\n15.2\n18.787014\n\n\n\n\n100 rows × 2 columns\n\n\n\n\nplt.plot(df_train.temp,df_train.ice_sales,'o')\n\n\n\n\n\n\n4. 분석\n- 분석: 의사결정나무\n\n# step1\nX = df_train[['temp']]\ny = df_train['ice_sales']\n# step2 \npredictr = sklearn.tree.DecisionTreeRegressor()\n# step3 \npredictr.fit(X,y)\n# step4 \ndf_train['ice_sales_hat'] = predictr.predict(X)\n\n\n# plt.plot(df_train.temp,df_train.ice_sales,'o')\n# plt.plot(df_train.temp,df_train.ice_sales_hat,'--')\nplt.plot(df_train.temp[1:],df_train.ice_sales[1:],'o')\nplt.plot(df_train.temp[1:],df_train.ice_sales_hat[1:],'--')\n\n\n\n\n- 12.5~18 구간사이의 unseen data를 가상으로 만들고 예측값을 살펴보자.\n\nXX = df_test = pd.DataFrame({'temp':np.linspace(12.5,18,100)})\n\n\ndf_test['ice_sales_hat'] = predictr.predict(XX)\n\n\nplt.plot(df_train.temp[1:],df_train.ice_sales[1:],'o',color='C0',alpha=0.5)\nplt.plot(df_train.temp[1:],df_train.ice_sales_hat[1:],'--',color='C1',alpha=0.5)\nplt.plot(df_test.temp,df_test.ice_sales_hat,'--',color='C2',linewidth=2)\n\n\n\n\n- -15~0 구간사이의 unseen data를 가상으로 만들고 예측값을 살펴보자.\n\nXX = df_test = pd.DataFrame({'temp':np.linspace(-15,0,100)})\n\n\ndf_test['ice_sales_hat'] = predictr.predict(XX)\n\n\nplt.plot(df_train.temp[1:],df_train.ice_sales[1:],'o',color='C0',alpha=0.5)\nplt.plot(df_train.temp[1:],df_train.ice_sales_hat[1:],'--',color='C1',alpha=0.5)\nplt.plot(df_test.temp,df_test.ice_sales_hat,'--',color='C2',linewidth=2)\n\n\n\n\n\n뭐 이 데이터에서는 최선이지 않을까?\n\n\n\n6. HW\n- 없어요. 다른과목 중간고사 준비 잘하세요!"
  },
  {
    "objectID": "posts/04wk-014.html#a.-df.info",
    "href": "posts/04wk-014.html#a.-df.info",
    "title": "04wk-014: msno를 이용한 결측치 시각화",
    "section": "A. df.info()",
    "text": "A. df.info()\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 5 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   A       668 non-null    float64\n 1   B       656 non-null    float64\n 2   C       608 non-null    float64\n 3   D       668 non-null    float64\n 4   E       660 non-null    float64\ndtypes: float64(5)\nmemory usage: 39.2 KB"
  },
  {
    "objectID": "posts/04wk-014.html#b.-msno.bar",
    "href": "posts/04wk-014.html#b.-msno.bar",
    "title": "04wk-014: msno를 이용한 결측치 시각화",
    "section": "B. msno.bar()",
    "text": "B. msno.bar()\n\nmsno.bar(df)\n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/04wk-014.html#a.-msno.matrix",
    "href": "posts/04wk-014.html#a.-msno.matrix",
    "title": "04wk-014: msno를 이용한 결측치 시각화",
    "section": "A. msno.matrix()",
    "text": "A. msno.matrix()\n\nmsno.matrix(df)\n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/04wk-014.html#b.-msno.heatmap",
    "href": "posts/04wk-014.html#b.-msno.heatmap",
    "title": "04wk-014: msno를 이용한 결측치 시각화",
    "section": "B. msno.heatmap()",
    "text": "B. msno.heatmap()\n\nmsno.heatmap(df)\n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/04wk-014.html#c.-msno.dendrogram",
    "href": "posts/04wk-014.html#c.-msno.dendrogram",
    "title": "04wk-014: msno를 이용한 결측치 시각화",
    "section": "C. msno.dendrogram()",
    "text": "C. msno.dendrogram()\n\nmsno.dendrogram(df)\n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/05wk-020.html",
    "href": "posts/05wk-020.html",
    "title": "05wk-020: StandardScaler를 이용한 전처리",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.preprocessing \n\n\n\n3. StandardScaler()\n- 예제자료 로드\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv').loc[:7,['toeic','gpa']]\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n5\n65\n1.846885\n\n\n6\n290\n0.309928\n\n\n7\n730\n0.336081\n\n\n\n\n\n\n\n- 스케일러를 생성\n\nsclr = sklearn.preprocessing.StandardScaler()\nsclr.fit_transform(df)\n\narray([[-0.8680409 , -0.98104887],\n       [ 1.81575704, -0.73905505],\n       [ 0.3061207 ,  0.75205327],\n       [-1.10287322, -0.08287854],\n       [ 0.17193081,  2.13248542],\n       [-1.10287322,  0.44828929],\n       [-0.34805505, -0.77533368],\n       [ 1.12803382, -0.75451182]])\n\n\n- 계산식\n\n(df.toeic - df.toeic.mean())/df.toeic.std(ddof=0) # 계산식\n\n0   -0.868041\n1    1.815757\n2    0.306121\n3   -1.102873\n4    0.171931\n5   -1.102873\n6   -0.348055\n7    1.128034\nName: toeic, dtype: float64\n\n\n\n\n4. 비교\n- MinMaxScaler와 StandardScaler는 데이터의 스케일을 조정하는 두 가지 일반적인 방법이다.\n\nMinMaxScaler:\n\n작동 원리: 데이터를 0과 1 사이의 값으로 조정\n장점: 원하는 범위 내로 데이터를 조정할 때 유용. 특히 신경망에서는 활성화 함수의 범위와 일치하도록 입력 값을 조정하는 데 유용.1\n단점: 이상치에 매우 민감하다. 이상치 때문에 전체 데이터의 스케일이 크게 영향받을 수 있음.\n\nStandardScaler:\n\n작동 원리: 데이터의 평균을 0, 표준편차를 1로 만드는 방식으로 조정.\n장점: 이상치에 MinMaxScaler보다 덜 민감함. 많은 통계적 기법들, 특히 PCA 같은 선형 알고리즘에서 잘 작동함.2\n단점: MinMaxScaler와 달리, 표준화된 데이터의 값이 특정 범위 내에 있음을 보장하지 않음.3\n\n\n1 sigmoid, tanh와 같은 활성화 함수의 출력값과 맞추는 용도2 그야 PCA는 정규분포를 가정하고 만든 알고리즘이라~3 MinMaxScaler도 딱히 엄격하게 보장하는건 아니야- 무식한 설명 (1)\n\nMinMaxScaler: 컴퓨터공학과, 전자공학과 느낌\nStandardScaler: 통계학과 느낌\n\n- 무식한 설명 (2)\n\nMinMaxScaler: 데이터가 기본적으로 0$\\(1 혹은 -1\\)$1 사이의 범위에 있다고 가정한다.\nStandardScaler: 데이터가 기본적으로 정규분포를 따른다고 가정하는 모형들과 잘 맞는다.\n\n- 둘 중 어느 것을 선택할지 결정하기 위한 고려사항:\n\n이상치가 많으면 StandardScaler가 더 적합할 수 있다.\n모델의 알고리즘과 특성에 따라 선택해야 한다. 예를 들어, 신경망은 일반적으로 0과 1 사이의 값이나 -1과 1 사이의 값으로 입력을 받는 활성화 함수를 사용하므로 MinMaxScaler가 적합할 수 있다.\n\n결론적으로, 두 스케일링 방법 중 어느 것이 더 좋은지는 사용 사례와 데이터의 특성에 따라 다르기 때문에, 가능한 경우 둘 다 시도해보고 모델의 성능을 비교하는 것이 좋다.\n- 둘 중 어느 것을 선택할지 결정하기 위한 고려사항 – 무식한 설명\n\n보통은 아무거나 해도 큰일 안남.\n아주 특수한 경우4를 제외하고는 어차피 이론적인 선택기준은 없음.\n\n\n\n4 Classical PCA"
  },
  {
    "objectID": "posts/05wk-021.html#a.-학부12학년-수준의-설명",
    "href": "posts/05wk-021.html#a.-학부12학년-수준의-설명",
    "title": "05wk-021: 취업(오버피팅) / 오버피팅의 개념 – 추가해설",
    "section": "A. 학부1~2학년 수준의 설명",
    "text": "A. 학부1~2학년 수준의 설명\n- 과적합(Overfitting): 머신러닝과 통계에서 자주 나타나는 문제로, 모델이 학습데이터에 과도하게 최적화가 되어서 실제로 새로운 데이터나 테스트데이터에서 성능이 저하되는 현상을 말함.\n- 오버피팅의 원인:\n\n불필요한 특징: 불필요한 특징이 데이터에 포함되어 있다면 오버피팅이 발생할 수 있음."
  },
  {
    "objectID": "posts/05wk-021.html#b.-일반인-수준의-설명",
    "href": "posts/05wk-021.html#b.-일반인-수준의-설명",
    "title": "05wk-021: 취업(오버피팅) / 오버피팅의 개념 – 추가해설",
    "section": "B. 일반인 수준의 설명",
    "text": "B. 일반인 수준의 설명\n- 시험 공부(1): 공부를 하랬더니 외우고 있음..\n- 시험 공부(2): (시험 하루 전날에) 공부 그만하고 술이나 먹으러 가자.. 더 공부하면 train error만 줄일 뿐이야..\n- 운전: 특정도로에서만 운전연습을 했음. 그래서 그 도로의 구멍, 곡률, 신호등의 위치까지 완벽하게 숙지하였음. 그 결과 그 도로에서는 잘하게 되었지만, 그 도로 이외의 다른도로에서 운전을 한다면 문제가 발생함.\n- 언어: 특정 주제나 특정 상황에 대한 대화만을 반복적으로 연습하여, 그 상황에서는 완벽한 대화가 가능하지만 그 외의 상황에서는 대화를 제대로 이어나갈 수 없음.\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\n오버피팅에 대한 내 개념: Predictor가 언더라잉이 아니라 오차항을 적합하고 있는 상황.\n\n이 예제에서 언더라잉은 gpa,toiec만으로 충분히 적합할 수 있다.\n따라서 gpa,toiec 이의외 변수가 적합하고 있는 것은 오차항이다.\n그래서 오버핏이 발생"
  },
  {
    "objectID": "posts/02wk-007.html#a.-데이터",
    "href": "posts/02wk-007.html#a.-데이터",
    "title": "02wk-007: 타이타닉 / Autogluon (Fsize,Drop)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\n!kaggle competitions download -c titanic\n!unzip titanic.zip -d ./titanic\ndf_train = TabularDataset('titanic/train.csv')\ndf_test = TabularDataset('titanic/test.csv')\n!rm titanic.zip\n!rm -rf titanic/\n\nDownloading titanic.zip to /home/cgb2/Dropbox/07_lectures/2023-09-MP2023/posts\n  0%|                                               | 0.00/34.1k [00:00&lt;?, ?B/s]\n100%|██████████████████████████████████████| 34.1k/34.1k [00:00&lt;00:00, 21.5MB/s]\nArchive:  titanic.zip\n  inflating: ./titanic/gender_submission.csv  \n  inflating: ./titanic/test.csv      \n  inflating: ./titanic/train.csv     \n\n\n- 피처엔지니어링\n\n_df_train = df_train.eval('Fsize = SibSp + Parch').drop(['SibSp','Parch'],axis=1)\n_df_test = df_test.eval('Fsize = SibSp + Parch').drop(['SibSp','Parch'],axis=1)"
  },
  {
    "objectID": "posts/02wk-007.html#b.-predictor-생성",
    "href": "posts/02wk-007.html#b.-predictor-생성",
    "title": "02wk-007: 타이타닉 / Autogluon (Fsize,Drop)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231024_084434\""
  },
  {
    "objectID": "posts/02wk-007.html#c.-적합fit",
    "href": "posts/02wk-007.html#c.-적합fit",
    "title": "02wk-007: 타이타닉 / Autogluon (Fsize,Drop)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(_df_train) # 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231024_084434\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   265.23 GB / 490.57 GB (54.1%)\nTrain Data Rows:    891\nTrain Data Columns: 10\nLabel Column: Survived\nPreprocessing data ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/learner/default_learner.py:215: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    123682.78 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/features/generators/fillna.py:58: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  X.fillna(self._fillna_feature_map, inplace=True, downcast=False)\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    10 features in original data used to generate 27 features in processed data.\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2417.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.18s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8101   = Validation score   (accuracy)\n    0.45s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8268   = Validation score   (accuracy)\n    0.23s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.47s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8212   = Validation score   (accuracy)\n    0.33s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: CatBoost ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8268   = Validation score   (accuracy)\n    0.69s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8045   = Validation score   (accuracy)\n    0.78s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.7989   = Validation score   (accuracy)\n    0.78s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\nNo improvement since epoch 9: early stopping\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n    0.8268   = Validation score   (accuracy)\n    1.92s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n    0.8212   = Validation score   (accuracy)\n    0.21s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.838    = Validation score   (accuracy)\n    5.24s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBMLarge ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8268   = Validation score   (accuracy)\n    0.4s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8603   = Validation score   (accuracy)\n    0.46s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 12.83s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231024_084434\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7ff0efb17a00&gt;\n\n\n- 리더보드확인 (모의고사 채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.860335       0.069945  7.157853                0.000595           0.459994            2       True         14\n1        NeuralNetTorch   0.837989       0.021271  5.238818                0.021271           5.238818            1       True         12\n2         LightGBMLarge   0.826816       0.003424  0.397473                0.003424           0.397473            1       True         13\n3              CatBoost   0.826816       0.004246  0.686450                0.004246           0.686450            1       True          7\n4              LightGBM   0.826816       0.004521  0.231312                0.004521           0.231312            1       True          4\n5       NeuralNetFastAI   0.826816       0.010309  1.919406                0.010309           1.919406            1       True         10\n6               XGBoost   0.821229       0.004762  0.214719                0.004762           0.214719            1       True         11\n7      RandomForestEntr   0.821229       0.052209  0.334242                0.052209           0.334242            1       True          6\n8      RandomForestGini   0.815642       0.099201  0.473848                0.099201           0.473848            1       True          5\n9            LightGBMXT   0.810056       0.003292  0.445553                0.003292           0.445553            1       True          3\n10       ExtraTreesGini   0.804469       0.100738  0.783195                0.100738           0.783195            1       True          8\n11       ExtraTreesEntr   0.798883       0.040266  0.782176                0.040266           0.782176            1       True          9\n12       KNeighborsDist   0.653631       0.002721  0.008844                0.002721           0.008844            1       True          2\n13       KNeighborsUnif   0.653631       0.028206  0.010227                0.028206           0.010227            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.860335\n0.069945\n7.157853\n0.000595\n0.459994\n2\nTrue\n14\n\n\n1\nNeuralNetTorch\n0.837989\n0.021271\n5.238818\n0.021271\n5.238818\n1\nTrue\n12\n\n\n2\nLightGBMLarge\n0.826816\n0.003424\n0.397473\n0.003424\n0.397473\n1\nTrue\n13\n\n\n3\nCatBoost\n0.826816\n0.004246\n0.686450\n0.004246\n0.686450\n1\nTrue\n7\n\n\n4\nLightGBM\n0.826816\n0.004521\n0.231312\n0.004521\n0.231312\n1\nTrue\n4\n\n\n5\nNeuralNetFastAI\n0.826816\n0.010309\n1.919406\n0.010309\n1.919406\n1\nTrue\n10\n\n\n6\nXGBoost\n0.821229\n0.004762\n0.214719\n0.004762\n0.214719\n1\nTrue\n11\n\n\n7\nRandomForestEntr\n0.821229\n0.052209\n0.334242\n0.052209\n0.334242\n1\nTrue\n6\n\n\n8\nRandomForestGini\n0.815642\n0.099201\n0.473848\n0.099201\n0.473848\n1\nTrue\n5\n\n\n9\nLightGBMXT\n0.810056\n0.003292\n0.445553\n0.003292\n0.445553\n1\nTrue\n3\n\n\n10\nExtraTreesGini\n0.804469\n0.100738\n0.783195\n0.100738\n0.783195\n1\nTrue\n8\n\n\n11\nExtraTreesEntr\n0.798883\n0.040266\n0.782176\n0.040266\n0.782176\n1\nTrue\n9\n\n\n12\nKNeighborsDist\n0.653631\n0.002721\n0.008844\n0.002721\n0.008844\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.653631\n0.028206\n0.010227\n0.028206\n0.010227\n1\nTrue\n1\n\n\n\n\n\n\n\n- validation set의 의미:"
  },
  {
    "objectID": "posts/02wk-007.html#d.-예측-predict",
    "href": "posts/02wk-007.html#d.-예측-predict",
    "title": "02wk-007: 타이타닉 / Autogluon (Fsize,Drop)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(df_train.Survived == predictr.predict(_df_train)).mean()\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/features/generators/fillna.py:58: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  X.fillna(self._fillna_feature_map, inplace=True, downcast=False)\n\n\n0.9438832772166106\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\n# df_test.assign(Survived = predictr.predict(_df_test)).loc[:,['PassengerId','Survived']]\\\n# .to_csv(\"autogluon(Fsize,Drop)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/06wk-024.html",
    "href": "posts/06wk-024.html",
    "title": "06wk-024: 취업(다중공선성) / RidgeCV",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport sklearn.linear_model\n\n\n\n3. Data\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nnp.random.seed(43052)\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\n\n\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\n\n4. RidgeCV\n- RidgeCV 클래스에서 모형을 선택해보자.\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.RidgeCV()\n## step3\npredictr.fit(X,y)\n## step4 -- pass \n\nRidgeCV()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeCVRidgeCV()\n\n\n\npredictr.score(X,y)\n\n0.9999996840224911\n\n\n\npredictr.score(XX,yy)\n\n0.1191494594976158\n\n\n- alpha들의 후보를 우리가 직접 선정하자.\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.RidgeCV(alphas=[5e2, 5e3, 5e4, 5e5, 5e6, 5e7, 5e8])\n## step3\npredictr.fit(X,y)\n## step4 -- pass \n\nRidgeCV(alphas=[500.0, 5000.0, 50000.0, 500000.0, 5000000.0, 50000000.0,\n                500000000.0])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeCVRidgeCV(alphas=[500.0, 5000.0, 50000.0, 500000.0, 5000000.0, 50000000.0,\n                500000000.0])\n\n\n\npredictr.score(X,y)\n\n0.7521268560159359\n\n\n\npredictr.score(XX,yy)\n\n0.7450309251010895\n\n\n\npredictr.alpha_\n\n50000000.0\n\n\n참고로 이 적합결과는 아래의 코드를 실행한것과 같다\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.Ridge(alpha=50000000.0)\n## step3\npredictr.fit(X,y)\n## step4 -- pass \n\nRidge(alpha=50000000.0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=50000000.0)\n\n\n\npredictr.score(X,y)\n\n0.752126856015936\n\n\n\npredictr.score(XX,yy)\n\n0.7450309251010895"
  },
  {
    "objectID": "posts/02wk-008.html#a.-데이터",
    "href": "posts/02wk-008.html#a.-데이터",
    "title": "02wk-008: 타이타닉 / Autogluon (best_quality)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\n!kaggle competitions download -c titanic\n!unzip titanic.zip -d ./titanic\ndf_train = TabularDataset('titanic/train.csv')\ndf_test = TabularDataset('titanic/test.csv')\n!rm titanic.zip\n!rm -rf titanic/\n\nDownloading titanic.zip to /home/cgb2/Dropbox/07_lectures/2023-09-MP2023/posts\n  0%|                                               | 0.00/34.1k [00:00&lt;?, ?B/s]\n100%|██████████████████████████████████████| 34.1k/34.1k [00:00&lt;00:00, 5.34MB/s]\nArchive:  titanic.zip\n  inflating: ./titanic/gender_submission.csv  \n  inflating: ./titanic/test.csv      \n  inflating: ./titanic/train.csv"
  },
  {
    "objectID": "posts/02wk-008.html#b.-predictor-생성",
    "href": "posts/02wk-008.html#b.-predictor-생성",
    "title": "02wk-008: 타이타닉 / Autogluon (best_quality)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231024_084544\""
  },
  {
    "objectID": "posts/02wk-008.html#c.-적합fit",
    "href": "posts/02wk-008.html#c.-적합fit",
    "title": "02wk-008: 타이타닉 / Autogluon (best_quality)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(df_train,presets='best_quality') # 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nPresets specified: ['best_quality']\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nStack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231024_084544\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   265.20 GB / 490.57 GB (54.1%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/learner/default_learner.py:215: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    123275.96 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/features/generators/fillna.py:58: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  X.fillna(self._fillna_feature_map, inplace=True, downcast=False)\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2417.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.21s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif_BAG_L1 ...\n    0.6308   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.03s    = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ...\n    0.6364   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ...\nWill use sequential fold fitting strategy because import of ray failed. Reason: ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install ray==2.6.3`\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.835    = Validation score   (accuracy)\n    1.39s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8406   = Validation score   (accuracy)\n    1.18s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ...\n    0.8373   = Validation score   (accuracy)\n    0.6s     = Training   runtime\n    0.09s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ...\n    0.8361   = Validation score   (accuracy)\n    0.36s    = Training   runtime\n    0.07s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8608   = Validation score   (accuracy)\n    4.16s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ...\n    0.8294   = Validation score   (accuracy)\n    0.69s    = Training   runtime\n    0.07s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ...\n    0.8328   = Validation score   (accuracy)\n    0.65s    = Training   runtime\n    0.11s    = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\nNo improvement since epoch 7: early stopping\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\nNo improvement since epoch 6: early stopping\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\nNo improvement since epoch 7: early stopping\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n    0.853    = Validation score   (accuracy)\n    10.32s   = Training   runtime\n    0.14s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n    0.8406   = Validation score   (accuracy)\n    1.14s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8462   = Validation score   (accuracy)\n    31.34s   = Training   runtime\n    0.15s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2193.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2195.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8406   = Validation score   (accuracy)\n    3.43s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8608   = Validation score   (accuracy)\n    0.61s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 57.87s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231024_084544\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7fd0b87ab370&gt;\n\n\n- 리더보드확인 (모의고사채점)\n\npredictr.leaderboard()\n\n                      model  score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           CatBoost_BAG_L1   0.860831       0.028274   4.163593                0.028274           4.163593            1       True          7\n1       WeightedEnsemble_L2   0.860831       0.029458   4.777558                0.001185           0.613964            2       True         14\n2    NeuralNetFastAI_BAG_L1   0.852974       0.140585  10.323885                0.140585          10.323885            1       True         10\n3     NeuralNetTorch_BAG_L1   0.846240       0.148010  31.344297                0.148010          31.344297            1       True         12\n4           LightGBM_BAG_L1   0.840629       0.022044   1.180209                0.022044           1.180209            1       True          4\n5      LightGBMLarge_BAG_L1   0.840629       0.025207   3.430948                0.025207           3.430948            1       True         13\n6            XGBoost_BAG_L1   0.840629       0.032869   1.136343                0.032869           1.136343            1       True         11\n7   RandomForestGini_BAG_L1   0.837262       0.093387   0.595516                0.093387           0.595516            1       True          5\n8   RandomForestEntr_BAG_L1   0.836139       0.074692   0.359487                0.074692           0.359487            1       True          6\n9         LightGBMXT_BAG_L1   0.835017       0.023820   1.392500                0.023820           1.392500            1       True          3\n10    ExtraTreesEntr_BAG_L1   0.832772       0.105990   0.649323                0.105990           0.649323            1       True          9\n11    ExtraTreesGini_BAG_L1   0.829405       0.071771   0.686410                0.071771           0.686410            1       True          8\n12    KNeighborsDist_BAG_L1   0.636364       0.014100   0.004636                0.014100           0.004636            1       True          2\n13    KNeighborsUnif_BAG_L1   0.630752       0.031787   0.003633                0.031787           0.003633            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nCatBoost_BAG_L1\n0.860831\n0.028274\n4.163593\n0.028274\n4.163593\n1\nTrue\n7\n\n\n1\nWeightedEnsemble_L2\n0.860831\n0.029458\n4.777558\n0.001185\n0.613964\n2\nTrue\n14\n\n\n2\nNeuralNetFastAI_BAG_L1\n0.852974\n0.140585\n10.323885\n0.140585\n10.323885\n1\nTrue\n10\n\n\n3\nNeuralNetTorch_BAG_L1\n0.846240\n0.148010\n31.344297\n0.148010\n31.344297\n1\nTrue\n12\n\n\n4\nLightGBM_BAG_L1\n0.840629\n0.022044\n1.180209\n0.022044\n1.180209\n1\nTrue\n4\n\n\n5\nLightGBMLarge_BAG_L1\n0.840629\n0.025207\n3.430948\n0.025207\n3.430948\n1\nTrue\n13\n\n\n6\nXGBoost_BAG_L1\n0.840629\n0.032869\n1.136343\n0.032869\n1.136343\n1\nTrue\n11\n\n\n7\nRandomForestGini_BAG_L1\n0.837262\n0.093387\n0.595516\n0.093387\n0.595516\n1\nTrue\n5\n\n\n8\nRandomForestEntr_BAG_L1\n0.836139\n0.074692\n0.359487\n0.074692\n0.359487\n1\nTrue\n6\n\n\n9\nLightGBMXT_BAG_L1\n0.835017\n0.023820\n1.392500\n0.023820\n1.392500\n1\nTrue\n3\n\n\n10\nExtraTreesEntr_BAG_L1\n0.832772\n0.105990\n0.649323\n0.105990\n0.649323\n1\nTrue\n9\n\n\n11\nExtraTreesGini_BAG_L1\n0.829405\n0.071771\n0.686410\n0.071771\n0.686410\n1\nTrue\n8\n\n\n12\nKNeighborsDist_BAG_L1\n0.636364\n0.014100\n0.004636\n0.014100\n0.004636\n1\nTrue\n2\n\n\n13\nKNeighborsUnif_BAG_L1\n0.630752\n0.031787\n0.003633\n0.031787\n0.003633\n1\nTrue\n1"
  },
  {
    "objectID": "posts/02wk-008.html#d.-예측-predict",
    "href": "posts/02wk-008.html#d.-예측-predict",
    "title": "02wk-008: 타이타닉 / Autogluon (best_quality)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(df_train.Survived == predictr.predict(df_train)).mean()\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/features/generators/fillna.py:58: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  X.fillna(self._fillna_feature_map, inplace=True, downcast=False)\n\n\n0.898989898989899\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\n# df_test.assign(Survived = predictr.predict(df_test)).loc[:,['PassengerId','Survived']]\\\n# .to_csv(\"autogluon(best_quality)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/06wk-023.html#a.-정확한-설명",
    "href": "posts/06wk-023.html#a.-정확한-설명",
    "title": "06wk-023: 취업(다중공선성) / Ridge – 추가해설",
    "section": "A. 정확한 설명",
    "text": "A. 정확한 설명\n- SVD를 이용하여 이론적인 계산하면 sklearn.linear_model.LinearRegression()로 적합한 결과보다 sklearn.linear_model.Ridge()로 적합한 결과를 더 좋게 만드는 \\(\\alpha\\)가 항상 존재함을 증명할 수 있음."
  },
  {
    "objectID": "posts/06wk-023.html#b.-직관적-설명-엄밀하지-않은-설명",
    "href": "posts/06wk-023.html#b.-직관적-설명-엄밀하지-않은-설명",
    "title": "06wk-023: 취업(다중공선성) / Ridge – 추가해설",
    "section": "B. 직관적 설명 (엄밀하지 않은 설명)",
    "text": "B. 직관적 설명 (엄밀하지 않은 설명)\n\nStep1: LinearRegression은 왜 망했는가?\n- 토익의 계수는 실제로 \\(\\frac{1}{100}\\)이다. 적딩히\n\ntoeic_coef + … + toeic499_coef \\(\\approx\\) 0.01 이라면\n\n대충 맞는 답이다.\n\n## step1 --- toeic, gpa 만 남기고 나머지 변수를 삭제\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\npredictr = sklearn.linear_model.LinearRegression()\n## step3 \npredictr.fit(X,y)\n## step4 -- pass \n#---# \nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 1.0000\ntest_score: 0.1171\n\n\n\ns= pd.Series(predictr.coef_)\ns.index = X.columns\ns[1:].sum()\n\n0.010302732920632579\n\n\n- 그런데 사실 저 0.01이라는 값은 몇개의 계수만 있어도 만들 수 있다. (toeic2와 toeic3에 해당하는 계수)\n\ns['toeic2']+s['toeic3']\n\n0.010142832986793252\n\n\n- 이런논리로 치면 toeic2, toeic3에 해당하는 계수만 있다면 사실 \\(y\\)를 설명하는데 충분했고, 나머지는 불필요한 특징이 된다. (그리고 불필요한 특징은 오버피팅을 유발한다)\n\n\nStep2: Ridge의 아이디어\n- Ridge의 아이디어: toeic2, toeic3 와 같이 몇개의 변수로만 0.01이라는 수를 설명할 수 없도록 “강제”하자. 즉 몇개의 변수로만 0.01이라는 수를 설명할 수 없도록 “패널티”를 주자.\n- 패널티: 유사토익들의 계수값을 제곱한뒤 합치고(=L2-norm을 구하고), 그 값이 0에서 떨어져 있을 수록 패널티를 줄꺼야!\n\n이러한 패널티를 줄 경우 결과적으로 0.01의 값이 “동일하게 나누어져서” 나오는 값(=\\(\\frac{1}{100}\\frac{1}{501}\\))이 계수값으로 추정된다. (왜? – 요건 정확하게 이해하는게 사실 지금은 힘듬)\n\n- 잘 적용된 Ridge의 결과를 보면 아래와 같이 계수값이 저장되어 있음.\n\n## step1 --- toeic, gpa 만 남기고 나머지 변수를 삭제\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\npredictr = sklearn.linear_model.Ridge(alpha=5e8)\n## step3 \npredictr.fit(X,y)\n## step4 -- pass \n#---# \nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 0.7507\ntest_score: 0.7438\n\n\n\ns = pd.Series(predictr.coef_)\ns.index = X.columns\ns\n\ngpa         0.000001\ntoeic       0.000019\ntoeic0      0.000018\ntoeic1      0.000018\ntoeic2      0.000019\n              ...   \ntoeic495    0.000018\ntoeic496    0.000019\ntoeic497    0.000019\ntoeic498    0.000019\ntoeic499    0.000019\nLength: 502, dtype: float64\n\n\n\n0.01/501\n\n1.9960079840319362e-05\n\n\n- 결국 Ridge를 사용하면 계수들의 값이 “동일하게 나누어지는” 효과가 나타남\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\n모든 계수가 “엔빵”된다는 의미는 아니에요. 예를들어 X1=toeic, X2≈toeic*2 라면?\n\n\n- 패널티를 주는 정도? \\(\\alpha\\)로 조절함.. \\(\\alpha\\)를 크게 할수록 패널티를 많이줌"
  },
  {
    "objectID": "posts/06wk-023.html#c.-alpha에-따른-계수값-변화",
    "href": "posts/06wk-023.html#c.-alpha에-따른-계수값-변화",
    "title": "06wk-023: 취업(다중공선성) / Ridge – 추가해설",
    "section": "C. \\(\\alpha\\)에 따른 계수값 변화",
    "text": "C. \\(\\alpha\\)에 따른 계수값 변화\n- 여러개의 predictor 학습\n\n## step1 --- toeic, gpa 만 남기고 나머지 변수를 삭제\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\nalphas = [5e2, 5e3, 5e4, 5e5, 5e6, 5e7, 5e8]\npredictrs = [sklearn.linear_model.Ridge(alpha=alpha) for alpha in alphas]\n## step3 \nfor predictr in predictrs:\n    predictr.fit(X,y)\n## step4 -- pass \n\n- 계수값 시각화\n\npredictrs[0].alpha\n\n500.0\n\n\n\nplt.plot(predictrs[0].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[0].alpha))\nplt.plot(predictrs[3].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[1].alpha))\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f671255c340&gt;\n\n\n\n\n\n\nplt.plot(predictrs[3].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[3].alpha))\nplt.plot(predictrs[5].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[5].alpha))\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f67129dd690&gt;\n\n\n\n\n\n\nplt.plot(predictrs[5].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[5].alpha))\nplt.plot(predictrs[-1].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[-1].alpha))\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f6712a4fd30&gt;\n\n\n\n\n\n- 직관: 마지막 predictor의 계수값을 살펴보자.\n\npredictrs[-1].coef_\n\narray([1.10421248e-06, 1.89938091e-05, 1.77768343e-05, 1.82118332e-05,\n       1.90895673e-05, 1.87128138e-05, 1.90343037e-05, 1.82483251e-05,\n       1.90405022e-05, 1.85802628e-05, 1.90021086e-05, 1.88952130e-05,\n       1.96003229e-05, 1.89154663e-05, 1.86638217e-05, 1.92666606e-05,\n       1.97107043e-05, 1.92214868e-05, 1.92961317e-05, 1.93321368e-05,\n       1.92194541e-05, 1.85663279e-05, 1.86805137e-05, 1.81649873e-05,\n       1.78656367e-05, 1.83171419e-05, 1.94428947e-05, 1.89710925e-05,\n       2.00598946e-05, 1.88384883e-05, 1.98903125e-05, 1.81113551e-05,\n       1.85043847e-05, 1.84424971e-05, 1.91508275e-05, 1.97427867e-05,\n       1.93598061e-05, 1.98264264e-05, 1.89934042e-05, 1.84770850e-05,\n       1.83617634e-05, 1.79346774e-05, 1.84943159e-05, 1.89803006e-05,\n       1.78633749e-05, 1.80073666e-05, 1.85664525e-05, 1.97390143e-05,\n       1.86574281e-05, 1.92233226e-05, 1.91281904e-05, 1.85617627e-05,\n       1.83939489e-05, 1.84309427e-05, 1.88142167e-05, 1.84159665e-05,\n       1.94078579e-05, 1.84515402e-05, 1.88107980e-05, 1.85889903e-05,\n       1.89357356e-05, 1.88750847e-05, 1.92107444e-05, 1.81799279e-05,\n       1.92122152e-05, 1.97863670e-05, 1.89851436e-05, 1.88974919e-05,\n       1.88566578e-05, 1.95841935e-05, 1.86398380e-05, 1.95801159e-05,\n       1.87550098e-05, 1.87392625e-05, 1.87462595e-05, 1.96056001e-05,\n       1.80626630e-05, 1.88237701e-05, 1.83108446e-05, 1.88087164e-05,\n       1.84723703e-05, 1.84767748e-05, 1.89267252e-05, 1.87604297e-05,\n       1.86945591e-05, 1.92924236e-05, 1.77843453e-05, 1.85415541e-05,\n       1.91448999e-05, 1.98281375e-05, 1.97994651e-05, 1.86653004e-05,\n       1.87298830e-05, 1.87474975e-05, 1.90018315e-05, 1.92043808e-05,\n       1.88941675e-05, 1.81646176e-05, 1.91508494e-05, 2.04322537e-05,\n       1.92111546e-05, 1.93061022e-05, 1.92088349e-05, 1.80206353e-05,\n       1.89399818e-05, 1.96895533e-05, 1.94410839e-05, 1.92051217e-05,\n       1.84961416e-05, 1.89785667e-05, 1.92235780e-05, 1.86729143e-05,\n       1.88439733e-05, 1.76776615e-05, 1.87493841e-05, 1.86986837e-05,\n       1.81917859e-05, 1.94657238e-05, 1.82063420e-05, 1.78143049e-05,\n       1.88432683e-05, 1.90674860e-05, 1.86411824e-05, 1.93286721e-05,\n       1.75163829e-05, 1.86852659e-05, 2.02343956e-05, 1.82025623e-05,\n       1.89153395e-05, 1.98862774e-05, 1.94775038e-05, 1.90665531e-05,\n       1.94170642e-05, 1.88227118e-05, 1.88792179e-05, 1.89712787e-05,\n       1.87855482e-05, 1.87895464e-05, 2.00798925e-05, 1.97167119e-05,\n       1.91644137e-05, 1.90990710e-05, 1.85836048e-05, 1.82346595e-05,\n       1.85731253e-05, 1.84871242e-05, 1.90728256e-05, 1.90277156e-05,\n       1.93085319e-05, 1.91719254e-05, 1.80097271e-05, 1.82517485e-05,\n       1.90904218e-05, 1.85232604e-05, 1.88184612e-05, 1.84002976e-05,\n       2.00337440e-05, 1.86478638e-05, 1.93507546e-05, 1.85547358e-05,\n       1.97154574e-05, 1.91189346e-05, 1.93320777e-05, 1.85313268e-05,\n       1.91085306e-05, 1.88406812e-05, 1.87444892e-05, 1.96637559e-05,\n       1.83552699e-05, 1.80759243e-05, 1.94662845e-05, 1.93761303e-05,\n       1.98339288e-05, 1.87139235e-05, 1.91131387e-05, 1.85801855e-05,\n       1.91544816e-05, 1.98413649e-05, 1.84027849e-05, 1.81842651e-05,\n       1.95888229e-05, 1.80738476e-05, 1.92457286e-05, 1.91474170e-05,\n       1.88737956e-05, 1.78029998e-05, 1.97734483e-05, 1.92409710e-05,\n       1.97346045e-05, 1.99425451e-05, 1.89157923e-05, 1.82538525e-05,\n       1.87475300e-05, 1.79663692e-05, 1.94360535e-05, 1.93333725e-05,\n       1.81368431e-05, 1.91860664e-05, 2.03648683e-05, 1.92870391e-05,\n       1.92561212e-05, 1.92408929e-05, 1.77556464e-05, 1.89317813e-05,\n       1.95230859e-05, 1.91845519e-05, 1.88923023e-05, 1.88368476e-05,\n       1.89013580e-05, 1.82113056e-05, 1.86295402e-05, 1.92236940e-05,\n       1.80025543e-05, 1.92322271e-05, 1.80917953e-05, 1.87188051e-05,\n       1.93772655e-05, 1.87894009e-05, 1.86773984e-05, 1.96830961e-05,\n       1.94593808e-05, 1.99377297e-05, 1.85707832e-05, 1.88667594e-05,\n       1.85589760e-05, 1.98498326e-05, 1.88878514e-05, 1.90686529e-05,\n       1.86868639e-05, 1.90576790e-05, 1.95494214e-05, 1.86567117e-05,\n       1.85992014e-05, 1.77199587e-05, 1.82193592e-05, 1.90965903e-05,\n       1.96016869e-05, 1.88116657e-05, 1.81131528e-05, 1.85436209e-05,\n       1.92951259e-05, 1.92495993e-05, 1.84570073e-05, 1.94529446e-05,\n       1.92760629e-05, 1.92236816e-05, 1.85750512e-05, 1.95451343e-05,\n       1.82912208e-05, 1.88851896e-05, 1.86295173e-05, 1.84150640e-05,\n       1.95101106e-05, 1.98423439e-05, 1.88687440e-05, 1.91657943e-05,\n       1.89387389e-05, 1.89907539e-05, 1.90653825e-05, 1.80854343e-05,\n       1.86906336e-05, 1.85793308e-05, 1.84992786e-05, 1.93964742e-05,\n       1.83344151e-05, 1.89611068e-05, 1.91457644e-05, 1.88755070e-05,\n       1.98511526e-05, 1.93068196e-05, 1.93316489e-05, 1.89507435e-05,\n       1.89083004e-05, 1.91358509e-05, 1.87803906e-05, 1.78160168e-05,\n       1.94603877e-05, 2.02569965e-05, 1.87423291e-05, 1.94609617e-05,\n       1.91292677e-05, 1.85958571e-05, 1.88629266e-05, 1.90600256e-05,\n       1.82221314e-05, 1.95093258e-05, 1.89176339e-05, 2.00028045e-05,\n       1.94052035e-05, 1.86744967e-05, 1.89125601e-05, 2.02089363e-05,\n       1.80569192e-05, 2.02141130e-05, 1.93147541e-05, 1.89011113e-05,\n       1.93335891e-05, 1.96767360e-05, 1.90364715e-05, 1.94635849e-05,\n       1.90397143e-05, 1.91973258e-05, 1.85857694e-05, 1.91487106e-05,\n       1.92897509e-05, 1.99589223e-05, 1.89690091e-05, 1.90089893e-05,\n       1.80391078e-05, 1.89867708e-05, 1.91430968e-05, 1.92719424e-05,\n       1.95648244e-05, 1.85975115e-05, 1.92077870e-05, 1.84415844e-05,\n       1.88715614e-05, 1.85970322e-05, 1.93261490e-05, 1.86726361e-05,\n       1.97716032e-05, 1.92749150e-05, 2.00954709e-05, 1.90876286e-05,\n       1.89190693e-05, 1.98831620e-05, 1.91612367e-05, 1.86269524e-05,\n       1.89155394e-05, 1.89824518e-05, 1.98347756e-05, 1.86788886e-05,\n       1.83508292e-05, 1.85069060e-05, 1.86909372e-05, 1.85978543e-05,\n       1.88150510e-05, 1.89755849e-05, 1.90099289e-05, 1.90515657e-05,\n       1.93189513e-05, 1.82151178e-05, 1.78471089e-05, 1.91763316e-05,\n       1.84903926e-05, 1.92863572e-05, 1.90497739e-05, 1.87657428e-05,\n       1.87801680e-05, 1.85137448e-05, 1.91226761e-05, 1.94084785e-05,\n       1.81950620e-05, 1.81823646e-05, 1.87513814e-05, 1.97922951e-05,\n       1.87200102e-05, 1.98409879e-05, 1.85874173e-05, 1.90513332e-05,\n       1.85234477e-05, 1.81902197e-05, 1.76367508e-05, 1.90389194e-05,\n       1.85299355e-05, 1.95358518e-05, 1.81772601e-05, 1.93671350e-05,\n       1.91528856e-05, 1.91322975e-05, 1.85830738e-05, 1.85626882e-05,\n       1.86250726e-05, 1.84514809e-05, 1.86800234e-05, 1.89256964e-05,\n       1.90280385e-05, 1.88870537e-05, 1.86929332e-05, 1.95167742e-05,\n       1.86377119e-05, 1.93693632e-05, 1.94429807e-05, 1.90730542e-05,\n       1.86276638e-05, 1.86225787e-05, 1.87333026e-05, 1.94293224e-05,\n       1.87174307e-05, 1.93106731e-05, 1.91898445e-05, 1.91446507e-05,\n       1.83627209e-05, 1.85185991e-05, 1.90680366e-05, 1.88180597e-05,\n       1.86586581e-05, 1.80051184e-05, 1.83329730e-05, 1.82088945e-05,\n       1.87516598e-05, 1.82744310e-05, 1.90219092e-05, 1.89098591e-05,\n       1.89001214e-05, 1.90959896e-05, 1.77157866e-05, 1.91760361e-05,\n       1.80496598e-05, 1.85629242e-05, 1.93527162e-05, 1.85046434e-05,\n       1.97977476e-05, 1.82757747e-05, 1.92849021e-05, 1.86829990e-05,\n       1.86752898e-05, 1.95540241e-05, 1.92250030e-05, 1.84817730e-05,\n       1.94636774e-05, 1.86057300e-05, 1.90096458e-05, 1.91037821e-05,\n       1.98095086e-05, 1.92558748e-05, 1.94175627e-05, 1.86155519e-05,\n       1.91386204e-05, 1.89659072e-05, 1.89507918e-05, 1.88868989e-05,\n       1.91223138e-05, 1.81488441e-05, 1.95885497e-05, 1.87850789e-05,\n       1.90457546e-05, 1.96549561e-05, 1.86983597e-05, 1.89788151e-05,\n       1.98384237e-05, 1.99479277e-05, 1.91275095e-05, 1.89970341e-05,\n       1.85749782e-05, 1.91683345e-05, 1.91850806e-05, 1.97386011e-05,\n       1.93320833e-05, 1.92560345e-05, 1.85426153e-05, 1.85185853e-05,\n       1.85764448e-05, 1.94279426e-05, 1.97685699e-05, 1.91733090e-05,\n       1.84972022e-05, 1.89924907e-05, 1.83467563e-05, 1.95149016e-05,\n       1.84410610e-05, 1.86536281e-05, 1.88181888e-05, 1.85487807e-05,\n       1.88565643e-05, 1.89056942e-05, 1.95082352e-05, 1.91711709e-05,\n       1.91422027e-05, 1.91363321e-05, 1.89114818e-05, 1.85390554e-05,\n       1.92949067e-05, 1.88019353e-05, 1.85332879e-05, 1.86699430e-05,\n       1.96934870e-05, 2.01293426e-05, 1.81411289e-05, 1.86806981e-05,\n       1.90987154e-05, 1.85866377e-05, 1.96875267e-05, 1.88785203e-05,\n       1.94435510e-05, 1.85812461e-05, 1.97178935e-05, 1.90067232e-05,\n       2.02306858e-05, 1.86213361e-05, 1.94255182e-05, 1.86417320e-05,\n       1.95689564e-05, 1.97728792e-05, 1.94352125e-05, 1.93768903e-05,\n       1.90643113e-05, 1.79709383e-05, 1.90573271e-05, 1.85638225e-05,\n       1.91337229e-05, 1.86437625e-05])\n\n\n\n불필요한 변수가 나올 수 없는 구조가 되어버렸음 (한 두개로 0.01을 만들 수 없음)\n모든 변수는 대략 2e-5(\\(\\approx \\frac{1}{100}\\frac{1}{501}\\))정도 만큼 똑같이 중요하다고 생각된다.\n고급: 살짝 1/(100*501)보다 전체적으로 값이 작아보이는데, 이는 기분탓이 아님 (Ridge 특징)\n\n\n1/100*1/501\n\n1.9960079840319362e-05"
  },
  {
    "objectID": "posts/06wk-023.html#d.-alpha에-따른-실험내용-정리",
    "href": "posts/06wk-023.html#d.-alpha에-따른-실험내용-정리",
    "title": "06wk-023: 취업(다중공선성) / Ridge – 추가해설",
    "section": "D. \\(\\alpha\\)에 따른 실험내용 정리",
    "text": "D. \\(\\alpha\\)에 따른 실험내용 정리\n- 예비개념: L2-penalty는 그냥 대충 분산같은것..\n\nx = np.random.randn(5)\nl2_penalty = (x**2).sum()\nl2_penalty, 5*(x.var()+(x.mean()**2))\n\n(7.084638326878741, 7.084638326878739)\n\n\n- \\(\\alpha\\)가 커질수록 생기는 일\n\n크게 느낀것: 계수들의 값이 점점 비슷해짐 –&gt; 계수들의 값들을 모아서 분산을 구하면 작아진다는 의미 –&gt; L2-penalty 가 작아진다는 의미\n미묘하게 느껴지는 점: toeic, 그리고 toeic0~toeic499 까지의 계수총합은 0.01이 되어야 하는데, 그 총합이 미묘하게 작어지는 느낌.\n\n\nfor predictr in predictrs: \n    print(\n        f'alpha={predictr.alpha:.2e}\\t'\n        f'l2_penalty={((predictr.coef_)**2).sum():.6f}\\t'\n        f'sum(toeic_coefs)={((predictr.coef_[1:])).sum():.4f}\\t'\n        f'test_score={predictr.score(XX,yy):.4f}'\n    )\n\nalpha=5.00e+02  l2_penalty=0.046715 sum(toeic_coefs)=0.0103 test_score=0.2026\nalpha=5.00e+03  l2_penalty=0.021683 sum(toeic_coefs)=0.0102 test_score=0.4638\nalpha=5.00e+04  l2_penalty=0.003263 sum(toeic_coefs)=0.0099 test_score=0.6889\nalpha=5.00e+05  l2_penalty=0.000109 sum(toeic_coefs)=0.0099 test_score=0.7407\nalpha=5.00e+06  l2_penalty=0.000002 sum(toeic_coefs)=0.0099 test_score=0.7447\nalpha=5.00e+07  l2_penalty=0.000000 sum(toeic_coefs)=0.0098 test_score=0.7450\nalpha=5.00e+08  l2_penalty=0.000000 sum(toeic_coefs)=0.0095 test_score=0.7438\n\n\n- L2-penalty의 느낌은 대충 아래와 같이 분산으로 이해해도 무방\n\nfor predictr in predictrs: \n    print(\n        f'alpha={predictr.alpha:.2e}\\t'\n        f'var(coefs)={(predictr.coef_).var()*501:.6f}\\t'\n        f'sum(toeic_coefs)={((predictr.coef_[1:])).sum():.4f}\\t'\n        f'test_score={predictr.score(XX,yy):.4f}'\n    )\n\nalpha=5.00e+02  var(coefs)=0.046618 sum(toeic_coefs)=0.0103 test_score=0.2026\nalpha=5.00e+03  var(coefs)=0.021638 sum(toeic_coefs)=0.0102 test_score=0.4638\nalpha=5.00e+04  var(coefs)=0.003256 sum(toeic_coefs)=0.0099 test_score=0.6889\nalpha=5.00e+05  var(coefs)=0.000109 sum(toeic_coefs)=0.0099 test_score=0.7407\nalpha=5.00e+06  var(coefs)=0.000001 sum(toeic_coefs)=0.0099 test_score=0.7447\nalpha=5.00e+07  var(coefs)=0.000000 sum(toeic_coefs)=0.0098 test_score=0.7450\nalpha=5.00e+08  var(coefs)=0.000000 sum(toeic_coefs)=0.0095 test_score=0.7438"
  },
  {
    "objectID": "posts/06wk-023.html#e.-alpha가-크다고-무조건-좋은건-아니다.",
    "href": "posts/06wk-023.html#e.-alpha가-크다고-무조건-좋은건-아니다.",
    "title": "06wk-023: 취업(다중공선성) / Ridge – 추가해설",
    "section": "E. \\(\\alpha\\)가 크다고 무조건 좋은건 아니다.",
    "text": "E. \\(\\alpha\\)가 크다고 무조건 좋은건 아니다.\n\n## step1 --- toeic, gpa 만 남기고 나머지 변수를 삭제\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\npredictr = sklearn.linear_model.Ridge(alpha=1e12)\n## step3 \npredictr.fit(X,y)\n## step4 -- pass \n\nRidge(alpha=1000000000000.0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=1000000000000.0)\n\n\n\nprint(f'train_score={predictr.score(X,y):.4f}')\nprint(f'test_score={predictr.score(XX,yy):.4f}')\n\ntrain_score=0.0191\ntest_score=0.0140\n\n\n\npredictr.coef_[1:].sum() # 이 값이 0.01이어야 하는데, 많이 작아짐\n\n0.00012585319204891574"
  },
  {
    "objectID": "posts/03wk-011.html#a.-data-정리",
    "href": "posts/03wk-011.html#a.-data-정리",
    "title": "03wk-011: Medical Cost / 회귀분석",
    "section": "A. Data 정리",
    "text": "A. Data 정리\n\ndf_train.columns\n\nIndex(['age', 'sex', 'bmi', 'children', 'smoker', 'region', 'charges'], dtype='object')\n\n\n\nX = pd.get_dummies(df_train.drop(['charges'],axis=1))\ny = df_train[['charges']]\n\n\nX\n\n\n\n\n\n\n\n\nage\nbmi\nchildren\nsex_female\nsex_male\nsmoker_no\nsmoker_yes\nregion_northeast\nregion_northwest\nregion_southeast\nregion_southwest\n\n\n\n\n0\n19\n27.900\n0\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n1\n18\n33.770\n1\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n2\n28\n33.000\n3\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n33\n22.705\n0\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n4\n32\n28.880\n0\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\n30.970\n3\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1334\n18\n31.920\n0\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n1335\n18\n36.850\n0\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n1336\n21\n25.800\n0\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n1337\n61\n29.070\n0\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n\n\n1338 rows × 11 columns\n\n\n\n\ny\n\n\n\n\n\n\n\n\ncharges\n\n\n\n\n0\n16884.92400\n\n\n1\n1725.55230\n\n\n2\n4449.46200\n\n\n3\n21984.47061\n\n\n4\n3866.85520\n\n\n...\n...\n\n\n1333\n10600.54830\n\n\n1334\n2205.98080\n\n\n1335\n1629.83350\n\n\n1336\n2007.94500\n\n\n1337\n29141.36030\n\n\n\n\n1338 rows × 1 columns"
  },
  {
    "objectID": "posts/03wk-011.html#b.-predictor-생성",
    "href": "posts/03wk-011.html#b.-predictor-생성",
    "title": "03wk-011: Medical Cost / 회귀분석",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LinearRegression()"
  },
  {
    "objectID": "posts/03wk-011.html#c.-학습",
    "href": "posts/03wk-011.html#c.-학습",
    "title": "03wk-011: Medical Cost / 회귀분석",
    "section": "C. 학습",
    "text": "C. 학습\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/03wk-011.html#d.-예측",
    "href": "posts/03wk-011.html#d.-예측",
    "title": "03wk-011: Medical Cost / 회귀분석",
    "section": "D. 예측",
    "text": "D. 예측\n\ndf_train.assign(yhat = predictr.predict(X))\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\nyhat\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n25293.713028\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n3448.602834\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n6706.988491\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n3754.830163\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n5592.493386\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\nmale\n30.970\n3\nno\nnorthwest\n10600.54830\n12351.323686\n\n\n1334\n18\nfemale\n31.920\n0\nno\nnortheast\n2205.98080\n3511.930809\n\n\n1335\n18\nfemale\n36.850\n0\nno\nsoutheast\n1629.83350\n4149.132486\n\n\n1336\n21\nfemale\n25.800\n0\nno\nsouthwest\n2007.94500\n1246.584939\n\n\n1337\n61\nfemale\n29.070\n0\nyes\nnorthwest\n29141.36030\n37085.623268\n\n\n\n\n1338 rows × 8 columns"
  },
  {
    "objectID": "posts/03wk-011.html#e.-평가",
    "href": "posts/03wk-011.html#e.-평가",
    "title": "03wk-011: Medical Cost / 회귀분석",
    "section": "E. 평가",
    "text": "E. 평가\n\npredictr.score(X,y) # R^2\n\n0.7509130345985207\n\n\n\n0.7 이상이면 망한모형까지는 아님 (대회용으로는 부적절할 수 있으나 대충 쓸 수는 있는 정도)"
  },
  {
    "objectID": "posts/07wk-033.html",
    "href": "posts/07wk-033.html",
    "title": "07wk-033: 취업(다중공선성) / 의사결정나무",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.model_selection\nimport sklearn.linear_model\nimport sklearn.tree\n\n\n\n3. Data\n\nnp.random.seed(43052)\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\n\n\nX,y = df_train.loc[:,'gpa':],df_train['employment_score']\nXX,yy = df_test.loc[:,'gpa':],df_test['employment_score']\n\n\n실제 kaggle에서는 yy를 모르는 상황임\n\n\n\n4. 분석\n- 분석1: 의사결정나무\n\n## step1 -- pass \n## step2 \npredictr = sklearn.tree.DecisionTreeRegressor(random_state=42)\n## step3 \npredictr.fit(X,y)\n## step4\ndf_train['employment_score_hat'] = predictr.predict(X)\ndf_test['employment_score_hat'] = predictr.predict(XX)\n#---#\nprint(f'train score: {predictr.score(X,y):.4f}')\nprint(f'test score: {predictr.score(XX,yy):.4f}')\n\ntrain score: 1.0000\ntest score: 0.8300\n\n\n\n오버핏이긴한데 나쁘지 않음..\n\n- 분석2: Lasso\n\n## step1 -- pass \n## step2 \npredictr = sklearn.linear_model.LassoCV()\n## step3 \npredictr.fit(X,y)\n## step4\ndf_train['employment_score_hat'] = predictr.predict(X)\ndf_test['employment_score_hat'] = predictr.predict(XX)\n#---#\nprint(f'train score: {predictr.score(X,y):.4f}')\nprint(f'test score: {predictr.score(XX,yy):.4f}')\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.405e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.201e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.824e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.914e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.018e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.785e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.007e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.099e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.120e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.084e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.171e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.346e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.655e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.898e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.112e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.258e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.147e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.844e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.174e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.206e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.447e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.739e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.238e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.561e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.673e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.799e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.659e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.574e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.414e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.699e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.640e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.357e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.353e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.341e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.380e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.561e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.144e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.869e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.042e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.151e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.362e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.393e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.565e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.660e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.788e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.947e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.008e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.337e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.410e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.400e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.299e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.387e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.721e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.116e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.587e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.865e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.174e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.254e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.614e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.732e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.030e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.196e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.200e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.779e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.594e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.524e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.851e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.160e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.410e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.755e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.131e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.388e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.581e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.547e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.379e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.953e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.392e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\ntrain score: 0.8994\ntest score: 0.8587\n\n\n- 총평: Lasso가 좋긴해요. 그런데 의사결정나무도 나쁘지 않아요.\n\n참고로 Lasso는 엄청 발전된 모델\n의사결정나무는 아주 초기모델임"
  },
  {
    "objectID": "posts/04wk-016.html#a.-결측치-체크",
    "href": "posts/04wk-016.html#a.-결측치-체크",
    "title": "04wk-016: 타이타닉 / 결측치처리+로지스틱 – 추가해설",
    "section": "A. 결측치 체크",
    "text": "A. 결측치 체크\n- 결측치확인\n\ndf_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB"
  },
  {
    "objectID": "posts/04wk-016.html#b.-시각화",
    "href": "posts/04wk-016.html#b.-시각화",
    "title": "04wk-016: 타이타닉 / 결측치처리+로지스틱 – 추가해설",
    "section": "B. 시각화",
    "text": "B. 시각화\n\nmsno.matrix(df_train) \n# msno.bar(df_train) # 큰 의미 X \n# msno.dendrogram(df_train) # 큰 의미 X \n# msno.heatmap(df_train) # 큰 의미 X \n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/04wk-016.html#c.-결측치-처리",
    "href": "posts/04wk-016.html#c.-결측치-처리",
    "title": "04wk-016: 타이타닉 / 결측치처리+로지스틱 – 추가해설",
    "section": "C. 결측치 처리",
    "text": "C. 결측치 처리\n\ndf_train.select_dtypes(include=\"number\")\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\n27.0\n0\n0\n13.0000\n\n\n887\n888\n1\n1\n19.0\n0\n0\n30.0000\n\n\n888\n889\n0\n3\nNaN\n1\n2\n23.4500\n\n\n889\n890\n1\n1\n26.0\n0\n0\n30.0000\n\n\n890\n891\n0\n3\n32.0\n0\n0\n7.7500\n\n\n\n\n891 rows × 7 columns\n\n\n\n\ndef impute_missing(df):\n    df_imputed = df.copy()\n    df_num = df.select_dtypes(include=\"number\")\n    df_cat = df.select_dtypes(exclude=\"number\")\n    df_imputed[df_num.columns] = sklearn.impute.SimpleImputer().fit_transform(df_num) \n    df_imputed[df_cat.columns] = sklearn.impute.SimpleImputer(strategy='most_frequent').fit_transform(df_cat) \n    return df_imputed\n\n\nimpute_missing(df_test)\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892.0\n3.0\nKelly, Mr. James\nmale\n34.50000\n0.0\n0.0\n330911\n7.8292\nB57 B59 B63 B66\nQ\n\n\n1\n893.0\n3.0\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.00000\n1.0\n0.0\n363272\n7.0000\nB57 B59 B63 B66\nS\n\n\n2\n894.0\n2.0\nMyles, Mr. Thomas Francis\nmale\n62.00000\n0.0\n0.0\n240276\n9.6875\nB57 B59 B63 B66\nQ\n\n\n3\n895.0\n3.0\nWirz, Mr. Albert\nmale\n27.00000\n0.0\n0.0\n315154\n8.6625\nB57 B59 B63 B66\nS\n\n\n4\n896.0\n3.0\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.00000\n1.0\n1.0\n3101298\n12.2875\nB57 B59 B63 B66\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n413\n1305.0\n3.0\nSpector, Mr. Woolf\nmale\n30.27259\n0.0\n0.0\nA.5. 3236\n8.0500\nB57 B59 B63 B66\nS\n\n\n414\n1306.0\n1.0\nOliva y Ocana, Dona. Fermina\nfemale\n39.00000\n0.0\n0.0\nPC 17758\n108.9000\nC105\nC\n\n\n415\n1307.0\n3.0\nSaether, Mr. Simon Sivertsen\nmale\n38.50000\n0.0\n0.0\nSOTON/O.Q. 3101262\n7.2500\nB57 B59 B63 B66\nS\n\n\n416\n1308.0\n3.0\nWare, Mr. Frederick\nmale\n30.27259\n0.0\n0.0\n359309\n8.0500\nB57 B59 B63 B66\nS\n\n\n417\n1309.0\n3.0\nPeter, Master. Michael J\nmale\n30.27259\n1.0\n1.0\n2668\n22.3583\nB57 B59 B63 B66\nC\n\n\n\n\n418 rows × 11 columns\n\n\n\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\n결측치 처리가 별로임: 사실 타이타닉 자료의 경우 위와 같이 train/test를 독립된 데이터세트로 보고 결측치를 처리하는 것 보다 모듈19-MinMaxScaler에서 소개한 것 처럼 train에서 fit한 imputer를 test에서 .transform()하는 것이 훨씬 자연스럽다. 다만 이 예제를 설명하는 단계에서는 train/test를 독립된 데이터세트로 보고 전처리를 할 경우 생기는 비상식을 이해하기 어려워 위와 같은 방식으로 진행했음."
  },
  {
    "objectID": "posts/04wk-016.html#a.-자료의-정리",
    "href": "posts/04wk-016.html#a.-자료의-정리",
    "title": "04wk-016: 타이타닉 / 결측치처리+로지스틱 – 추가해설",
    "section": "A. 자료의 정리",
    "text": "A. 자료의 정리\n\n{c:len(set(df_train[c])) for c in df_train.select_dtypes(include=\"object\").columns}\n\n{'Name': 891, 'Sex': 2, 'Ticket': 681, 'Cabin': 148, 'Embarked': 4}\n\n\n\nX = pd.get_dummies(impute_missing(df_train).drop(['Survived','Name','Ticket','Cabin'],axis=1))\ny = impute_missing(df_train)[['Survived']]\n\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\n너무 많은 범주: Name, Ticket, Cabin은 너무 많은 범주를 포함하고 있으므로 제외한다고 설명했다. 그런데 사실\n\n이름에는 혼인여부에 대한 정보가 포함될 수도 있어서 버리기 아깝다.\n범주가 너무 많다면 통째로 변수를 제거하는 것 보다, 공통범주로 묶어서 처리하는 방법도 고려할 수 있다.\n\n는 점에서 다른 분석방법을 고려할 수도 있다.\n\n\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\nPassengerId: 사실 PassengerId불필요한 정보이고 (PassengerId 와 y는 corr이 없음) 있어봤자 오버핏만 유발하므로 제외하는게 좋다. 단지 이 시점에서는 “불필요한 변수를 제거하지 않으면 왜 오버피팅 현상이 발생하는가?” 를 설명할 수 없어서 남겨두었던 것."
  },
  {
    "objectID": "posts/04wk-016.html#b.-predictor-생성",
    "href": "posts/04wk-016.html#b.-predictor-생성",
    "title": "04wk-016: 타이타닉 / 결측치처리+로지스틱 – 추가해설",
    "section": "B. predictor 생성",
    "text": "B. predictor 생성\n\npredictr = sklearn.linear_model.LogisticRegression()"
  },
  {
    "objectID": "posts/04wk-016.html#c.-학습",
    "href": "posts/04wk-016.html#c.-학습",
    "title": "04wk-016: 타이타닉 / 결측치처리+로지스틱 – 추가해설",
    "section": "C. 학습",
    "text": "C. 학습\n\npredictr.fit(X,y)\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/04wk-016.html#d.-예측",
    "href": "posts/04wk-016.html#d.-예측",
    "title": "04wk-016: 타이타닉 / 결측치처리+로지스틱 – 추가해설",
    "section": "D. 예측",
    "text": "D. 예측\n\npredictr.predict(X)\n\narray([0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n       0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n       0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n       0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n       0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n       1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n       0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n       0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1.,\n       1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n       1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n       1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1.,\n       1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n       0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n       1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n       1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n       0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n       0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n       0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n       0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n       0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n       0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n       1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0.,\n       0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n       0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n       1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n       0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n       1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n       1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n       1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n       0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n       0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n       1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1.,\n       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n       1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n       1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n       0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n       0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n       0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0.,\n       0., 1., 0., 1., 1., 0., 0.])"
  },
  {
    "objectID": "posts/04wk-016.html#e.-평가",
    "href": "posts/04wk-016.html#e.-평가",
    "title": "04wk-016: 타이타닉 / 결측치처리+로지스틱 – 추가해설",
    "section": "E. 평가",
    "text": "E. 평가\n\npredictr.score(X,y)\n\n0.8024691358024691"
  },
  {
    "objectID": "posts/07wk-028.html#a.-결측치의-존재",
    "href": "posts/07wk-028.html#a.-결측치의-존재",
    "title": "07wk-028: 선형모형의 적",
    "section": "A. 결측치의 존재",
    "text": "A. 결측치의 존재\n- 문제: 데이터에서 누락된 값이 있는 경우, 선형모델이 돌아가지 않음. (“NaN이 있어서 모형을 적합할 수 없습니다”라는 에러 발생)\n- 해결방법\n\n방법1: 결측치를 제거\n\n결측치가 포함된 열을 제거\n결측치가 포함된 행을 제거\n위의 두 방법을 혼합\n\n방법2: 결측치를 impute\n\ntrain 에서는 fit_transform, test 에서는 transform\ntrain, test에서 모두 fit_transform\n임의의 값 (예를들면 -999)로 일괄 impute\ninterploation (이미지나 시계열 자료)"
  },
  {
    "objectID": "posts/07wk-028.html#b.-다중공선성의-존재",
    "href": "posts/07wk-028.html#b.-다중공선성의-존재",
    "title": "07wk-028: 선형모형의 적",
    "section": "B. 다중공선성의 존재",
    "text": "B. 다중공선성의 존재\n- 문제: 데이터의 설명변수가 역할이 겹칠경우 선형모형의 일반화 성능이 좋지 않음.\n- 해결방법\n\n방법1: 변수제거\n\nX의 corr을 파악하고 (혹은 히트맵을 그리고) 느낌적으로 제거\nPCA등 차원축소기법을 이용한 제거\n\n방법2: 공선성을 가지는 변수를 모아 새로운 변수로 변환\n\n느낌적으로 변환 (예시 Fsize = Sibsp + Parch + 1, 이후 Sibsp, Parch 는 drop)\nPCA를 이용한 변환\n\n방법3: Ridge, Lasso 등 패널티계열을 사용\n\nRigde\nLasso\nElastic net\n\n\n- 방법1-1 (X의 corr을 파악하고 느낌적으로 제거) 의 예시\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nX = df.loc[:,'gpa':'toeic2']\nX\n\n\n\n\n\n\n\n\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\n\n\n\n\n0\n0.051535\n135\n129.566309\n133.078481\n121.678398\n\n\n1\n0.355496\n935\n940.563187\n935.723570\n939.190519\n\n\n2\n2.228435\n485\n493.671390\n493.909118\n475.500970\n\n\n3\n1.179701\n65\n62.272565\n55.957257\n68.521468\n\n\n4\n3.962356\n445\n449.280637\n438.895582\n433.598274\n\n\n...\n...\n...\n...\n...\n...\n\n\n495\n4.288465\n280\n276.680902\n274.502675\n277.868536\n\n\n496\n2.601212\n310\n296.940263\n301.545000\n306.725610\n\n\n497\n0.042323\n225\n206.793217\n228.335345\n222.115146\n\n\n498\n1.041416\n320\n327.461442\n323.019899\n329.589337\n\n\n499\n3.626883\n375\n370.966595\n364.668477\n371.853566\n\n\n\n\n500 rows × 5 columns\n\n\n\ncorr 조사\n\nX.corr()\n\n\n\n\n\n\n\n\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\n\n\n\n\ngpa\n1.000000\n-0.033983\n-0.035722\n-0.037734\n-0.034828\n\n\ntoeic\n-0.033983\n1.000000\n0.999435\n0.999322\n0.999341\n\n\ntoeic0\n-0.035722\n0.999435\n1.000000\n0.998746\n0.998828\n\n\ntoeic1\n-0.037734\n0.999322\n0.998746\n1.000000\n0.998721\n\n\ntoeic2\n-0.034828\n0.999341\n0.998828\n0.998721\n1.000000\n\n\n\n\n\n\n\nheatmap 플랏\n\nsns.heatmap(X.corr(),annot=True)\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/07wk-028.html#c.-관련이-없는-변수의-존재",
    "href": "posts/07wk-028.html#c.-관련이-없는-변수의-존재",
    "title": "07wk-028: 선형모형의 적",
    "section": "C. 관련이 없는 변수의 존재",
    "text": "C. 관련이 없는 변수의 존재\n- 문제: 데이터에서 불필요한 설명변수가 너무 많을 경우 선형모형의 일반화 성능이 좋지 않음.\n\n불필요한 설명변수임의 쉬운 예시: 고객이름, ID, Index 관련 변수\n\n- 해결방법\n\n방법1: 변수제거\n\n(y,X)의 corr을 파악하고 (혹은 히트맵을 그리고) 느낌적으로 제거\nPCA를 이용한 제거\nLasso를 이용한 제거\n\n방법2: 더 많은 데이터를 확보 (궁극기술, 그런데 차원의 저주때문에 힘듬)\n\n- 방법1-1의 예시\n\nnp.random.seed(1)\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf_balance = pd.DataFrame((np.random.randn(500,3)).reshape(500,3)*1,columns = ['balance'+str(i) for i in range(3)])\ndf_train = pd.concat([df,df_balance],axis=1)\ndf_train\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\n\n\n\n\n0\n135\n0.051535\n0\n1.624345\n-0.611756\n-0.528172\n\n\n1\n935\n0.355496\n0\n-1.072969\n0.865408\n-2.301539\n\n\n2\n485\n2.228435\n0\n1.744812\n-0.761207\n0.319039\n\n\n3\n65\n1.179701\n0\n-0.249370\n1.462108\n-2.060141\n\n\n4\n445\n3.962356\n1\n-0.322417\n-0.384054\n1.133769\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n-1.326490\n0.308204\n1.115489\n\n\n496\n310\n2.601212\n1\n1.008196\n-3.016032\n-1.619646\n\n\n497\n225\n0.042323\n0\n2.005141\n-0.187626\n-0.148941\n\n\n498\n320\n1.041416\n0\n1.165335\n0.196645\n-0.632590\n\n\n499\n375\n3.626883\n1\n-0.209847\n1.897161\n-1.381391\n\n\n\n\n500 rows × 6 columns\n\n\n\n\ndf_train.corr()\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\n\n\n\n\ntoeic\n1.000000\n-0.033983\n0.260183\n0.002682\n0.110530\n0.024664\n\n\ngpa\n-0.033983\n1.000000\n0.711022\n-0.025197\n0.005272\n0.020794\n\n\nemployment\n0.260183\n0.711022\n1.000000\n-0.007348\n0.036706\n0.032284\n\n\nbalance0\n0.002682\n-0.025197\n-0.007348\n1.000000\n-0.059167\n0.040035\n\n\nbalance1\n0.110530\n0.005272\n0.036706\n-0.059167\n1.000000\n-0.030215\n\n\nbalance2\n0.024664\n0.020794\n0.032284\n0.040035\n-0.030215\n1.000000\n\n\n\n\n\n\n\n\nsns.heatmap(df_train.corr(),annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n설명변수 balance0,1,2(=X3,X4,X5)는 반응변수 employment(=y)와 관련이 없어 -&gt; X3,X4,X5는 제외하자.\n반응변수 employment(=y)와 관련이 있는 설명변수인 toiec,gpa (=X1,X2)는 남기자.\n공선성체크: 설명변수 toeic, gpa (=X1,X2)의 corr은 -0.034 로 높지 않으니 다중공선성문제를 걱정할 필요가 없음.\n\n- 방법1-3의 예시\n\nnp.random.seed(1)\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf_balance = pd.DataFrame((np.random.randn(500,3)).reshape(500,3)*1,columns = ['balance'+str(i) for i in range(3)])\ndf_train = pd.concat([df,df_balance],axis=1)\ndf_train\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\n\n\n\n\n0\n135\n0.051535\n0\n1.624345\n-0.611756\n-0.528172\n\n\n1\n935\n0.355496\n0\n-1.072969\n0.865408\n-2.301539\n\n\n2\n485\n2.228435\n0\n1.744812\n-0.761207\n0.319039\n\n\n3\n65\n1.179701\n0\n-0.249370\n1.462108\n-2.060141\n\n\n4\n445\n3.962356\n1\n-0.322417\n-0.384054\n1.133769\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n-1.326490\n0.308204\n1.115489\n\n\n496\n310\n2.601212\n1\n1.008196\n-3.016032\n-1.619646\n\n\n497\n225\n0.042323\n0\n2.005141\n-0.187626\n-0.148941\n\n\n498\n320\n1.041416\n0\n1.165335\n0.196645\n-0.632590\n\n\n499\n375\n3.626883\n1\n-0.209847\n1.897161\n-1.381391\n\n\n\n\n500 rows × 6 columns\n\n\n\n\n# step1\nX,y = df_train[['toeic','gpa','balance0','balance1','balance2']], df_train['employment']\n# step2 \npredictr = sklearn.linear_model.LogisticRegressionCV(\n    Cs = [0.1, 1, 10, 100],\n    penalty='l1',\n    solver='liblinear',\n    random_state=42\n)\n# step3 \npredictr.fit(X,y)\n# step4 -- pass\n\nLogisticRegressionCV(Cs=[0.1, 1, 10, 100], penalty='l1', random_state=42,\n                     solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionCVLogisticRegressionCV(Cs=[0.1, 1, 10, 100], penalty='l1', random_state=42,\n                     solver='liblinear')\n\n\n\npredictr.coef_\n\narray([[0.00260249, 1.41401358, 0.        , 0.        , 0.        ]])\n\n\n\ns = pd.Series(predictr.coef_.reshape(-1))\ns.index= X.columns\ns\n\ntoeic       0.002602\ngpa         1.414014\nbalance0    0.000000\nbalance1    0.000000\nbalance2    0.000000\ndtype: float64"
  },
  {
    "objectID": "posts/07wk-028.html#d.-이상치의-존재",
    "href": "posts/07wk-028.html#d.-이상치의-존재",
    "title": "07wk-028: 선형모형의 적",
    "section": "D. 이상치의 존재",
    "text": "D. 이상치의 존재\n- 문제: 이상치가 존재할 경우 전체 모형이 무너질 수 있음\n- 이상치가 있을 경우 해결할 수 있는 방법\n\n방법1: 이상치를 제거하고 분석한다.\n\n느낌적으로 제거함.\n이상치를 감지하는 지표을 사용하여 제거한 이후 분석\n이상치를 자동으로 감지하는 모형을 사용하여 이상치를 제거한 이후 분석\n\n방법2: 로버스트 선형회귀 계열을 이용\n\nsklearn.linear_model.HuberRegressor 등\n\n\n방법3: 이상치를 완화시키는 변환을 사용\n\nsklearn.preprocessing.PowerTransformer 이용\n\n\n- 방법3-1의 예시\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:100,3].to_numpy()\ntemp.sort()\nice_sales = 10 + temp * 0.5 + np.random.randn(100)\nice_sales[0] = 50\ndf_train = pd.DataFrame({'temp':temp,'ice_sales':ice_sales})[:10]\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n50.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n5\n-0.3\n10.205951\n\n\n6\n0.3\n8.486925\n\n\n7\n0.4\n8.817227\n\n\n8\n0.4\n8.273155\n\n\n9\n0.7\n8.863784\n\n\n\n\n\n\n\n\ntransformr = sklearn.preprocessing.PowerTransformer()\nx,y = transformr.fit_transform(df_train).T\n\n\nsns.scatterplot(df_train,x='temp',y='ice_sales',label='before')\nsns.scatterplot(x=x,y=y,label='after')\n\n&lt;Axes: xlabel='temp', ylabel='ice_sales'&gt;\n\n\n\n\n\n—참고—\nPowerTransformer()는 자료가 정규분포가 아닌 경우 강제로 정규화하는 변환이다.\n\nx = np.random.exponential(scale=10, size=1000)\ntransformr = sklearn.preprocessing.PowerTransformer(method='box-cox')  # 'box-cox' 또는 'yeo-johnson' 중 선택 가능, 디폴트는 이오존슨\ny = transformr.fit_transform(x.reshape(-1, 1))\n\n\nfig, ax = plt.subplots(1,2)\nax[0].hist(x,bins=25)\nax[1].hist(y,bins=25)\nfig.set_figwidth(12)"
  },
  {
    "objectID": "posts/07wk-028.html#e.-교호작용의-존재",
    "href": "posts/07wk-028.html#e.-교호작용의-존재",
    "title": "07wk-028: 선형모형의 적",
    "section": "E. 교호작용의 존재",
    "text": "E. 교호작용의 존재\n- 문제: 설명 변수 간의 상호 작용이 있는 경우 이를 고려하지 않으면 모델이 데이터를 잘 설명하지 못할 수 있음.\n- 해결: 고려하면 됩니당.."
  },
  {
    "objectID": "posts/03wk-010.html#a.-데이터",
    "href": "posts/03wk-010.html#a.-데이터",
    "title": "03wk-010: 아이스크림(범주) / 회귀분석 – 추가해설",
    "section": "A. 데이터",
    "text": "A. 데이터\n\nX = df_train[['temp','type']] # 독립변수, 설명변수, 피쳐\ny = df_train[['sales']] # 종속변수, 반응변수, 타겟 \n\n\nX = X.assign(type = [type == 'choco' for type in X.type])"
  },
  {
    "objectID": "posts/03wk-010.html#b.-predictor-생성",
    "href": "posts/03wk-010.html#b.-predictor-생성",
    "title": "03wk-010: 아이스크림(범주) / 회귀분석 – 추가해설",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LinearRegression() \npredictr \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/03wk-010.html#c.-학습-fit-learn",
    "href": "posts/03wk-010.html#c.-학습-fit-learn",
    "title": "03wk-010: 아이스크림(범주) / 회귀분석 – 추가해설",
    "section": "C. 학습 (fit, learn)",
    "text": "C. 학습 (fit, learn)\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/03wk-010.html#d.-예측-predict",
    "href": "posts/03wk-010.html#d.-예측-predict",
    "title": "03wk-010: 아이스크림(범주) / 회귀분석 – 추가해설",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n\nyhat = predictr.predict(X)\n\n\nplt.plot(df_train.temp,df_train.sales,'o',alpha=0.5)\nplt.plot(df_train.temp,yhat, 'x',alpha=0.5)\n\n\n\n\n- 어떻게 맞춘거지?\n\\[\\textup{아이스크림 판매량} = 40 + \\textup{아이스크림종류} \\times (-20) + \\textup{온도} \\times 2.5 + \\textup{오차(운)}\\]\n\npredictr.coef_, predictr.intercept_\n\n(array([[  2.52239574, -20.54021854]]), array([40.16877158]))\n\n\n- 온도가 -2이고, type이 초코라면? 예측값은?\n\nXX = pd.DataFrame({'temp':[-2.0],'type':[1]})\nXX\n\n\n\n\n\n\n\n\ntemp\ntype\n\n\n\n\n0\n-2.0\n1\n\n\n\n\n\n\n\n\npredictr.predict(XX)\n\narray([[14.58376156]])\n\n\n- 온도가 -2이고, type이 바닐라라면? 예측값은?\n\nXX = pd.DataFrame({'temp':[-2.0],'type':[0]})\nXX\n\n\n\n\n\n\n\n\ntemp\ntype\n\n\n\n\n0\n-2.0\n0\n\n\n\n\n\n\n\n\npredictr.predict(XX)\n\narray([[35.1239801]])"
  },
  {
    "objectID": "posts/07wk-027.html",
    "href": "posts/07wk-027.html",
    "title": "07wk-027: 아이스크림(이상치) / 회귀분석 – 추가해설",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.linear_model\nimport matplotlib.pyplot as plt\n\n\n\n3. Data\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:100,3].to_numpy()\ntemp.sort()\nice_sales = 10 + temp * 0.5 + np.random.randn(100)\nice_sales[0] = 200\ndf_train = pd.DataFrame({'temp':temp,'ice_sales':ice_sales})\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n200.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n...\n...\n...\n\n\n95\n12.4\n17.508688\n\n\n96\n13.4\n17.105376\n\n\n97\n14.7\n17.164930\n\n\n98\n15.0\n18.555388\n\n\n99\n15.2\n18.787014\n\n\n\n\n100 rows × 2 columns\n\n\n\n\nplt.plot(df_train.temp,df_train.ice_sales,'o')\n\n\n\n\n\n상상: 온도가 -4.1인 지점에서 “썰매축제”가 열렸다고 가정하자. 그래서 사람이 갑자기 많이 왔음. 그래서 아이스크림이 많이 팔렸음.\n\n\n\n4. 분석\n- 선형회귀로 적합\n\n# step1 \nX,y = df_train[['temp']], df_train['ice_sales']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3 \npredictr.fit(X,y)\n# step4 \ndf_train['ice_sales_hat'] = predictr.predict(X)\n\n- 시각화\n\n# plt.plot(df_train.temp,df_train.ice_sales,'o')\n# plt.plot(df_train.temp,df_train.ice_sales_hat,'--')\nplt.plot(df_train.temp[1:],df_train.ice_sales[1:],'o')\nplt.plot(df_train.temp[1:],df_train.ice_sales_hat[1:],'--')\n\n\n\n\n- 새로운 unseen data를 가정, 데이터는 온도가 12.5~18 에 걸쳐있다고 가정한다.\n\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\nice_sales_hat\n\n\n\n\n0\n-4.1\n200.000000\n20.989373\n\n\n1\n-3.7\n9.234175\n20.731457\n\n\n2\n-3.0\n9.642778\n20.280103\n\n\n3\n-1.3\n9.657894\n19.183959\n\n\n4\n-0.5\n9.987787\n18.668126\n\n\n...\n...\n...\n...\n\n\n95\n12.4\n17.508688\n10.350324\n\n\n96\n13.4\n17.105376\n9.705533\n\n\n97\n14.7\n17.164930\n8.867305\n\n\n98\n15.0\n18.555388\n8.673867\n\n\n99\n15.2\n18.787014\n8.544909\n\n\n\n\n100 rows × 3 columns\n\n\n\n\nXX = df_test = pd.DataFrame({'temp':np.linspace(12.5,18,50)})\ndf_test['ice_sales_hat'] = predictr.predict(XX)\nplt.plot(df_train.temp[1:],df_train.ice_sales[1:],'o')\nplt.plot(df_train.temp[1:],df_train.ice_sales_hat[1:],'--')\nplt.plot(df_test.temp,df_test.ice_sales_hat,'--')\n\n\n\n\n\n온도가 올라갈수록 아이스크림 판매량은 줄어든다는 해석 (더 온도가 올라간다면 판매량이 음수가 나올 수도 있겠음 )\n저 정도의 아웃라이어는 모형에서 제외하는게 타당하지 않나? (하지만 저러한 아웃라이어가 데이터의 가치가 있을 수도 있음. 그런데 또 데이터의 가치가 있는지 얺는지는 어떻게 판단하지?)\n\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\ntrain,test의 score로 오버피팅을 정의하는 교재를 제가 싫어하는 이유\n\n위 그림의 예시는 train score가 test score 보다 좋을 수 있음. 그런데 이건 딱히 오버핏이라 보기 어려움.\n\n\n\n\n\n5. Discussion\n- 딱히 정답이 없음.."
  },
  {
    "objectID": "posts/02wk-004.html#part-1-get-started",
    "href": "posts/02wk-004.html#part-1-get-started",
    "title": "02wk-004: 타이타닉 / Alexis Cook의 코드",
    "section": "Part 1: Get started",
    "text": "Part 1: Get started\nIn this section, you’ll learn more about the competition and make your first submission.\n\nJoin the competition!\nThe first thing to do is to join the competition! Open a new window with the competition page, and click on the “Join Competition” button, if you haven’t already. (If you see a “Submit Predictions” button instead of a “Join Competition” button, you have already joined the competition, and don’t need to do so again.)\n\nThis takes you to the rules acceptance page. You must accept the competition rules in order to participate. These rules govern how many submissions you can make per day, the maximum team size, and other competition-specific details. Then, click on “I Understand and Accept” to indicate that you will abide by the competition rules.\n\n\nThe challenge\nThe competition is simple: we want you to use the Titanic passenger data (name, age, price of ticket, etc) to try to predict who will survive and who will die.\n\n\nThe data\nTo take a look at the competition data, click on the Data tab at the top of the competition page. Then, scroll down to find the list of files.\nThere are three files in the data: (1) train.csv, (2) test.csv, and (3) gender_submission.csv.\n\n(1) train.csv\ntrain.csv contains the details of a subset of the passengers on board (891 passengers, to be exact – where each passenger gets a different row in the table). To investigate this data, click on the name of the file on the left of the screen. Once you’ve done this, you can view all of the data in the window.\n\nThe values in the second column (“Survived”) can be used to determine whether each passenger survived or not: - if it’s a “1”, the passenger survived. - if it’s a “0”, the passenger died.\nFor instance, the first passenger listed in train.csv is Mr. Owen Harris Braund. He was 22 years old when he died on the Titanic.\n\n\n(2) test.csv\nUsing the patterns you find in train.csv, you have to predict whether the other 418 passengers on board (in test.csv) survived.\nClick on test.csv (on the left of the screen) to examine its contents. Note that test.csv does not have a “Survived” column - this information is hidden from you, and how well you do at predicting these hidden values will determine how highly you score in the competition!\n\n\n(3) gender_submission.csv\nThe gender_submission.csv file is provided as an example that shows how you should structure your predictions. It predicts that all female passengers survived, and all male passengers died. Your hypotheses regarding survival will probably be different, which will lead to a different submission file. But, just like this file, your submission should have: - a “PassengerId” column containing the IDs of each passenger from test.csv. - a “Survived” column (that you will create!) with a “1” for the rows where you think the passenger survived, and a “0” where you predict that the passenger died."
  },
  {
    "objectID": "posts/02wk-004.html#part-2-your-coding-environment",
    "href": "posts/02wk-004.html#part-2-your-coding-environment",
    "title": "02wk-004: 타이타닉 / Alexis Cook의 코드",
    "section": "Part 2: Your coding environment",
    "text": "Part 2: Your coding environment\nIn this section, you’ll train your own machine learning model to improve your predictions. If you’ve never written code before or don’t have any experience with machine learning, don’t worry! We don’t assume any prior experience in this tutorial.\n\nThe Notebook\nThe first thing to do is to create a Kaggle Notebook where you’ll store all of your code. You can use Kaggle Notebooks to getting up and running with writing code quickly, and without having to install anything on your computer. (If you are interested in deep learning, we also offer free GPU access!)\nBegin by clicking on the Code tab on the competition page. Then, click on “New Notebook”.\n\nYour notebook will take a few seconds to load. In the top left corner, you can see the name of your notebook – something like “kernel2daed3cd79”.\n\nYou can edit the name by clicking on it. Change it to something more descriptive, like “Getting Started with Titanic”.\n\n\n\nYour first lines of code\nWhen you start a new notebook, it has two gray boxes for storing code. We refer to these gray boxes as “code cells”.\n\nThe first code cell already has some code in it. To run this code, put your cursor in the code cell. (If your cursor is in the right place, you’ll notice a blue vertical line to the left of the gray box.) Then, either hit the play button (which appears to the left of the blue line), or hit [Shift] + [Enter] on your keyboard.\nIf the code runs successfully, three lines of output are returned. Below, you can see the same code that you just ran, along with the output that you should see in your notebook.\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nThis shows us where the competition data is stored, so that we can load the files into the notebook. We’ll do that next.\n\n\nLoad the data\nThe second code cell in your notebook now appears below the three lines of output with the file locations.\n\nType the two lines of code below into your second code cell. Then, once you’re done, either click on the blue play button, or hit [Shift] + [Enter].\n\n#train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\nYour code should return the output above, which corresponds to the first five rows of the table in train.csv. It’s very important that you see this output in your notebook before proceeding with the tutorial! &gt; If your code does not produce this output, double-check that your code is identical to the two lines above. And, make sure your cursor is in the code cell before hitting [Shift] + [Enter].\nThe code that you’ve just written is in the Python programming language. It uses a Python “module” called pandas (abbreviated as pd) to load the table from the train.csv file into the notebook. To do this, we needed to plug in the location of the file (which we saw was /kaggle/input/titanic/train.csv).\n&gt; If you’re not already familiar with Python (and pandas), the code shouldn’t make sense to you – but don’t worry! The point of this tutorial is to (quickly!) make your first submission to the competition. At the end of the tutorial, we suggest resources to continue your learning.\nAt this point, you should have at least three code cells in your notebook.\n\nCopy the code below into the third code cell of your notebook to load the contents of the test.csv file. Don’t forget to click on the play button (or hit [Shift] + [Enter])!\n\n#test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n\n\n\n\n\n\n\nAs before, make sure that you see the output above in your notebook before continuing.\nOnce all of the code runs successfully, all of the data (in train.csv and test.csv) is loaded in the notebook. (The code above shows only the first 5 rows of each table, but all of the data is there – all 891 rows of train.csv and all 418 rows of test.csv!)"
  },
  {
    "objectID": "posts/02wk-004.html#part-3-your-first-submission",
    "href": "posts/02wk-004.html#part-3-your-first-submission",
    "title": "02wk-004: 타이타닉 / Alexis Cook의 코드",
    "section": "Part 3: Your first submission",
    "text": "Part 3: Your first submission\nRemember our goal: we want to find patterns in train.csv that help us predict whether the passengers in test.csv survived.\nIt might initially feel overwhelming to look for patterns, when there’s so much data to sort through. So, we’ll start simple.\n\nExplore a pattern\nRemember that the sample submission file in gender_submission.csv assumes that all female passengers survived (and all male passengers died).\nIs this a reasonable first guess? We’ll check if this pattern holds true in the data (in train.csv).\nCopy the code below into a new code cell. Then, run the cell.\n\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)/len(women)\n\nprint(\"% of women who survived:\", rate_women)\n\n% of women who survived: 0.7420382165605095\n\n\n\n여성의 생존률을 구하는 코드입니다, 이전에 accuracy를 구하던 테크닉을 활용하면 아래의 코드도 가능합니다\n\ntrain_data[train_data.Sex == 'female'].Survived.mean()\n\n0.7420382165605095\n\n\n\nBefore moving on, make sure that your code returns the output above. The code above calculates the percentage of female passengers (in train.csv) who survived.\nThen, run the code below in another code cell:\n\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)/len(men)\n\nprint(\"% of men who survived:\", rate_men)\n\n% of men who survived: 0.18890814558058924\n\n\nThe code above calculates the percentage of male passengers (in train.csv) who survived.\nFrom this you can see that almost 75% of the women on board survived, whereas only 19% of the men lived to tell about it. Since gender seems to be such a strong indicator of survival, the submission file in gender_submission.csv is not a bad first guess!\nBut at the end of the day, this gender-based submission bases its predictions on only a single column. As you can imagine, by considering multiple columns, we can discover more complex patterns that can potentially yield better-informed predictions. Since it is quite difficult to consider several columns at once (or, it would take a long time to consider all possible patterns in many different columns simultaneously), we’ll use machine learning to automate this for us."
  },
  {
    "objectID": "posts/02wk-004.html#your-first-machine-learning-model",
    "href": "posts/02wk-004.html#your-first-machine-learning-model",
    "title": "02wk-004: 타이타닉 / Alexis Cook의 코드",
    "section": "Your first machine learning model",
    "text": "Your first machine learning model\nWe’ll build what’s known as a random forest model. This model is constructed of several “trees” (there are three trees in the picture below, but we’ll construct 100!) that will individually consider each passenger’s data and vote on whether the individual survived. Then, the random forest model makes a democratic decision: the outcome with the most votes wins!\n\nThe code cell below looks for patterns in four different columns (“Pclass”, “Sex”, “SibSp”, and “Parch”) of the data. It constructs the trees in the random forest model based on patterns in the train.csv file, before generating predictions for the passengers in test.csv. The code also saves these new predictions in a CSV file submission.csv.\nCopy this code into your notebook, and run it in a new code cell.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission_AlexisCook.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!\n\n\nMake sure that your notebook outputs the same message above (Your submission was successfully saved!) before moving on. &gt; Again, don’t worry if this code doesn’t make sense to you! For now, we’ll focus on how to generate and submit predictions.\nOnce you’re ready, click on the “Save Version” button in the top right corner of your notebook. This will generate a pop-up window.\n- Ensure that the “Save and Run All” option is selected, and then click on the “Save” button. - This generates a window in the bottom left corner of the notebook. After it has finished running, click on the number to the right of the “Save Version” button. This pulls up a list of versions on the right of the screen. Click on the ellipsis (…) to the right of the most recent version, and select Open in Viewer.\n- Click on the Data tab on the top of the screen. Then, click on the “Submit” button to submit your results.\n\nCongratulations for making your first submission to a Kaggle competition! Within ten minutes, you should receive a message providing your spot on the leaderboard. Great work!"
  },
  {
    "objectID": "posts/02wk-004.html#part-4-learn-more",
    "href": "posts/02wk-004.html#part-4-learn-more",
    "title": "02wk-004: 타이타닉 / Alexis Cook의 코드",
    "section": "Part 4: Learn more!",
    "text": "Part 4: Learn more!\nIf you’re interested in learning more, we strongly suggest our (3-hour) Intro to Machine Learning course, which will help you fully understand all of the code that we’ve presented here. You’ll also know enough to generate even better predictions!"
  },
  {
    "objectID": "posts/02wk-004.html#a.-alexis-cook의-분석은-train에서-얼마나-잘-맞출까",
    "href": "posts/02wk-004.html#a.-alexis-cook의-분석은-train에서-얼마나-잘-맞출까",
    "title": "02wk-004: 타이타닉 / Alexis Cook의 코드",
    "section": "A. Alexis Cook의 분석은 train에서 얼마나 잘 맞출까?",
    "text": "A. Alexis Cook의 분석은 train에서 얼마나 잘 맞출까?\n- 원래코드\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission_AlexisCook.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!\n\n\n\nlen(predictions), len(X_test)\n\n(418, 418)\n\n\n- 이렇게 수정하면 될 듯\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)\n\n\nlen(predictions),len(y)\n\n(891, 891)\n\n\n\n(predictions == y).mean()\n\n0.8159371492704826"
  },
  {
    "objectID": "posts/02wk-004.html#b.-alexis-cook의-코드를-수정해보자",
    "href": "posts/02wk-004.html#b.-alexis-cook의-코드를-수정해보자",
    "title": "02wk-004: 타이타닉 / Alexis Cook의 코드",
    "section": "B. Alexis Cook의 코드를 수정해보자!",
    "text": "B. Alexis Cook의 코드를 수정해보자!\n- 코드를 수정해보자.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=5000, max_depth=1000, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)\n\n\n(predictions == y).mean()\n\n0.8170594837261503\n\n\n\n내가 만든게 더 좋은데??\n\n- 이것도 제출결과로 만들어보자.\n\npredictions = model.predict(X_test)\n\n- 아래와 같이 제출하면 에러가 발생\n\n# pd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\")\\\n# .assign(Survived=predictions)\\\n# .to_csv(\"AlexisCook수정_submission.csv\")\n\n- 아래와 같이 제출파일을 저장해야 한다.\n\n# pd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\")\\\n# .assign(Survived=predictions)\\\n# .to_csv(\"AlexisCook수정2_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/07wk-029.html",
    "href": "posts/07wk-029.html",
    "title": "07wk-029: 체중감량(교호작용) / 회귀분석 – 추가해설",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport sklearn.linear_model \nimport sklearn.tree\nimport sklearn.model_selection\n\n\n\n3. Data\n\n# n = 10000\n# Supplement = np.random.choice([True, False], n)\n# Exercise = np.random.choice([False, True], n)\n# Weight_Loss = np.where(\n#     (~Supplement & (~Exercise)),\n#     np.random.normal(loc=0, scale=1, size=n),  \n#     np.where(\n#         (Supplement & (Exercise)),\n#         np.random.normal(loc=15.00, scale=1, size=n),\n#         np.where(\n#             (~Supplement & (Exercise)),\n#             np.random.normal(loc=5.00, scale=1, size=n),\n#             np.random.normal(loc=0.5, scale=1, size=n)\n#         )\n#     )\n# )\n# df = pd.DataFrame({\n#     'Supplement': Supplement,\n#     'Exercise': Exercise,\n#     'Weight_Loss': Weight_Loss\n# })\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/weightloss.csv')\n\n\ndf_train\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\n\n\n\n\n0\nFalse\nFalse\n-0.877103\n\n\n1\nTrue\nFalse\n1.604542\n\n\n2\nTrue\nTrue\n13.824148\n\n\n3\nTrue\nTrue\n13.004505\n\n\n4\nTrue\nTrue\n13.701128\n\n\n...\n...\n...\n...\n\n\n9995\nTrue\nFalse\n1.558841\n\n\n9996\nFalse\nFalse\n-0.217816\n\n\n9997\nFalse\nTrue\n4.072701\n\n\n9998\nTrue\nFalse\n-0.253796\n\n\n9999\nFalse\nFalse\n-1.399092\n\n\n\n\n10000 rows × 3 columns\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n- 운동과 체중감량보조제를 병행하면 시너지가 나는 것 같음\n\n\n4. 분석\n- 분석1: 모형을 아래와 같이 본다. – 언더피팅\n\n\\({\\bf X}\\): Supplement, Exercise\n\\({\\bf y}\\): Weight_Loss\n\n\n# step1\nX = df_train[['Supplement','Exercise']]\ny = df_train['Weight_Loss']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train['Weight_Loss_hat'] = predictr.predict(X)\n#---#\nprint(f'train score = {predictr.score(X,y):.4f}')\n\ntrain score = 0.8208\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss_hat')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n-2.373106\n7.374557\n\n\nTrue\n2.845934\n12.593598\n\n\n\n\n\n\n\n\n운동을 하면 10키로 감량효과가 있다고 추정하고 있음.\n보충제를 먹으면 5키로 감량효과가 있다고 추정하고 있음.\n대충 (10,5)의 숫자를 바꿔가면서 적합해봤는데 이게 최선이라는 의미임\n\n- 분석2: 모형을 아래와 같이 본다. – 딱 맞아요\n\n\\({\\bf X}\\): Supplement, Exercise, Supplement \\(\\times\\) Exercise\n\\({\\bf y}\\): Weight_Loss\n\n\nNote: 기본적인 운동의 효과 및 보조제의 효과는 각각 Supplement, Exercise 로 적합하고 운동과 보조제의 시너지는 Supplement\\(\\times\\)Exercise 로 적합한다.\n\n\n# step1 \nX = df_train.eval('Interaction = Supplement * Exercise')[['Supplement','Exercise','Interaction']]\ny = df_train['Weight_Loss']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3 \npredictr.fit(X,y)\n# step4 -- pass \ndf_train['Weight_Loss_hat'] = predictr.predict(X)\n#---#\nprint(f'train score = {predictr.score(X,y):.4f}')\n\ntrain score = 0.9728\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss_hat')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\n운동의 효과는 5정도 감량효과가 있다고 추정함.\n보충제를 먹으면 0.5키로 감량효과가 있다고 추정함.\n다만 운동을 하면서 보충제를 함께 먹을 경우 발생하는 추가적인 시너지효과가 9.5정도라고 추정하는 것임.\n\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\n만약에 운동을 안하고, 약만먹을 경우 부작용이 생긴다면? (이것도 교호작용의 일종)\n\n이러한 경우 위의 모형으로 단순적합하기 어렵다. (위의 모형은 “운동O/약O”인 case에서 발생하는 효과만 고려하도록 설계되어있음)\n따라서 이럴 경우 차라리 (운동,약)을 결합하여 새로운 범주형 변수를 만들고 그 변수에서 원핫인코딩을 하는게 좋다. (마지막 더미변수는 제외하는고 좋지만, 파이썬에서는 제외하지 않아도 큰일나는건 아님)\n사실 (운동,약)을 결합하여 모든 새로운 범주를 만들고 이중 필요없는 범주를 또 다시 제거해야하는 과정도 분석에 포함되어야 한다. (\\(p\\)-value를 보면서 뺼수도 있고 다른 방법을 쓸 수도 있고..)\n그런데 범주형 변수가 3개라면? –&gt; 솔직히 이것저것 생각하기 귀찮으니까 이럴떄는 “트리모형”계열을 사용하는게 속편하다. (아니면 교호작용이 없길 기도하거나)"
  },
  {
    "objectID": "posts/09wk-mid.html",
    "href": "posts/09wk-mid.html",
    "title": "09wk-mid",
    "section": "",
    "text": "True/False를 판단하는 문제는 답만 써도 무방함.\n“자료분석” 문제는 “kaggle style score = 50%”, “분석의 논리 = 50%” 의 배점으로 채점한다.\n“자료분석” 유형의 경우 분석의 논리가 매우 우수하거나, 창의적인 접근법으로 분석을 시도할 경우 가산점을 부여한다. (아이디어 단계에서도 가산점 부여가능) 가산점은 문항점수의 최대 100%까지 부여한다.\n“연구” 문항의 경우 세부문항을 정확하게 수행한 경우에만 100%의 점수를 부여하며 이를 어길시 부분점수를 부여하지 않는다. 연구결과의 시각화나 해석이 우수한 답안은 문항점수의 20%까지 가산점을 부여한다.\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.preprocessing \nimport sklearn.model_selection\n\n\n1. True/False (50점)\n\nhttps://www.kaggle.com/t/7911b05108f642e094ce13a77a5a4723\n\n\n링크를 shift + 클릭하면 새 창으로 열려서 문제 풀기 수월합니다.\n\n(1)-(3): 아래는 kaggle에서 titanic 데이터를 불러오는 코드이다.\ndf_train=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndf_test=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\n# !kaggle competitions download -c titanic\n# !unzip titanic.zip -d ./titanic\n# df_train = pd.read_csv('titanic/train.csv')\n# df_test = pd.read_csv('titanic/test.csv')\n# !rm titanic.zip\n# !rm -rf titanic/\n\n(1) 학습을 할 때는 df_train만을 이용하며 df_test는 전혀 사용하지 않는다.\n(2) df_train과 df_test의 열의 숫자는 동일하며 행의 숫자만 차이가 난다.\n(3) 여성생존자만 모두 생존한다고 가정하면 df_train에서의 accuracy는 78% 이상이다.\n(4)-(7) 아래는 모듈4에서 소개한 Alexis Cook의 분석코드이다.\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission_AlexisCook.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n(4) Alexis Cook의 코드에서는 사용된 설명변수는 [“Pclass”, “Sex”, “SibSp”, “Parch”] 이다.\n(5) Alexis Cook의 코드에서는 로지스틱모형을 이용하여 반응변수를 예측하였다.\n(6) model.fit(X,y)는 train data를 학습하기 위한 코드이다.\n(7) test data에서의 성능을 확인하기 위해서는 아래의 코드를 사용하면 된다.\noutput.score(X_test,y)\n(8)-(10) 아래는 모듈8에 소개된 코드중 일부이다.\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\n# load data\n...\n...\n\n# analysis\npredictr = TabularPredictor(\"Survived\")\npredictr.fit(df_train,presets='best_quality')\n(df_train.Survived == predictr.predict(df_train)).mean()\n(8) 위의 코드에서 반응변수는 “Survived”로 설정하였다.\n(9) predictr.fit(df_train,presets='best_quality')은 train data를 학습하는 단계이다.\n(10) (df_train.Survived == predictr.predict(df_train)).mean()은 모형의 성능을 train data에서 검증하는 단계이다.\n(11)-(14) 아래는 모듈11에 제시된 데이터프레임으로 선형모형의 계수값을 표현한 것이다.\n\n(11) 여성은 남성보다 평균적으로 보험료를 65.657180 만큼 더 낸다고 해석할 수 있다.\n(12) 나이가 1살 증가하면 보험료가 평균 256.856353 만큼 증가한다고 해석할 수 있다.\n(13) 보험료를 상대적으로 많이 내는 지역과 그렇지 않은 지역이 존재한다.\n(14) 흡연유무는 성별보다 보험료에 미치는 영향이 크다.\n(15)-(19) 아래는 모듈13에 대한 설명이다.\n(15) 4.분석-실패에서 아래와 같은 에러메시지가 나오는 이유는 train data에 결측치가 포함되어 있기 때문이다.\n\n(16) 결측치가 하나라도 포함된 모든 열을 제거한다면 에러메시지를 피할수는 있겠으나 오버피팅이 발생할 가능성이 있다.\n(17) 따라서 결측치를 하나라도 포함된 모든 열을 제거한뒤 Lasso를 쓰면 오버피팅을 피할 수 있다.\n(18) 결측치가 존재할 경우, 결측치가 있는 열을 제외하고 분석하는 것이 유일한 해결방법은 아니다. 예를들면 결측치를 적당한 값으로 impute한 뒤 분석할 수도 있다.\n(19) train data와 test data에서 결측치가 포함된 행을 모두 제거한 뒤, train data 를 이용하여 적합을 시키고 test data 에서 예측한뒤 제출하는 것도 가능한 분석방법 중 하나이다. 즉 아래와 같은 의사코드는 가능한 분석방법 중 하나이다.\ndf_train = 결측치가 포함된 행이 제거된 train data\ndf_test = 결측치가 포함된 행이 제거된 test data\nX,y = df_train 을 적당히 잘 정리한 자료의 튜플형태 \nXX = df_test 를 적당히 잘 정리한 자료\npredictr = sklearn.linear_model.LogisticRegression()\npredictr.fit(X,y)\nsubmission = predictr.predict(XX)의 값을 적당히 제출용으로 정리하여 만든 df\n(20)-(24) 아래는 모듈19, 모듈20 에 대한 설명이다.\n(20) sklearn.preprocessing.MinMaxScaler()를 이용하여 스케일러를 생성하고 train data에 .fit_transform() method를 적용하면 항상 \\([0,1]\\) 사이의 값이 나온다.\n(21) sklearn.preprocessing.MinMaxScaler()를 이용하여 스케일러는 .inverse_transform() method를 가지고 있으며 이는 변환된 값을 원래로 되돌리는 역할을 한다.\n(22) sklearn.preprocessing.StandardScaler()는 데이터의 평균을 0, 표준편차를 1로 만드는 방식으로 조정한다.\n(23) 만약에 train data와 test data로 나누어진 상황에서 스케일러를 이용하여 데이터를 변환하려면 아래와 같은 순서를 따르는 것이 하나의 방법이다.\n\n적당한 scaler 생성이후 train data 를 입력으로 넣고 .fit_transform() method 사용\n1과 동일한 scaler에 test data 를 입력으로 넣고 .transform() method 사용\n\n(24) (23)과 동일한 상황에서 아래와 같은 변환도 해법이 될 수 있다.\n\n적당한 scaler 생성이후 train data, test data를 합쳐서 입력으로 넣고 .fit_transform() method 사용\n\n(25)-(26) 아래는 모듈21 에 대한 설명이다.\n(25) 밸런스게임을 통하여 많은 변수를 모을수록 train data 에 대한 score를 올릴 수 있다.\n(26) 이 모듈의 예시는 불필요한 변수의 추가가 오히려 학습에 방해되어 test score를 낮추는 현상이 생길 수 있음을 시사한다.\n(27)-(33) 아래는 모듈22 에 대한 설명이다.\n(27) 이 모듈의 데이터에서 toeic과 toeic0의 상관계수값은 매우 높게 나올 것이다.\n(28) 이 모듈의 데이터에서 모든변수를 넣고 적합한다면,toeic, toeic0, ... ,toeic499에 적합된 계수값의 합은 0.01 정도의 값을 가진다.\n(29) 이 모듈의 데이터에서 gpa와 toeic만을 남기고 나머지 변수를 제거한뒤 돌리면 test score가 향상된다.\n(30) 즉 이 모듈의 데이터는 변수를 제거할수록 test score가 향상된다. 예를들면 gpa를 제거하여도 test score가 향상된다.\n(31) 이 예제에서 train score는 모든 데이터를 사용하여 적합하였을 경우 가장 높게 나타난다.\n(32) toeic, toeic1부터 toeic499까지의 변수들은 모두 employment_score와 높은 상관계수를 보인다.\n(33) 모듈22에 소개된 현상을 다중공선성이라고 말하며 이때는 계수값의 해석이 용이하지 않다.\n(34)-(38) 아래는 다중공선성과 Rigde, Lasso에 대한 종합적인 설명이다.\n(34) 다중공선성이 발생할 경우 올바른 변수선택으로 언제나 문제를 해결할 수 있다.\n(35) 다중공선성이 발생할 경우 \\({\\bf X}\\)의 correlation matrix는 단위행렬에 가깝게 나온다.\n(36) 다중공선성이 발생한다면 Ridge, Lasso와 같은 방법을 이용하여 해결할 수 있다.\n(37) 관측치의 수가 매우커진다면 (즉 데이터프레임에서 행의 숫자가 매우 커진다면) 다중공선성 문제가 심화될 가능성이 있다.\n(38) 다중공선성을 아래와 같은 시각화 코드로 진단할 수 있는 경우가 있다.\nsns.heatmap(df_train.corr(),annot=True)\n(39)-(42) 아래는 모듈23 에 대한 설명이다.\n(39) toeic, toiec0, …, toeic499 에 대응하는 계수값의 합은 \\(\\alpha\\) 와 관련없이 항상 일정하다.\n(40) 이 모듈에서 큰 \\(\\alpha\\)는 토익계수들의 값을 점점 비슷하게 만들어 주는 효과가 있다.\n(41) toeic, toiec0, …, toeic499 에 대응하는 계수값들의 분산은 \\(\\alpha\\)가 커짐에 따라 점점 작아진다.\n(42) \\(\\alpha\\)를 매우 잘 선택한다면 모듈23-4.True 에서 소개된 바와 같이 적절한 변수선택을 할 수 있다.\n(43)-(44) 아래는 모듈25 에 대한 설명이다.\n(43) \\(\\alpha\\)가 커질수록 0이 아닌 계수값의 수는 점점 증가한다.\n(44) Lasso를 사용하면, 토익과 유사한 변수의 수가 증가할수록 test_score가 좋아진다.\n(45)-(47) 아래는 모듈27 에 대한 설명이다.\n(45) 강의교재에 소개된 아래의 그림은 train score 보다 test score 가 낮은 상황이다.\n\n(46) 하지만 이러한 적합결과가 오버피팅이라고 보기는 어렵다.\n(47) 아웃라이어를 제거한다면 선형모형의 df_train, df_test에 대한 score가 모두 상승할 것이다.\n(48)-(50) 아래는 모듈29에 대한 설명이다.\n(48) 모듈29, 4.분석에 소개된 아래와 같은 모형은\n\n\\({\\bf X}\\): Supplement, Exercise, Supplement \\(\\times\\) Exercise\n\\({\\bf y}\\): Weight_Loss\n\n운동을 했으며, 보조제를 사용한 경우에 발생하는 추가적인 효과를 고려할 수 있다.\n(49) 이처럼 두 종류 이상의 범주형 범수가 존재하는 경우 범주형 범수간의 교호작용을 항상 고려해야 모형의 성능을 올릴 수 있다.\n(50) 실제로 교호작용이 있지만 이를 고려하지 않아서 모형의 성능이 떨어지는 경우는 언터피팅의 일종으로 볼 수 있다.\n\n\n2. 자료분석 (30점)\n(1) 아래의 자료를 해석하고 세부지침에 맞추어 분석하라.\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/icesales_train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/icesales_test.csv')\n\n세부지침\n\n반응변수 \\({\\bf y}\\)를 sales로 설정하고 나머지는 설명변수로 설정하라.\ndf_test에 sales에 대한 예측값을 포함하는 열을 추가하라.\n\n(2) 아래의 자료를 해석하고 세부지침에 맞추어 분석하라.\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/height_train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/height_test.csv')\n\n세부지침\n\n반응변수 \\({\\bf y}\\)를 height로 설정하고 나머지는 설명변수로 설정하라.\ndf_test에 height에 대한 예측값을 포함하는 열을 추가하라.\n\n(3) 아래의 자료를 해석하고 세부지침에 맞추어 분석하라.\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/synthetic_train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/master/posts/mid/synthetic_test.csv')\n\n세부지침\n\n반응변수 \\({\\bf y}\\)를 y로 설정하고 나머지 X1,X2,X3,X4는 설명변수로 설정하라.\ndf_test에 y에 대한 예측값을 포함하는 열을 추가하라.\n\n\n\n3. 연구 (20점)\n아래의 코드를 이용하여 자료를 불러오라.\n\nnp.random.seed(42)\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\n\n아래의 절차에 따라 자료를 연구하라. 결과를 해석하라.\n\n종속변수 \\({\\bf y}\\)를 employment_score로 설정하고 나머지는 설명변수로 설정하라.\nsklearn.linear_model.Ridge를 이용하여 \\(\\alpha = [10^1, 10^{1.5}, 10^2, 10^{2.5}, \\dots, 10^{9}]\\)에 대해 predictor를 생성하라.\n\\(\\alpha\\)의 변화에 따른 train score 와 test score 의 변화를 시각화하라.\n\\(\\alpha\\)에 변화에 따른 predictor가 학습한 coefficient값 (predictor.coef_) 들의 분산변화와 L2-penalty를 시각화하라.\n\n\n구현시 20점, 결과해석이 추가되면 가산점 4점"
  },
  {
    "objectID": "posts/02wk-005.html#a.-데이터",
    "href": "posts/02wk-005.html#a.-데이터",
    "title": "02wk-005: 타이타닉 / Autogluon",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\n!kaggle competitions download -c titanic\n!unzip titanic.zip -d ./titanic\ndf_train = TabularDataset('titanic/train.csv')\ndf_test = TabularDataset('titanic/test.csv')\n!rm titanic.zip\n!rm -rf titanic/\n\nDownloading titanic.zip to /home/cgb2/Dropbox/07_lectures/2023-09-MP2023/posts\n  0%|                                               | 0.00/34.1k [00:00&lt;?, ?B/s]\n100%|██████████████████████████████████████| 34.1k/34.1k [00:00&lt;00:00, 5.82MB/s]\nArchive:  titanic.zip\n  inflating: ./titanic/gender_submission.csv  \n  inflating: ./titanic/test.csv      \n  inflating: ./titanic/train.csv"
  },
  {
    "objectID": "posts/02wk-005.html#b.-predictor-생성",
    "href": "posts/02wk-005.html#b.-predictor-생성",
    "title": "02wk-005: 타이타닉 / Autogluon",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231024_084026\""
  },
  {
    "objectID": "posts/02wk-005.html#c.-적합fit",
    "href": "posts/02wk-005.html#c.-적합fit",
    "title": "02wk-005: 타이타닉 / Autogluon",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(df_train) \n# 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231024_084026\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   265.30 GB / 490.57 GB (54.1%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/learner/default_learner.py:215: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    124529.88 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/features/generators/fillna.py:58: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  X.fillna(self._fillna_feature_map, inplace=True, downcast=False)\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2417.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.19s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBMXT ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8156   = Validation score   (accuracy)\n    0.48s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8212   = Validation score   (accuracy)\n    0.16s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.54s    = Training   runtime\n    0.2s     = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8156   = Validation score   (accuracy)\n    0.61s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: CatBoost ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8268   = Validation score   (accuracy)\n    0.56s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8156   = Validation score   (accuracy)\n    0.82s    = Training   runtime\n    0.22s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.7s     = Training   runtime\n    0.23s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if is_categorical_dtype(col):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\nNo improvement since epoch 9: early stopping\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n    0.8324   = Validation score   (accuracy)\n    2.15s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(data):\n    0.8101   = Validation score   (accuracy)\n    0.23s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: NeuralNetTorch ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8212   = Validation score   (accuracy)\n    3.4s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/common/utils/pandas_utils.py:50: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2059.259259259259' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  memory_usage[column] = (\n    0.8324   = Validation score   (accuracy)\n    0.42s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8324   = Validation score   (accuracy)\n    0.48s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 12.03s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231024_084026\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7fd1a92d4af0&gt;\n\n\n- 리더보드확인 (모의고사 채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0         LightGBMLarge   0.832402       0.003578  0.422645                0.003578           0.422645            1       True         13\n1       NeuralNetFastAI   0.832402       0.011829  2.153854                0.011829           2.153854            1       True         10\n2   WeightedEnsemble_L2   0.832402       0.012396  2.629154                0.000567           0.475301            2       True         14\n3              CatBoost   0.826816       0.007282  0.558903                0.007282           0.558903            1       True          7\n4              LightGBM   0.821229       0.003267  0.163059                0.003267           0.163059            1       True          4\n5        NeuralNetTorch   0.821229       0.014016  3.402719                0.014016           3.402719            1       True         12\n6            LightGBMXT   0.815642       0.003467  0.479265                0.003467           0.479265            1       True          3\n7      RandomForestEntr   0.815642       0.175467  0.606128                0.175467           0.606128            1       True          6\n8      RandomForestGini   0.815642       0.204887  0.539022                0.204887           0.539022            1       True          5\n9        ExtraTreesGini   0.815642       0.224873  0.821764                0.224873           0.821764            1       True          8\n10              XGBoost   0.810056       0.005781  0.229575                0.005781           0.229575            1       True         11\n11       ExtraTreesEntr   0.810056       0.229908  0.699347                0.229908           0.699347            1       True          9\n12       KNeighborsDist   0.653631       0.016264  0.011564                0.016264           0.011564            1       True          2\n13       KNeighborsUnif   0.653631       0.028849  0.011524                0.028849           0.011524            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMLarge\n0.832402\n0.003578\n0.422645\n0.003578\n0.422645\n1\nTrue\n13\n\n\n1\nNeuralNetFastAI\n0.832402\n0.011829\n2.153854\n0.011829\n2.153854\n1\nTrue\n10\n\n\n2\nWeightedEnsemble_L2\n0.832402\n0.012396\n2.629154\n0.000567\n0.475301\n2\nTrue\n14\n\n\n3\nCatBoost\n0.826816\n0.007282\n0.558903\n0.007282\n0.558903\n1\nTrue\n7\n\n\n4\nLightGBM\n0.821229\n0.003267\n0.163059\n0.003267\n0.163059\n1\nTrue\n4\n\n\n5\nNeuralNetTorch\n0.821229\n0.014016\n3.402719\n0.014016\n3.402719\n1\nTrue\n12\n\n\n6\nLightGBMXT\n0.815642\n0.003467\n0.479265\n0.003467\n0.479265\n1\nTrue\n3\n\n\n7\nRandomForestEntr\n0.815642\n0.175467\n0.606128\n0.175467\n0.606128\n1\nTrue\n6\n\n\n8\nRandomForestGini\n0.815642\n0.204887\n0.539022\n0.204887\n0.539022\n1\nTrue\n5\n\n\n9\nExtraTreesGini\n0.815642\n0.224873\n0.821764\n0.224873\n0.821764\n1\nTrue\n8\n\n\n10\nXGBoost\n0.810056\n0.005781\n0.229575\n0.005781\n0.229575\n1\nTrue\n11\n\n\n11\nExtraTreesEntr\n0.810056\n0.229908\n0.699347\n0.229908\n0.699347\n1\nTrue\n9\n\n\n12\nKNeighborsDist\n0.653631\n0.016264\n0.011564\n0.016264\n0.011564\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.653631\n0.028849\n0.011524\n0.028849\n0.011524\n1\nTrue\n1"
  },
  {
    "objectID": "posts/02wk-005.html#d.-예측-predict",
    "href": "posts/02wk-005.html#d.-예측-predict",
    "title": "02wk-005: 타이타닉 / Autogluon",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(df_train.Survived == predictr.predict(df_train)).mean()\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/features/generators/fillna.py:58: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  X.fillna(self._fillna_feature_map, inplace=True, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n\n\n0.8810325476992144\n\n\n\n(df_train.Survived == (df_train.Sex == \"female\")).mean() # 예전점수와 비교\n\n0.7867564534231201\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ndf_test.assign(Survived = predictr.predict(df_test)).loc[:,['PassengerId','Survived']]\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/features/generators/fillna.py:58: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  X.fillna(self._fillna_feature_map, inplace=True, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:192: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n  df = df.fillna(column_fills, inplace=False, downcast=False)\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/fastai/tabular/core.py:233: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if not is_categorical_dtype(c):\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n0\n892\n0\n\n\n1\n893\n0\n\n\n2\n894\n0\n\n\n3\n895\n0\n\n\n4\n896\n0\n\n\n...\n...\n...\n\n\n413\n1305\n0\n\n\n414\n1306\n1\n\n\n415\n1307\n0\n\n\n416\n1308\n0\n\n\n417\n1309\n0\n\n\n\n\n418 rows × 2 columns\n\n\n\n\n# df_test.assign(Survived = predictr.predict(df_test)).loc[:,['PassengerId','Survived']]\\\n# .to_csv(\"autogluon_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/03wk-009.html#a.-질문",
    "href": "posts/03wk-009.html#a.-질문",
    "title": "03wk-009: 아이스크림 / 회귀분석 – 추가해설",
    "section": "A. 질문",
    "text": "A. 질문\n- 질문: 기온이 \\(x=-2.0\\) 일 때 아이스크림을 얼마정도 판다고 보는게 타당할까?"
  },
  {
    "objectID": "posts/03wk-009.html#b.-답1",
    "href": "posts/03wk-009.html#b.-답1",
    "title": "03wk-009: 아이스크림 / 회귀분석 – 추가해설",
    "section": "B. 답1",
    "text": "B. 답1\n- \\(x=-2.0\\) 근처의 데이터를 살펴보자.\n\ndf_train[(-3.0 &lt; df_train.temp) & (df_train.temp &lt; -1.0)]\n\n\n\n\n\n\n\n\ntemp\nsales\n\n\n\n\n3\n-1.3\n17.673681\n\n\n\n\n\n\n\n대충 17.67 근처이지 않을까?.."
  },
  {
    "objectID": "posts/03wk-009.html#c.-답2",
    "href": "posts/03wk-009.html#c.-답2",
    "title": "03wk-009: 아이스크림 / 회귀분석 – 추가해설",
    "section": "C. 답2",
    "text": "C. 답2\n- 자료를 바탕으로 그림을 그려보자.\n\nplt.plot(df_train.temp,df_train.sales,'o')\nplt.plot([-2.0],[17.67],'x')\n\n\n\n\n\n저거 보다 못팔 것 같은데?"
  },
  {
    "objectID": "posts/03wk-009.html#d.-아이디어",
    "href": "posts/03wk-009.html#d.-아이디어",
    "title": "03wk-009: 아이스크림 / 회귀분석 – 추가해설",
    "section": "D. 아이디어",
    "text": "D. 아이디어\n- 선을 기가 막히게 그어서 추세선을 만들고, 그 추세선 위의 점으로 예측하자.\n- 속마음: 사실 추세선을 알고 있긴함\n\nplt.plot(df_train.temp,df_train.sales,'o')\nplt.plot(df_train.temp,20+df_train.temp*2.5,'--')\n\n\n\n\n- 사실 \\(y=20+2.5x\\) 라는 추세선을 그으면 된다는 것을 알고 있다.\n- 그래서 \\(x=-2\\) 이라면 \\(y=20-2.5\\times 2=15\\) 라고 보는게 합리적임. (물론 오차가 있을 수 있지만 그건 운이므로 어쩔수 없는것임, 랜덤으로 뭐가 나올지까지 맞출 수는 없음1)\n1 만약 그렇다면 랜덤이 아니겠지?\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\n오버피팅: 랜덤으로 뭐가 나올지 맞춘다는 것은 오차항을 적합하겠다는 의미임.\n\n\n- 그렇지만 우리는 사실 \\(20, 2.5\\) 라는 숫자를 모른다. (이 숫자만 안다면 임의의 \\(x\\)에 대한 \\(y\\)값을 알 수 있을 텐데…)\n- 게임셋팅\n\n원래게임: 임의의 \\(x\\)에 대하여 합리적인 \\(y\\)를 잘 찾는 게임\n변형된게임: \\(20,2.5\\) 라는 숫자를 잘 찾는 게임, 즉 데이터를 보고 최대한 \\(y_i \\approx ax_i+b\\) 이 되도록 \\(a,b\\)를 잘 선택하는 게임"
  },
  {
    "objectID": "posts/03wk-009.html#a.-데이터",
    "href": "posts/03wk-009.html#a.-데이터",
    "title": "03wk-009: 아이스크림 / 회귀분석 – 추가해설",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 변수 설정\n\nX = df_train[['temp']] # 독립변수, 설명변수, 피쳐\ny = df_train[['sales']] # 종속변수, 반응변수, 타겟 \n\n\nplt.plot(X,y,'o')\n\n\n\n\n- 질문: 기온이 \\(x=-2.0\\) 일 때 아이스크림을 얼마정도 판다고 보는게 타당할까?\n\ndf_train[(-3.0 &lt; df_train.temp) & (df_train.temp &lt; -1.0)]\n\n\n\n\n\n\n\n\ntemp\nsales\n\n\n\n\n3\n-1.3\n17.673681\n\n\n\n\n\n\n\n- 답1: 대충 17.67 근처이지 않을까?..\n- 답2: 17.67 보다 작지 않을까?\n- 아이디어: 추세선을 그리고 거기서 예측해보면 어떨까?\n- 데이터를 학습하여 추세선을 적절히 그릴 수 있고, 그려진 추세선으로 예측까지 해줄수 있는 아이(predictor)를 만들자."
  },
  {
    "objectID": "posts/03wk-009.html#b.-predictor-생성",
    "href": "posts/03wk-009.html#b.-predictor-생성",
    "title": "03wk-009: 아이스크림 / 회귀분석 – 추가해설",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LinearRegression() \npredictr \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nlinear regression 이라는 방법으로 추세선을 만들고 예측하는 아이(predictor)를 만드는 코드"
  },
  {
    "objectID": "posts/03wk-009.html#c.-학습-fit-learn",
    "href": "posts/03wk-009.html#c.-학습-fit-learn",
    "title": "03wk-009: 아이스크림 / 회귀분석 – 추가해설",
    "section": "C. 학습 (fit, learn)",
    "text": "C. 학습 (fit, learn)\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/03wk-009.html#d.-예측-predict",
    "href": "posts/03wk-009.html#d.-예측-predict",
    "title": "03wk-009: 아이스크림 / 회귀분석 – 추가해설",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- Predictor: 데이터를 살펴보니 원래 true는 이럴것 같아요\n\nyhat = predictr.predict(X)\n\n\nplt.plot(X,y,'o',alpha=0.5)\nplt.plot(X,yhat, 'o--',alpha=0.5)\n\n\n\n\n- 최규빈: 저런 추세선을 그었다면, \\(y=ax+b\\) 꼴의 식에서 \\(a\\), \\(b\\)를 적당한 값으로 찾았다는 의미인데, 그 값은 어디있지?\n- Predictor: 아래에 있어요\n\na = predictr.coef_,\nb = predictr.intercept_\n\n\na,b\n\n((array([[2.51561216]]),), array([19.66713127]))\n\n\n- 최규빈: 확인해보자..\n\n(df_train.temp * 2.51561216 + 19.66713127)[:5], yhat[:5]\n\n(0     9.353121\n 1    10.359366\n 2    12.120295\n 3    16.396835\n 4    18.409325\n Name: temp, dtype: float64,\n array([[ 9.35312141],\n        [10.35936628],\n        [12.12029479],\n        [16.39683546],\n        [18.40932519]]))\n\n\n- 새로운 데이터 \\(x=-2\\) 에 대한 예측 (1) – 수식위주로\n\n2.51561216*(-2) + 19.66713127\n\n14.635906949999999\n\n\n\nplt.plot(X,y,'o',alpha=0.5)\nplt.plot(X,yhat,'--',alpha=0.5)\nplt.plot([-2],[14.635906949999999],'xr')\n\n\n\n\n- 새로운 데이터 \\(x=-2\\) 에 대한 예측 (2) – 코드위주로 (\\(\\star\\))\n\nXX = pd.DataFrame({'temp':[-2.0]})\n\n\npredictr.predict(XX)\n\narray([[14.63590695]])\n\n\n\nplt.plot(X,y,'o',alpha=0.5)\nplt.plot(X,yhat,'--',alpha=0.5)\nplt.plot(XX, predictr.predict(XX),'xr')"
  },
  {
    "objectID": "posts/01wk-002.html#a.-gender_submission.csv",
    "href": "posts/01wk-002.html#a.-gender_submission.csv",
    "title": "01wk-002: 타이타닉 데이터의 이해 – 추가해설",
    "section": "A. gender_submission.csv",
    "text": "A. gender_submission.csv\n- 왜 여자만 살까?\n\n\n\n\n\n\nGTP 답변\n\n\n\ngender_submission.csv파일은 단순히 모든 여성 승객이 생존한다고 가정한 예측을 담은 파일로, 실제 데이터에 기반한 예측이 아닙니다. 이 파일은 제출 파일의 형식과 구성을 보여주기 위한 예시입니다. 실제로는 성별이나 다른 특성에 따라 생존 여부가 결정되는 것이 아닌, 여러 다양한 변수들이 생존에 영향을 미치게 됩니다.\n실제로 타이타닉호에서는 여성들이 생존율이 높았습니다. 타이타닉호 침몰 사건 당시 여성과 아이들을 먼저 구조하고 구명보트에 태우는 등의 우선순위가 있었기 때문입니다. 따라서 “gender_submission.csv” 파일은 예시로서 제공되었고, 실제 예측 모델을 개발할 때에는 여러 다른 변수들도 고려하여 예측해야 합니다."
  },
  {
    "objectID": "posts/01wk-002.html#b.-embarked",
    "href": "posts/01wk-002.html#b.-embarked",
    "title": "01wk-002: 타이타닉 데이터의 이해 – 추가해설",
    "section": "B. embarked",
    "text": "B. embarked\n- 항구는 승객의 배경과 사회적 상황을 반영할 수 있다.\n\n\n\n\n\n\nGPT 답변\n\n\n\n네, 항구별로 승객들의 배경과 사회적 상황이 다를 수 있습니다. 다양한 항구에서 탑승한 승객들은 지역적, 경제적, 문화적인 차이를 반영할 수 있습니다. 몇 가지 예시를 들어보겠습니다:\n\n경제적 차이: 서로 다른 항구에서 탑승한 승객들은 다른 경제적 배경을 가지고 있을 수 있습니다. 상류층 승객이나 부유한 승객들이 탑승한 항구일수록 1등급 티켓을 선택한 경우가 많을 것입니다.\n문화적 차이: 다른 항구에서 탑승한 승객들은 다른 문화적 배경을 가지고 있을 수 있습니다. 이로 인해 승객들의 습관, 언어, 관습 등이 다를 수 있습니다.\n지역적 차이: 항구의 위치와 지역적 특성에 따라 승객들의 배경이 달라질 수 있습니다. 해당 지역의 주요 산업이나 경제 활동에 따라 승객들의 직업이나 사회적 지위가 다를 수 있습니다.\n가족 관계: 특정 항구에서 가족들이 함께 탑승한 경우, 해당 지역의 가족 구조와 관련된 차이가 반영될 수 있습니다.\n\n이러한 다양한 차이를 분석하여 항구별로 생존율이나 다른 변수들과의 상관 관계를 파악할 수 있습니다. 이는 모델링 과정에서 추가적인 특성 엔지니어링이나 분석에 활용될 수 있습니다.\n\n\n- 항구는 국적에 대한 정보를 내포할 수도 있다.\n\n\n\n\n\n\nGPT 답변\n\n\n\n타이타닉호의 데이터에서는 승객들의 국적에 대한 직접적인 정보는 제공되지 않습니다. 따라서 승객들의 국적에 대한 정보를 확인하는 것은 어렵습니다. 그러나 항구별로 승객들의 출발지나 탑승 도시에 따라서 일부 국적적 특성이 반영될 수는 있습니다.\n예를 들어, Cherbourg 항구가 프랑스에 위치하며, Queenstown 항구가 아일랜드에 위치해 있습니다. 따라서 해당 항구에서 탑승한 승객들 중 일부는 해당 국가의 국적을 가질 가능성이 높을 것입니다. 그러나 국적은 항상 탑승 항구와 일치하지 않을 수 있으며, 탑승한 승객들의 다양한 출신지와 배경을 고려해야 합니다.\n요약하면, 타이타닉호 데이터에서는 직접적인 국적 정보는 제공되지 않지만, 항구별로 승객들의 출신지나 탑승 도시에 따라서 일부 국적적 특성이 반영될 수 있습니다."
  },
  {
    "objectID": "posts/01wk-002.html#c.-sibsp와-parch에-대한-피처엔지니어링-아이디어",
    "href": "posts/01wk-002.html#c.-sibsp와-parch에-대한-피처엔지니어링-아이디어",
    "title": "01wk-002: 타이타닉 데이터의 이해 – 추가해설",
    "section": "C. sibsp와 parch에 대한 피처엔지니어링 아이디어",
    "text": "C. sibsp와 parch에 대한 피처엔지니어링 아이디어\n- 두 변수 sibsp와 parch를 합쳐 family_size라는 하나의 변수를 만들고, 이를 활용하여 y를 예측할 수 있다.\n\n데이터를 봐야 하겠지만 가족단위로 보트를 줬을 확률도 큼\n가족단위로 보트를 실제로 주었을 경우 family_size라는 변수는 매우 의미있는 해석이 됨\n\n\n\n\n\n\n\nGPT 답변\n\n\n\n가족 규모 특성 생성: ‘sibsp’와 ’parch’ 변수를 활용하여 승객의 가족 규모를 나타내는 새로운 변수를 만들 수 있습니다. 예를 들어, ‘sibsp’와 ’parch’ 값을 더한 후 1을 더해 ‘family_size’ 변수로 만들 수 있습니다. 이를 통해 가족 규모가 큰 승객이 생존 여부에 영향을 미칠 수 있는지 확인할 수 있습니다.\n\n\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\n다중공선성: ‘sibsp’ 와 ‘parch’ 를 더하여 새로운 변수를 만든다면, 이후에는 ‘sibsp’와 ’parch’ 둘중 하나를 제거하나 둘다 제거하는 방법을 고려해야 한다."
  },
  {
    "objectID": "posts/01wk-002.html#d.-embarked와-class에-대한-피처엔지니어링의-아이디어",
    "href": "posts/01wk-002.html#d.-embarked와-class에-대한-피처엔지니어링의-아이디어",
    "title": "01wk-002: 타이타닉 데이터의 이해 – 추가해설",
    "section": "D. embarked와 class에 대한 피처엔지니어링의 아이디어",
    "text": "D. embarked와 class에 대한 피처엔지니어링의 아이디어\n- 경우에 따라서 두가지 범주를 하나도 합쳐서 만든 새로운 변수가 매우 의미있을 수 있음\n\n경우에 따라서 특정 국적 (프랑스)의 1등급 손님이 구명보트를 많이 얻었을 수도 있음.\n이럴경우 embarked와 class를 독립적으로 생각하는것보다 둘을 합친 변수가 훨씬 유의미한 변수임\n\n\n\n\n\n\n\nGPT 답변\n\n\n\n물론, 자세한 설명을 드리겠습니다. “항구와 클래스의 조합 특성 생성”이라는 개념은 ‘embarked’ (탑승 항구)와 ‘pclass’ (티켓 클래스) 두 변수를 활용하여 새로운 특성을 만들어 분석하는 것을 의미합니다.\n예를 들어, “embarked” 변수에는 ‘C’, ‘Q’, ‘S’와 같은 항구 정보가 있으며, “pclass” 변수에는 ’1’, ‘2’, ’3’과 같은 티켓 클래스 정보가 있습니다. 이 두 변수를 조합하여 새로운 변수를 만들어보겠습니다.\n생성할 새로운 변수의 예시:\n“embarked_pclass”: ‘C_1’, ‘C_2’, ‘C_3’, ‘Q_1’, ‘Q_2’, ‘Q_3’, ‘S_1’, ‘S_2’, ‘S_3’ 위와 같이 “embarked”와 “pclass”를 조합하여 새로운 변수 값을 만듭니다. 예를 들어 ’embarked’가 ’C’이고 ’pclass’가 ’1’인 경우, “embarked_pclass” 변수 값은 ’C_1’이 됩니다. 이렇게 조합된 변수는 해당 승객이 어떤 항구에서 탑승했고 어떤 티켓 클래스를 사용했는지를 동시에 나타내는 특성입니다.\n이렇게 생성된 “embarked_pclass” 변수를 사용하여 특정 항구와 클래스의 조합이 생존 여부에 어떤 영향을 미치는지를 분석할 수 있습니다. 예를 들어, ‘C_1’ 조합이 생존율과 관련이 있는지, ‘Q_3’ 조합이 생존 여부에 영향을 미치는지 등을 확인하여 해당 특성이 모델링에 유용한 정보를 제공하는지 알아볼 수 있습니다.\n\n\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\n교호작용: 그냥 교호작용 고려한다는 소리죠? –&gt; 트리계열은 결과가 크게 좋아질 것이라 기대하기 어렵겠고, 선형모형계열로 적합한다면 결과가 좋아질 수도 있겠음.\n\n\n\n\n\n\n\n\nGPT 답변 2 – 그냥 “embarked” 변수와 “pclass” 변수를 각각 넣고 분석하는 것과 무슨차이냐는 질문에 대한 답변\n\n\n\n네, “embarked” 변수와 “pclass” 변수를 각각 넣고 분석하는 것도 가능합니다. 그러나 새로운 변수를 만드는 것은 데이터를 조금 더 유용하게 활용하는 방법 중 하나입니다. 여기에 몇 가지 이유가 있습니다:\n상호작용 효과 강조: 새로운 변수를 만들면 “embarked”와 “pclass”의 조합이 생존 여부에 미치는 영향을 강조할 수 있습니다. 단순히 각각의 변수를 개별적으로 사용하는 것보다 조합된 변수를 통해 서로 간의 상호작용 효과를 쉽게 확인할 수 있습니다.\n더 풍부한 패턴 탐색: 새로운 변수를 만들면 서로 다른 항구와 클래스 조합들 간의 생존률 차이를 더 세부적으로 분석할 수 있습니다. 이를 통해 특정 항구와 클래스의 승객들이 생존 여부에 미치는 영향을 더 자세히 이해할 수 있습니다.\n다양한 조합 고려: 조합된 변수를 만들면 “embarked”와 “pclass”의 가능한 모든 조합을 커버할 수 있습니다. 이는 분석의 전체적인 시각을 확장하고, 어떤 조합이 중요한지를 더 효과적으로 파악할 수 있도록 도와줍니다.\n모델 학습에 더 적합한 데이터 생성: 몇몇 머신 러닝 알고리즘은 상호작용 변수를 활용하여 모델의 성능을 향상시킬 수 있습니다. 이러한 알고리즘들은 새로운 변수를 활용하여 더 복잡한 패턴을 학습할 수 있습니다.\n새로운 변수를 만들어 분석하는 것은 분석의 창의성과 도메인 지식을 활용하여 데이터로부터 더 많은 의미를 도출하려는 노력의 일부입니다. 분석의 목적에 따라 각 변수를 개별적으로 사용하거나 조합하여 새로운 변수를 만들 수 있습니다.\n\n\n\n\n\n\n\n\n2023-10-24 추가해설\n\n\n\n교호작용: 선형모델을 적합하는 상황을 가정하자. 교호작용의 효과가 있는 자료에서, 이를 무시하고 적합한다면 (“embarked” 변수와 “pclass” 변수만 넣고 적합한다면) 언더핏이 생김."
  },
  {
    "objectID": "posts/04wk-018.html#a.-학습-이후에-예측평가-가능",
    "href": "posts/04wk-018.html#a.-학습-이후에-예측평가-가능",
    "title": "04wk-018: Predictor의 깊은 이해, 기호정리",
    "section": "A. 학습 이후에 예측/평가 가능",
    "text": "A. 학습 이후에 예측/평가 가능\n- Predictor의 list생성\n\npredictors = [sklearn.linear_model.LinearRegression() for i in range(2)]\npredictors\n\n[LinearRegression(), LinearRegression()]\n\n\n\n두개의 predictor를 만들어서 리스트로 정리함\n\n- 첫번째 predictor에 접근\n\npredictors[0]\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 두번째 predictor에 접근\n\npredictors[1]\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 첫번째 predictor를 학습\n\npredictors[0].fit(df_train_X,df_train_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 학습이후에는 .coef_, .intercept_ 값이 생성됨\n\npredictors[0].coef_, predictors[0].intercept_\n\n(array([[-0.99420514]]), array([20.86235044]))\n\n\n\npredictors[1].coef_, predictors[1].intercept_\n\nAttributeError: 'LinearRegression' object has no attribute 'coef_'\n\n\n- .coef_와 .intercept_값이 생겨야 .predict(X)를 통하여 예측을 할 수 있음\n\npredictors[0].predict(df_train_X)\n\narray([[ 0.97824769],\n       [-0.01595745],\n       [-1.01016259],\n       [-2.00436772],\n       [-2.99857286],\n       [-3.992778  ],\n       [-4.98698314],\n       [-5.98118827]])\n\n\n\npredictors[1].predict(df_train_X)\n\nNotFittedError: This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n\n\n- 예측을 해야 평가를 할 수 있음\n\npredictors[0].score(df_train_X,df_train_y)\n\n0.9996857763812423\n\n\n\npredictors[1].score(df_train_X,df_train_y)\n\nNotFittedError: This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
  },
  {
    "objectID": "posts/04wk-018.html#b.-.fitxy에서-xy의-형식",
    "href": "posts/04wk-018.html#b.-.fitxy에서-xy의-형식",
    "title": "04wk-018: Predictor의 깊은 이해, 기호정리",
    "section": "B. .fit(X,y)에서 X,y의 형식",
    "text": "B. .fit(X,y)에서 X,y의 형식\n\npredictr = predictors[0]\n\n\nXs = {'DataFrame(2d)': df_train_X, \n      'Seires(1d)': df_train_X.X,\n      'ndarray(2d)': np.array(df_train_X),\n      'ndarray(1d)': np.array(df_train_X).reshape(-1),\n      'list(2d)': np.array(df_train_X).tolist(),\n      'list(1d)': np.array(df_train_X).reshape(-1).tolist()}\n\n\nys = {'DataFrame(2d)': df_train_y, \n      'Seires(1d)': df_train_y.y,\n      'ndarray(2d)': np.array(df_train_y),\n      'ndarray(1d)': np.array(df_train_y).reshape(-1),\n      'list(2d)': np.array(df_train_y).tolist(),\n      'list(1d)': np.array(df_train_y).reshape(-1).tolist()}\n\n\ndef test(X,y):\n    try: \n        predictr.fit(X,y)\n        return 'no error'\n    except:\n        return 'error'\n\n\n{('X='+i,'y='+j): test(Xs[i],ys[j]) for i,j in itertools.product(Xs.keys(),ys.keys())}\n\n{('X=DataFrame(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=Seires(1d)'): 'no error',\n ('X=DataFrame(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=DataFrame(2d)', 'y=list(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=list(1d)'): 'no error',\n ('X=Seires(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=Seires(1d)', 'y=Seires(1d)'): 'error',\n ('X=Seires(1d)', 'y=ndarray(2d)'): 'error',\n ('X=Seires(1d)', 'y=ndarray(1d)'): 'error',\n ('X=Seires(1d)', 'y=list(2d)'): 'error',\n ('X=Seires(1d)', 'y=list(1d)'): 'error',\n ('X=ndarray(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=Seires(1d)'): 'no error',\n ('X=ndarray(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=ndarray(2d)', 'y=list(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=list(1d)'): 'no error',\n ('X=ndarray(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=ndarray(1d)', 'y=Seires(1d)'): 'error',\n ('X=ndarray(1d)', 'y=ndarray(2d)'): 'error',\n ('X=ndarray(1d)', 'y=ndarray(1d)'): 'error',\n ('X=ndarray(1d)', 'y=list(2d)'): 'error',\n ('X=ndarray(1d)', 'y=list(1d)'): 'error',\n ('X=list(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=list(2d)', 'y=Seires(1d)'): 'no error',\n ('X=list(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=list(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=list(2d)', 'y=list(2d)'): 'no error',\n ('X=list(2d)', 'y=list(1d)'): 'no error',\n ('X=list(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=list(1d)', 'y=Seires(1d)'): 'error',\n ('X=list(1d)', 'y=ndarray(2d)'): 'error',\n ('X=list(1d)', 'y=ndarray(1d)'): 'error',\n ('X=list(1d)', 'y=list(2d)'): 'error',\n ('X=list(1d)', 'y=list(1d)'): 'error'}\n\n\n- 결론: X는 2d만 가능, y는 2d,1d 모두 가능"
  },
  {
    "objectID": "posts/A1.html",
    "href": "posts/A1.html",
    "title": "A1: Quarto Blog 만들기",
    "section": "",
    "text": "- 윈도우 컴퓨터인 경우 새로운 계정을 만들어 준다.\n\n한글계정명일 경우는 필수\n영어계정이더라도 계정을 분리하는게 관리에 유리\n\n- 최종적으로는\n\ngithub 로그인 + repository 생성완료\nanaconda 설치완료\ngit 설치완료\nquarto 설치완료\n\n이어야 한다.\n\n\n\ngithub 로그인\nrepository 생성 (저는 asdf로 만들었어요)\n\n\n주의: repository 만들때 readme.md 파일을 생성 할 것. 그래야 이후의 작업이 편리.\n\n\n\n\n\n아나콘다 다운로드: https://www.anaconda.com/download\n아나콘다 설치\n그림1처럼 terminal을 열었을 경우 (base)로 표시되는지 확인\n그림2처럼 conda env list등의 명령어가 잘 동작하는지 확인\n\n\n그림1그림2\n\n\n\n\n\n그림1: 정상적으로 설치된 경우 (base)가 보임\n\n\n\n\n\n\n\n그림2: 정상적으로 설치된 경우 cond env list 등의 명령이 잘 동작함. 현재환경은 *로 표시됨\n\n\n\n\n\n\n\n\n\ngit 다운로드: https://git-scm.com/downloads\n설치\nterminal에서 git을 입력하여 정상적으로 설치되었는지 확인. 그림3 참고.\n\n\n본인이 편리한 경우 github desktop 혹은 sourcetree 등에서 작업해도 무방함. 하지만 수업에서는 다루지 않음.\n\n\n\n\n그림3: git이 정상적으로 설치된 경우 출력예시\n\n\n\n\n\n\nquarto 다운로드: https://quarto.org/docs/download/ 에서 pre-release 버전을 다운로드 할 것 (기능이 좀 더 많음)\n설치\n터미널에서 quarto help를 입력하여 정상적으로 설치되었는지 확인. 그림4 참고.\n\n\n\n\n그림4: quarto가 정상적으로 설치된 경우 출력예시"
  },
  {
    "objectID": "posts/A1.html#a.-github",
    "href": "posts/A1.html#a.-github",
    "title": "A1: Quarto Blog 만들기",
    "section": "",
    "text": "github 로그인\nrepository 생성 (저는 asdf로 만들었어요)\n\n\n주의: repository 만들때 readme.md 파일을 생성 할 것. 그래야 이후의 작업이 편리."
  },
  {
    "objectID": "posts/A1.html#b.-anaconda",
    "href": "posts/A1.html#b.-anaconda",
    "title": "A1: Quarto Blog 만들기",
    "section": "",
    "text": "아나콘다 다운로드: https://www.anaconda.com/download\n아나콘다 설치\n그림1처럼 terminal을 열었을 경우 (base)로 표시되는지 확인\n그림2처럼 conda env list등의 명령어가 잘 동작하는지 확인\n\n\n그림1그림2\n\n\n\n\n\n그림1: 정상적으로 설치된 경우 (base)가 보임\n\n\n\n\n\n\n\n그림2: 정상적으로 설치된 경우 cond env list 등의 명령이 잘 동작함. 현재환경은 *로 표시됨"
  },
  {
    "objectID": "posts/A1.html#c.-git",
    "href": "posts/A1.html#c.-git",
    "title": "A1: Quarto Blog 만들기",
    "section": "",
    "text": "git 다운로드: https://git-scm.com/downloads\n설치\nterminal에서 git을 입력하여 정상적으로 설치되었는지 확인. 그림3 참고.\n\n\n본인이 편리한 경우 github desktop 혹은 sourcetree 등에서 작업해도 무방함. 하지만 수업에서는 다루지 않음.\n\n\n\n\n그림3: git이 정상적으로 설치된 경우 출력예시"
  },
  {
    "objectID": "posts/A1.html#d.-quarto",
    "href": "posts/A1.html#d.-quarto",
    "title": "A1: Quarto Blog 만들기",
    "section": "",
    "text": "quarto 다운로드: https://quarto.org/docs/download/ 에서 pre-release 버전을 다운로드 할 것 (기능이 좀 더 많음)\n설치\n터미널에서 quarto help를 입력하여 정상적으로 설치되었는지 확인. 그림4 참고.\n\n\n\n\n그림4: quarto가 정상적으로 설치된 경우 출력예시"
  },
  {
    "objectID": "posts/07wk-031.html",
    "href": "posts/07wk-031.html",
    "title": "07wk-031: 체중감량(교호작용) / 의사결정나무",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd \nimport sklearn.linear_model\nimport sklearn.tree\n\n\n\n3. Data\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/weightloss.csv')\ndf_train\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\n\n\n\n\n0\nFalse\nFalse\n-0.877103\n\n\n1\nTrue\nFalse\n1.604542\n\n\n2\nTrue\nTrue\n13.824148\n\n\n3\nTrue\nTrue\n13.004505\n\n\n4\nTrue\nTrue\n13.701128\n\n\n...\n...\n...\n...\n\n\n9995\nTrue\nFalse\n1.558841\n\n\n9996\nFalse\nFalse\n-0.217816\n\n\n9997\nFalse\nTrue\n4.072701\n\n\n9998\nTrue\nFalse\n-0.253796\n\n\n9999\nFalse\nFalse\n-1.399092\n\n\n\n\n10000 rows × 3 columns\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n- 운동과 체중감량보조제를 병행하면 시너지가 나는 것 같음\n\n\n4. 분석\n- 분석1: 선형회귀 (교호작용 고려 X)\n\n# step 1\nX,y = df_train[['Supplement','Exercise']], df_train['Weight_Loss']\n# step 2 \npredictr = sklearn.linear_model.LinearRegression()\n# step 3 \npredictr.fit(X,y)\n# step 4 \ndf_train['Weight_Loss_hat'] = predictr.predict(X)\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss_hat')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n-2.373106\n7.374557\n\n\nTrue\n2.845934\n12.593598\n\n\n\n\n\n\n\n- 분석2: 의사결정나무\n\n# step 1\nX,y = df_train[['Supplement','Exercise']], df_train['Weight_Loss']\n# step 2 \npredictr = sklearn.tree.DecisionTreeRegressor()\n# step 3 \npredictr.fit(X,y)\n# step 4 \ndf_train['Weight_Loss_hat'] = predictr.predict(X)\n\n\ndf_train\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\nWeight_Loss_hat\n\n\n\n\n0\nFalse\nFalse\n-0.877103\n0.021673\n\n\n1\nTrue\nFalse\n1.604542\n0.497573\n\n\n2\nTrue\nTrue\n13.824148\n14.966363\n\n\n3\nTrue\nTrue\n13.004505\n14.966363\n\n\n4\nTrue\nTrue\n13.701128\n14.966363\n\n\n...\n...\n...\n...\n...\n\n\n9995\nTrue\nFalse\n1.558841\n0.497573\n\n\n9996\nFalse\nFalse\n-0.217816\n0.021673\n\n\n9997\nFalse\nTrue\n4.072701\n4.991314\n\n\n9998\nTrue\nFalse\n-0.253796\n0.497573\n\n\n9999\nFalse\nFalse\n-1.399092\n0.021673\n\n\n\n\n10000 rows × 4 columns\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss_hat')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363"
  },
  {
    "objectID": "posts/A2.html",
    "href": "posts/A2.html",
    "title": "A2: 코드스페이스, 리눅스명령어, kaggle.json",
    "section": "",
    "text": "1. 코드스페이스\n- 장점:\n\n슈퍼장점: 자동으로 깃헙연결 되어있음 (요즘 깃헙쓰기 짜증나서 이게 장점임)\n로칼컴퓨터대비 장점: 내 컴퓨터 깨끗하게 쓸 수 있음 + 내 맘대로 무한 리눅스서버 사용가능\n코랩/kaggle 대비 장점: 한번 정리된 셋팅이 영구적으로 적용됨. (autogluon 맨날 안깔아도 된다)\n\n- 단점:\n\n컴터가 별로임, 사용시간 제한있음 (월 100시간?)\n단점을 극복하려면 결재해야함\n\n- 초기설정: 복붙설정하기, 주피터 extension 설치\n\n\n2. kaggle.json\n\n\n3. 간단한 리눅스 명령어\n- 이동: ls, cd\n- 폴더생성 및 삭제: mkdir, rm -rf\n- 복붙: cp/mv\n- zip압출풀기: unzip\n\n리눅스에 익숙해지면 코랩, 캐글등을 쉽게 사용할 수 있다.\n\n\n코랩 혹은 캐글에서 사용하지 않더라도, 코랩 혹은 캐글에서 사용하는 코드를 쉽게 해석할 수 있다.\n\n\n\n4. 타이타닉 데이터 불러오기\n\n\n5. Autogluon 설치\n- 코드스페이스에서 아래를 실행\npip install -U pip\npip install -U setuptools wheel\n\n# CPU version of pytorch has smaller footprint - see installation instructions in\n# pytorch documentation - https://pytorch.org/get-started/locally/\npip install torch==1.13.1+cpu torchvision==0.14.1+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html\n\npip install autogluon"
  },
  {
    "objectID": "posts/07wk-030.html",
    "href": "posts/07wk-030.html",
    "title": "07wk-030: 아이스크림(교호작용) / 회귀분석",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport sklearn.linear_model \nimport sklearn.tree\nimport sklearn.model_selection\n\n\n\n3. 예비학습\n\ndf = pd.DataFrame({'X1':[2,3,4,1],'X2':['A','B','A','C']})\ndf \n\n\n\n\n\n\n\n\nX1\nX2\n\n\n\n\n0\n2\nA\n\n\n1\n3\nB\n\n\n2\n4\nA\n\n\n3\n1\nC\n\n\n\n\n\n\n\n\npd.get_dummies(df)\n\n\n\n\n\n\n\n\nX1\nX2_A\nX2_B\nX2_C\n\n\n\n\n0\n2\nTrue\nFalse\nFalse\n\n\n1\n3\nFalse\nTrue\nFalse\n\n\n2\n4\nTrue\nFalse\nFalse\n\n\n3\n1\nFalse\nFalse\nTrue\n\n\n\n\n\n\n\n\nX2_A, X2_B, X2_C는 셋다 있을 필요는 없지 않나? –&gt; 공선성문제가 생길수도 있음.\n\n\npd.get_dummies(df,drop_first=True)\n\n\n\n\n\n\n\n\nX1\nX2_B\nX2_C\n\n\n\n\n0\n2\nFalse\nFalse\n\n\n1\n3\nTrue\nFalse\n\n\n2\n4\nFalse\nFalse\n\n\n3\n1\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n4. Data\n- load\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()\nchoco = 40 + temp * 2.0 + np.random.randn(100)*3\nvanilla = 60 + temp * 5.0 + np.random.randn(100)*3\ndf1 = pd.DataFrame({'temp':temp,'sales':choco}).assign(type='choco')\ndf2 = pd.DataFrame({'temp':temp,'sales':vanilla}).assign(type='vanilla')\ndf_train = pd.concat([df1,df2])\ndf_train\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n\n\n1\n-3.7\n35.852524\nchoco\n\n\n2\n-3.0\n37.428335\nchoco\n\n\n3\n-1.3\n38.323681\nchoco\n\n\n4\n-0.5\n39.713362\nchoco\n\n\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n\n\n96\n13.4\n129.300464\nvanilla\n\n\n97\n14.7\n136.596568\nvanilla\n\n\n98\n15.0\n136.213140\nvanilla\n\n\n99\n15.2\n135.595252\nvanilla\n\n\n\n\n200 rows × 3 columns\n\n\n\n- 시각화 및 해석\n\nplt.plot(df_train[df.type=='choco'].temp,df_train[df.type=='choco'].sales,'o',label='choco')\nplt.plot(df_train[df.type=='vanilla'].temp,df_train[df.type=='vanilla'].sales,'o',label='vanilla')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f7fa2047e20&gt;\n\n\n\n\n\n\n온도에 따른 아이스크림 판매량이 아이스크림의 tpye에 따라 동일하다면 기울기가 동일하고 절편이 다른 두 직선이 나올것임.\n하지만 현재는 초코보다 바닐라맛이 기온의 영향을 많이 받아보임 \\(\\to\\) (바닐라아이스크림,온도)는 (초코아이스크림,온도)보다 궁합이 좋다. \\(\\to\\) 아이스크림 type과 온도사이에는 교호작용이 존재한다.\n\n\n\n5. 분석1\n- 분석1: 모형을 아래와 같이 본다.\n\n\\({\\bf X}\\): temp, type\n\\({\\bf y}\\): sales\n\n\n# step1 \nX,y = pd.get_dummies(df_train[['temp','type']],drop_first=True), df['sales']\n# step2\npredictr = sklearn.linear_model.LinearRegression()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train['sales_hat'] = predictr.predict(X)\n\n\npredictr.score(X,y)\n\n0.9249530603100549\n\n\n\n점수가 잘나왔다고 너무 좋아하지 마세요.\n시각화를 반드시 해보고 더 맞출수 있는 여지가 있는지 항상 확인할 것\n\n\nplt.plot(df_train[df.type=='choco'].temp,df_train[df.type=='choco'].sales,'o',label='choco',color='C0',alpha=0.5)\nplt.plot(df_train[df.type=='choco'].temp,df_train[df.type=='choco'].sales_hat,'--',color='C0')\nplt.plot(df_train[df.type=='vanilla'].temp,df_train[df.type=='vanilla'].sales,'o',label='vanilla',color='C1',alpha=0.5)\nplt.plot(df_train[df.type=='vanilla'].temp,df_train[df.type=='vanilla'].sales_hat,'--',color='C1')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f7fa2199ab0&gt;\n\n\n\n\n\n\n이 모형은 초코/바닐라에 대한 기울기차이를 “표현”할 수 없다. 이러한 상황은 “모형의 표현력이 약하다” 혹은 “언더피팅”인 상황이라고 한다.\n\n\n\n6. 분석2\n- 모형을 아래와 같이 본다.\n\n\\({\\bf X}\\): temp, type, temp \\(\\times\\) type\n\\({\\bf y}\\): sales\n\n\npd.get_dummies(df_train[['temp','type']],drop_first=True).eval('interaction = temp*type_vanilla')\n\n\n\n\n\n\n\n\ntemp\ntype_vanilla\ninteraction\n\n\n\n\n0\n-4.1\nFalse\n-0.0\n\n\n1\n-3.7\nFalse\n-0.0\n\n\n2\n-3.0\nFalse\n-0.0\n\n\n3\n-1.3\nFalse\n-0.0\n\n\n4\n-0.5\nFalse\n-0.0\n\n\n...\n...\n...\n...\n\n\n95\n12.4\nTrue\n12.4\n\n\n96\n13.4\nTrue\n13.4\n\n\n97\n14.7\nTrue\n14.7\n\n\n98\n15.0\nTrue\n15.0\n\n\n99\n15.2\nTrue\n15.2\n\n\n\n\n200 rows × 3 columns\n\n\n\n\n# step1 \nX = pd.get_dummies(df_train[['temp','type']],drop_first=True).eval('interaction = temp*type_vanilla')\ny = df['sales']\n# step2\npredictr = sklearn.linear_model.LinearRegression()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train['sales_hat'] = predictr.predict(X)\n\n\npredictr.score(X,y)\n\n0.9865793819066231\n\n\n\nplt.plot(df_train[df.type=='choco'].temp,df_train[df.type=='choco'].sales,'o',label='choco',color='C0',alpha=0.5)\nplt.plot(df_train[df.type=='choco'].temp,df_train[df.type=='choco'].sales_hat,'--',color='C0')\nplt.plot(df_train[df.type=='vanilla'].temp,df_train[df.type=='vanilla'].sales,'o',label='vanilla',color='C1',alpha=0.5)\nplt.plot(df_train[df.type=='vanilla'].temp,df_train[df.type=='vanilla'].sales_hat,'--',color='C1')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f7fa1925690&gt;\n\n\n\n\n\n\nNote: 초코/바닐라에 대한 절편차이는 type로, 초코/바닐라에 대한 기울기 차이는 temp\\(\\times\\)type로 표현한다."
  },
  {
    "objectID": "posts/03wk-013.html#a.-데이터-정리",
    "href": "posts/03wk-013.html#a.-데이터-정리",
    "title": "03wk-013: 타이타닉 / 로지스틱 – 추가해설",
    "section": "A. 데이터 정리",
    "text": "A. 데이터 정리\n\nX = pd.get_dummies(df_train.drop(['PassengerId','Survived'],axis=1))\ny = df_train[['Survived']]"
  },
  {
    "objectID": "posts/03wk-013.html#b.-predictor-생성",
    "href": "posts/03wk-013.html#b.-predictor-생성",
    "title": "03wk-013: 타이타닉 / 로지스틱 – 추가해설",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LogisticRegression()\npredictr \n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/03wk-013.html#c.-학습-fit-learn",
    "href": "posts/03wk-013.html#c.-학습-fit-learn",
    "title": "03wk-013: 타이타닉 / 로지스틱 – 추가해설",
    "section": "C. 학습 (fit, learn)",
    "text": "C. 학습 (fit, learn)\n\npredictr.fit(X,y)\n\nValueError: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
  },
  {
    "objectID": "posts/03wk-013.html#a.-데이터정리",
    "href": "posts/03wk-013.html#a.-데이터정리",
    "title": "03wk-013: 타이타닉 / 로지스틱 – 추가해설",
    "section": "A. 데이터정리",
    "text": "A. 데이터정리\n\nX = pd.get_dummies(df_train[[\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]])\ny = df_train[[\"Survived\"]]"
  },
  {
    "objectID": "posts/03wk-013.html#b.-predictor-생성-1",
    "href": "posts/03wk-013.html#b.-predictor-생성-1",
    "title": "03wk-013: 타이타닉 / 로지스틱 – 추가해설",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LogisticRegression()"
  },
  {
    "objectID": "posts/03wk-013.html#c.-학습",
    "href": "posts/03wk-013.html#c.-학습",
    "title": "03wk-013: 타이타닉 / 로지스틱 – 추가해설",
    "section": "C. 학습",
    "text": "C. 학습\n\npredictr.fit(X, y)\n\n/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/03wk-013.html#d.-예측",
    "href": "posts/03wk-013.html#d.-예측",
    "title": "03wk-013: 타이타닉 / 로지스틱 – 추가해설",
    "section": "D. 예측",
    "text": "D. 예측\n\n#predictr.predict(X)\ndf_train.assign(Survived_hat=predictr.predict(X)).loc[:,['Survived','Survived_hat']]\n\n\n\n\n\n\n\n\nSurvived\nSurvived_hat\n\n\n\n\n0\n0\n0\n\n\n1\n1\n1\n\n\n2\n1\n1\n\n\n3\n1\n1\n\n\n4\n0\n0\n\n\n...\n...\n...\n\n\n886\n0\n0\n\n\n887\n1\n1\n\n\n888\n0\n1\n\n\n889\n1\n0\n\n\n890\n0\n0\n\n\n\n\n891 rows × 2 columns"
  },
  {
    "objectID": "posts/03wk-013.html#e.-평가",
    "href": "posts/03wk-013.html#e.-평가",
    "title": "03wk-013: 타이타닉 / 로지스틱 – 추가해설",
    "section": "E. 평가",
    "text": "E. 평가\n\npredictr.score(X,y)\n\n0.8002244668911336"
  }
]